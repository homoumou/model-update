{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "533f5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236d2ab",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47219bdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'..\\Dataset\\SWAT\\SWaT.csv') \n",
    "df.insert(78,'Label','Normal')\n",
    "#Attack on FIT401\n",
    "df.loc[((df['Timestamp'] >='2019-07-20T07:08:46Z') & (df['Timestamp'] < '2019-07-20T07:10:311Z')),'Label']='Attack'\n",
    "#Attack on LIT301\n",
    "df.loc[((df['Timestamp'] >='2019-07-20T07:15:00Z') & (df['Timestamp'] < '2019-07-20T07:19:321Z')),'Label']='Attack'\n",
    "#Attack on P601\n",
    "df.loc[((df['Timestamp'] >='2019-07-20T07:26:57Z') & (df['Timestamp'] < '2019-07-20T07:30:481Z')),'Label']='Attack'\n",
    "#Multipoint Attack\n",
    "df.loc[((df['Timestamp'] >='2019-07-20T07:38:50Z') & (df['Timestamp'] < '2019-07-20T07:46:201Z')),'Label']='Attack'\n",
    "##Attack on MV501\n",
    "df.loc[((df['Timestamp'] >='2019-07-20T07:54:00Z') & (df['Timestamp'] < '2019-07-20T07:56:01Z')),'Label']='Attack'\n",
    "#Attack on P301\n",
    "df.loc[((df['Timestamp'] >='2019-07-20T08:02:56Z') & (df['Timestamp'] < '2019-07-20T08:16:181Z')),'Label']='Attack'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "500b1e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>FIT 101</th>\n",
       "      <th>LIT 101</th>\n",
       "      <th>MV 101</th>\n",
       "      <th>P1_STATE</th>\n",
       "      <th>P101 Status</th>\n",
       "      <th>P102 Status</th>\n",
       "      <th>AIT 201</th>\n",
       "      <th>AIT 202</th>\n",
       "      <th>AIT 203</th>\n",
       "      <th>...</th>\n",
       "      <th>LSH 602</th>\n",
       "      <th>LSH 603</th>\n",
       "      <th>LSL 601</th>\n",
       "      <th>LSL 602</th>\n",
       "      <th>LSL 603</th>\n",
       "      <th>P6 STATE</th>\n",
       "      <th>P601 Status</th>\n",
       "      <th>P602 Status</th>\n",
       "      <th>P603 Status</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-20T04:30:00Z</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>729.865800</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.293002</td>\n",
       "      <td>198.077423</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-20T04:30:01Z</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>729.434000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.293002</td>\n",
       "      <td>198.385025</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-20T04:30:02.004013Z</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>729.120000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.293002</td>\n",
       "      <td>198.436300</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-20T04:30:03.004013Z</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>728.688200</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.289157</td>\n",
       "      <td>198.667000</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-20T04:30:04Z</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>727.706900</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.289157</td>\n",
       "      <td>198.897720</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14991</th>\n",
       "      <td>2019-07-20T08:39:55.001007Z</td>\n",
       "      <td>4.200429</td>\n",
       "      <td>491.169769</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.319918</td>\n",
       "      <td>257.703156</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14992</th>\n",
       "      <td>2019-07-20T08:39:56.0050048Z</td>\n",
       "      <td>4.253915</td>\n",
       "      <td>491.405273</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.317354</td>\n",
       "      <td>257.703156</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14993</th>\n",
       "      <td>2019-07-20T08:39:57.0050048Z</td>\n",
       "      <td>4.303558</td>\n",
       "      <td>492.308100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.317354</td>\n",
       "      <td>257.703156</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>2019-07-20T08:39:58.0050048Z</td>\n",
       "      <td>4.323736</td>\n",
       "      <td>492.465100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.316713</td>\n",
       "      <td>257.703156</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>2019-07-20T08:39:59.004013Z</td>\n",
       "      <td>4.323736</td>\n",
       "      <td>492.896881</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.313829</td>\n",
       "      <td>257.933868</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14996 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Timestamp   FIT 101     LIT 101  MV 101  P1_STATE  \\\n",
       "0              2019-07-20T04:30:00Z  0.000000  729.865800       1         3   \n",
       "1              2019-07-20T04:30:01Z  0.000000  729.434000       1         3   \n",
       "2       2019-07-20T04:30:02.004013Z  0.000000  729.120000       1         3   \n",
       "3       2019-07-20T04:30:03.004013Z  0.000000  728.688200       1         3   \n",
       "4              2019-07-20T04:30:04Z  0.000000  727.706900       1         3   \n",
       "...                             ...       ...         ...     ...       ...   \n",
       "14991   2019-07-20T08:39:55.001007Z  4.200429  491.169769       2         2   \n",
       "14992  2019-07-20T08:39:56.0050048Z  4.253915  491.405273       2         2   \n",
       "14993  2019-07-20T08:39:57.0050048Z  4.303558  492.308100       2         2   \n",
       "14994  2019-07-20T08:39:58.0050048Z  4.323736  492.465100       2         2   \n",
       "14995   2019-07-20T08:39:59.004013Z  4.323736  492.896881       2         2   \n",
       "\n",
       "       P101 Status  P102 Status     AIT 201   AIT 202     AIT 203  ...  \\\n",
       "0                2            1  142.527557  9.293002  198.077423  ...   \n",
       "1                2            1  142.527557  9.293002  198.385025  ...   \n",
       "2                2            1  142.527557  9.293002  198.436300  ...   \n",
       "3                2            1  142.527557  9.289157  198.667000  ...   \n",
       "4                2            1  142.527557  9.289157  198.897720  ...   \n",
       "...            ...          ...         ...       ...         ...  ...   \n",
       "14991            2            1  131.408615  9.319918  257.703156  ...   \n",
       "14992            2            1  131.408615  9.317354  257.703156  ...   \n",
       "14993            2            1  131.408615  9.317354  257.703156  ...   \n",
       "14994            2            1  131.408615  9.316713  257.703156  ...   \n",
       "14995            2            1  131.408615  9.313829  257.933868  ...   \n",
       "\n",
       "       LSH 602   LSH 603   LSL 601   LSL 602 LSL 603  P6 STATE  P601 Status  \\\n",
       "0       Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "1       Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "2       Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "3       Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "4       Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "...        ...       ...       ...       ...     ...       ...          ...   \n",
       "14991   Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "14992   Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "14993   Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "14994   Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "14995   Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "\n",
       "       P602 Status  P603 Status   Label  \n",
       "0                1            1  Normal  \n",
       "1                1            1  Normal  \n",
       "2                1            1  Normal  \n",
       "3                1            1  Normal  \n",
       "4                1            1  Normal  \n",
       "...            ...          ...     ...  \n",
       "14991            1            1  Normal  \n",
       "14992            1            1  Normal  \n",
       "14993            1            1  Normal  \n",
       "14994            1            1  Normal  \n",
       "14995            1            1  Normal  \n",
       "\n",
       "[14996 rows x 79 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3f0ce96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIT 101</th>\n",
       "      <th>LIT 101</th>\n",
       "      <th>MV 101</th>\n",
       "      <th>P1_STATE</th>\n",
       "      <th>P101 Status</th>\n",
       "      <th>P102 Status</th>\n",
       "      <th>AIT 201</th>\n",
       "      <th>AIT 202</th>\n",
       "      <th>AIT 203</th>\n",
       "      <th>FIT 201</th>\n",
       "      <th>...</th>\n",
       "      <th>P501 Status</th>\n",
       "      <th>P502 Status</th>\n",
       "      <th>PIT 501</th>\n",
       "      <th>PIT 502</th>\n",
       "      <th>PIT 503</th>\n",
       "      <th>FIT 601</th>\n",
       "      <th>P6 STATE</th>\n",
       "      <th>P601 Status</th>\n",
       "      <th>P602 Status</th>\n",
       "      <th>P603 Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.0</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>14996.0</td>\n",
       "      <td>14996.0</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.0</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.0</td>\n",
       "      <td>14996.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.740876</td>\n",
       "      <td>733.960251</td>\n",
       "      <td>1.156175</td>\n",
       "      <td>2.061616</td>\n",
       "      <td>1.374166</td>\n",
       "      <td>1.0</td>\n",
       "      <td>138.766501</td>\n",
       "      <td>9.210022</td>\n",
       "      <td>247.985162</td>\n",
       "      <td>0.869760</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160.706744</td>\n",
       "      <td>4.673115</td>\n",
       "      <td>115.157048</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.015337</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.634632</td>\n",
       "      <td>110.960185</td>\n",
       "      <td>0.384272</td>\n",
       "      <td>0.240466</td>\n",
       "      <td>0.483923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.265845</td>\n",
       "      <td>0.175812</td>\n",
       "      <td>11.806186</td>\n",
       "      <td>1.121283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.707502</td>\n",
       "      <td>18.183883</td>\n",
       "      <td>5.324942</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>491.169769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>113.849014</td>\n",
       "      <td>8.768457</td>\n",
       "      <td>198.077423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>158.132523</td>\n",
       "      <td>2.450902</td>\n",
       "      <td>111.654060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>640.595184</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>131.536789</td>\n",
       "      <td>9.090170</td>\n",
       "      <td>239.887200</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>159.526400</td>\n",
       "      <td>2.851376</td>\n",
       "      <td>114.233528</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>819.636841</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>143.713150</td>\n",
       "      <td>9.233082</td>\n",
       "      <td>246.218918</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160.231354</td>\n",
       "      <td>2.883414</td>\n",
       "      <td>114.714172</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>820.971436</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>144.033585</td>\n",
       "      <td>9.345873</td>\n",
       "      <td>257.190460</td>\n",
       "      <td>2.320187</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160.695969</td>\n",
       "      <td>2.963509</td>\n",
       "      <td>115.002563</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.403484</td>\n",
       "      <td>825.092957</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.821335</td>\n",
       "      <td>9.490067</td>\n",
       "      <td>272.289154</td>\n",
       "      <td>2.342357</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>219.014359</td>\n",
       "      <td>192.371765</td>\n",
       "      <td>170.565247</td>\n",
       "      <td>0.137315</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            FIT 101       LIT 101        MV 101      P1_STATE   P101 Status  \\\n",
       "count  14996.000000  14996.000000  14996.000000  14996.000000  14996.000000   \n",
       "mean       0.740876    733.960251      1.156175      2.061616      1.374166   \n",
       "std        1.634632    110.960185      0.384272      0.240466      0.483923   \n",
       "min        0.000000    491.169769      0.000000      2.000000      1.000000   \n",
       "25%        0.000000    640.595184      1.000000      2.000000      1.000000   \n",
       "50%        0.000000    819.636841      1.000000      2.000000      1.000000   \n",
       "75%        0.000000    820.971436      1.000000      2.000000      2.000000   \n",
       "max        4.403484    825.092957      2.000000      3.000000      2.000000   \n",
       "\n",
       "       P102 Status       AIT 201       AIT 202       AIT 203       FIT 201  \\\n",
       "count      14996.0  14996.000000  14996.000000  14996.000000  14996.000000   \n",
       "mean           1.0    138.766501      9.210022    247.985162      0.869760   \n",
       "std            0.0      8.265845      0.175812     11.806186      1.121283   \n",
       "min            1.0    113.849014      8.768457    198.077423      0.000000   \n",
       "25%            1.0    131.536789      9.090170    239.887200      0.000384   \n",
       "50%            1.0    143.713150      9.233082    246.218918      0.000513   \n",
       "75%            1.0    144.033585      9.345873    257.190460      2.320187   \n",
       "max            1.0    146.821335      9.490067    272.289154      2.342357   \n",
       "\n",
       "       ...  P501 Status  P502 Status       PIT 501       PIT 502  \\\n",
       "count  ...      14996.0      14996.0  14996.000000  14996.000000   \n",
       "mean   ...          2.0          1.0    160.706744      4.673115   \n",
       "std    ...          0.0          0.0      5.707502     18.183883   \n",
       "min    ...          2.0          1.0    158.132523      2.450902   \n",
       "25%    ...          2.0          1.0    159.526400      2.851376   \n",
       "50%    ...          2.0          1.0    160.231354      2.883414   \n",
       "75%    ...          2.0          1.0    160.695969      2.963509   \n",
       "max    ...          2.0          1.0    219.014359    192.371765   \n",
       "\n",
       "            PIT 503       FIT 601  P6 STATE   P601 Status  P602 Status  \\\n",
       "count  14996.000000  14996.000000   14996.0  14996.000000      14996.0   \n",
       "mean     115.157048      0.000402       2.0      1.015337          1.0   \n",
       "std        5.324942      0.003317       0.0      0.122895          0.0   \n",
       "min      111.654060      0.000000       2.0      1.000000          1.0   \n",
       "25%      114.233528      0.000256       2.0      1.000000          1.0   \n",
       "50%      114.714172      0.000320       2.0      1.000000          1.0   \n",
       "75%      115.002563      0.000320       2.0      1.000000          1.0   \n",
       "max      170.565247      0.137315       2.0      2.000000          1.0   \n",
       "\n",
       "       P603 Status  \n",
       "count      14996.0  \n",
       "mean           1.0  \n",
       "std            0.0  \n",
       "min            1.0  \n",
       "25%            1.0  \n",
       "50%            1.0  \n",
       "75%            1.0  \n",
       "max            1.0  \n",
       "\n",
       "[8 rows x 66 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c6e34c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Timestamp', 'FIT 101', 'LIT 101', 'MV 101', 'P1_STATE', 'P101 Status',\n",
       "       'P102 Status', 'AIT 201', 'AIT 202', 'AIT 203', 'FIT 201', 'LS 201',\n",
       "       'LS 202', 'LSL 203', 'LSLL 203', 'MV201', 'P2_STATE', 'P201 Status',\n",
       "       'P202 Status', 'P203 Status', 'P204 Status', 'P205 Status',\n",
       "       'P206 Status', 'P207 Status', 'P208 Status', 'AIT 301', 'AIT 302',\n",
       "       'AIT 303', 'DPIT 301', 'FIT 301', 'LIT 301', 'MV 301', 'MV 302',\n",
       "       'MV 303', 'MV 304', 'P3_STATE', 'P301 Status', 'P302 Status', 'AIT 401',\n",
       "       'AIT 402', 'FIT 401', 'LIT 401', 'LS 401', 'P4_STATE', 'P401 Status',\n",
       "       'P402 Status', 'P403 Status', 'P404 Status', 'UV401', 'AIT 501',\n",
       "       'AIT 502', 'AIT 503', 'AIT 504', 'FIT 501', 'FIT 502', 'FIT 503',\n",
       "       'FIT 504', 'MV 501', 'MV 502', 'MV 503', 'MV 504', 'P5_STATE',\n",
       "       'P501 Status', 'P502 Status', 'PIT 501', 'PIT 502', 'PIT 503',\n",
       "       'FIT 601', 'LSH 601', 'LSH 602', 'LSH 603', 'LSL 601', 'LSL 602',\n",
       "       'LSL 603', 'P6 STATE', 'P601 Status', 'P602 Status', 'P603 Status',\n",
       "       'Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a662eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal    13016\n",
       "Attack     1980\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2820772",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.drop(['Timestamp'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0cc2439c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "['LS 201', 'LS 202', 'LSL 203', 'LSLL 203', 'LS 401', 'LSH 601', 'LSH 602', 'LSH 603', 'LSL 601', 'LSL 602', 'LSL 603', 'Label']\n"
     ]
    }
   ],
   "source": [
    "# Get list of categorical variables\n",
    "s = (test_df.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd3f509d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIT 101</th>\n",
       "      <th>LIT 101</th>\n",
       "      <th>MV 101</th>\n",
       "      <th>P1_STATE</th>\n",
       "      <th>P101 Status</th>\n",
       "      <th>P102 Status</th>\n",
       "      <th>AIT 201</th>\n",
       "      <th>AIT 202</th>\n",
       "      <th>AIT 203</th>\n",
       "      <th>FIT 201</th>\n",
       "      <th>...</th>\n",
       "      <th>LSH 602</th>\n",
       "      <th>LSH 603</th>\n",
       "      <th>LSL 601</th>\n",
       "      <th>LSL 602</th>\n",
       "      <th>LSL 603</th>\n",
       "      <th>P6 STATE</th>\n",
       "      <th>P601 Status</th>\n",
       "      <th>P602 Status</th>\n",
       "      <th>P603 Status</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>729.865800</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.293002</td>\n",
       "      <td>198.077423</td>\n",
       "      <td>2.335437</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>729.434000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.293002</td>\n",
       "      <td>198.385025</td>\n",
       "      <td>2.335437</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>729.120000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.293002</td>\n",
       "      <td>198.436300</td>\n",
       "      <td>2.335437</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>728.688200</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.289157</td>\n",
       "      <td>198.667000</td>\n",
       "      <td>2.335437</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>727.706900</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.289157</td>\n",
       "      <td>198.897720</td>\n",
       "      <td>2.335437</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14991</th>\n",
       "      <td>4.200429</td>\n",
       "      <td>491.169769</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.319918</td>\n",
       "      <td>257.703156</td>\n",
       "      <td>2.316086</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14992</th>\n",
       "      <td>4.253915</td>\n",
       "      <td>491.405273</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.317354</td>\n",
       "      <td>257.703156</td>\n",
       "      <td>2.314292</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14993</th>\n",
       "      <td>4.303558</td>\n",
       "      <td>492.308100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.317354</td>\n",
       "      <td>257.703156</td>\n",
       "      <td>2.313651</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>4.323736</td>\n",
       "      <td>492.465100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.316713</td>\n",
       "      <td>257.703156</td>\n",
       "      <td>2.313651</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>4.323736</td>\n",
       "      <td>492.896881</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.313829</td>\n",
       "      <td>257.933868</td>\n",
       "      <td>2.313651</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14996 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FIT 101     LIT 101  MV 101  P1_STATE  P101 Status  P102 Status  \\\n",
       "0      0.000000  729.865800       1         3            2            1   \n",
       "1      0.000000  729.434000       1         3            2            1   \n",
       "2      0.000000  729.120000       1         3            2            1   \n",
       "3      0.000000  728.688200       1         3            2            1   \n",
       "4      0.000000  727.706900       1         3            2            1   \n",
       "...         ...         ...     ...       ...          ...          ...   \n",
       "14991  4.200429  491.169769       2         2            2            1   \n",
       "14992  4.253915  491.405273       2         2            2            1   \n",
       "14993  4.303558  492.308100       2         2            2            1   \n",
       "14994  4.323736  492.465100       2         2            2            1   \n",
       "14995  4.323736  492.896881       2         2            2            1   \n",
       "\n",
       "          AIT 201   AIT 202     AIT 203   FIT 201  ...  LSH 602  LSH 603  \\\n",
       "0      142.527557  9.293002  198.077423  2.335437  ...        0        0   \n",
       "1      142.527557  9.293002  198.385025  2.335437  ...        0        0   \n",
       "2      142.527557  9.293002  198.436300  2.335437  ...        0        0   \n",
       "3      142.527557  9.289157  198.667000  2.335437  ...        0        0   \n",
       "4      142.527557  9.289157  198.897720  2.335437  ...        0        0   \n",
       "...           ...       ...         ...       ...  ...      ...      ...   \n",
       "14991  131.408615  9.319918  257.703156  2.316086  ...        0        0   \n",
       "14992  131.408615  9.317354  257.703156  2.314292  ...        0        0   \n",
       "14993  131.408615  9.317354  257.703156  2.313651  ...        0        0   \n",
       "14994  131.408615  9.316713  257.703156  2.313651  ...        0        0   \n",
       "14995  131.408615  9.313829  257.933868  2.313651  ...        0        0   \n",
       "\n",
       "       LSL 601  LSL 602  LSL 603  P6 STATE  P601 Status  P602 Status  \\\n",
       "0            0        0        0         2            1            1   \n",
       "1            0        0        0         2            1            1   \n",
       "2            0        0        0         2            1            1   \n",
       "3            0        0        0         2            1            1   \n",
       "4            0        0        0         2            1            1   \n",
       "...        ...      ...      ...       ...          ...          ...   \n",
       "14991        0        0        0         2            1            1   \n",
       "14992        0        0        0         2            1            1   \n",
       "14993        0        0        0         2            1            1   \n",
       "14994        0        0        0         2            1            1   \n",
       "14995        0        0        0         2            1            1   \n",
       "\n",
       "       P603 Status  Label  \n",
       "0                1      1  \n",
       "1                1      1  \n",
       "2                1      1  \n",
       "3                1      1  \n",
       "4                1      1  \n",
       "...            ...    ...  \n",
       "14991            1      1  \n",
       "14992            1      1  \n",
       "14993            1      1  \n",
       "14994            1      1  \n",
       "14995            1      1  \n",
       "\n",
       "[14996 rows x 78 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_df = test_df.copy()\n",
    "\n",
    "# inactive = 0 active = 1     label: normal = 1 , attack = 0\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    label_df[col] = label_encoder.fit_transform(test_df[col])\n",
    "\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3a7f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = label_df.Label\n",
    "X = label_df.drop(['Label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a700196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c0a21f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    13016\n",
       "0     1980\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0bf4953c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.71482317, 0.5       , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.71353006, 0.5       , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.71258972, 0.5       , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.97730752, 0.00340896, 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.98188965, 0.00387913, 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.98188965, 0.00517218, 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized=scaler.fit_transform(values)\n",
    "X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec7282c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# # Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_normalized, Y, train_size=0.9, test_size=0.1,\n",
    "                                                                random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df861d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13496, 77)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "970f88ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert to time domain dataset\n",
    "def create_dataset(X, time_steps):\n",
    "    Xs = []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "    return np.array(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb73a8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13495, 1, 77)\n",
      "(13495, 1, 1)\n",
      "(1499, 1, 77)\n",
      "(1499, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape to [samples, time_steps, n_features]\n",
    "\n",
    "TIME_STEPS = 1\n",
    "X_train = pd.DataFrame(X_train_full)\n",
    "X_train = create_dataset(X_train, TIME_STEPS)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_train = create_dataset(y_train, TIME_STEPS)\n",
    "X_valid = pd.DataFrame(X_valid_full)\n",
    "X_valid = create_dataset(X_valid, TIME_STEPS)\n",
    "y_valid = pd.DataFrame(y_valid)\n",
    "y_valid = create_dataset(y_valid, TIME_STEPS)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c977d4",
   "metadata": {},
   "source": [
    "# Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a0dda9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,input_size, output_size, hidden_size, num_layers):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        #layer 1\n",
    "        self.lstm = nn.LSTM(self.input_size, \n",
    "                            self.hidden_size,\n",
    "                            self.num_layers,\n",
    "                            batch_first=False,dropout = 0)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        #layer 2\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "      \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "     \n",
    "    def forward(self, x):     \n",
    "        per_out=[]\n",
    "        lstm_out, self.hidden_cell = self.lstm(x)\n",
    "        per_out.append(lstm_out)\n",
    "        lstm_out = self.dropout(lstm_out)    \n",
    "        out = self.fc(lstm_out)\n",
    "        per_out.append(out)\n",
    "        score = self.sigmoid(out)\n",
    "        return score, per_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cdf3a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(x,y,model):\n",
    "    output,pre_out = model(x)\n",
    "    output = torch.reshape(output,[-1,1])\n",
    "    correct = (output.ge(0.5) == y).sum().item()\n",
    "    n = y.shape[0]\n",
    "    return correct/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1c2a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 77\n",
    "output_size = 1\n",
    "hidden_size = 20\n",
    "num_layers =2\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = LSTM(input_size,output_size, hidden_size, num_layers).to(device)\n",
    "loss_function = nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "# optimizer = torch.optim.SGD(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "29d31a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataloader(X_train_full,y_train,X_valid_full,y_valid):\n",
    "    TIME_STEPS = 1\n",
    "    X_train = pd.DataFrame(X_train_full)\n",
    "    X_train = create_dataset(X_train, TIME_STEPS)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_train = create_dataset(y_train, TIME_STEPS)\n",
    "    X_valid = pd.DataFrame(X_valid_full)\n",
    "    X_valid = create_dataset(X_valid, TIME_STEPS)\n",
    "    y_valid = pd.DataFrame(y_valid)\n",
    "    y_valid = create_dataset(y_valid, TIME_STEPS)\n",
    "# 将输入和输出封装进Data.TensorDataset()类对象\n",
    "    x = torch.tensor(X_train[:,:,:]).float()\n",
    "    x = torch.where(torch.isnan(x), torch.full_like(x, 0), x)\n",
    "    y = y_train[:]\n",
    "    y = np.array(y)\n",
    "# y = torch.tensor(np.reshape(y,[-1,1]))\n",
    "    y = torch.tensor(y)\n",
    "    y = y.float()\n",
    "    torch_dataset = Data.TensorDataset(x,y)\n",
    "    train_dataloader = torch.utils.data.DataLoader(torch_dataset, batch_size=60, shuffle=False, num_workers=0)\n",
    "    return X_train, y_train, X_valid, y_valid, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4551c23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13495, 1, 77])\n",
      "torch.Size([13495, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as Data\n",
    "# 将输入和输出封装进Data.TensorDataset()类对象\n",
    "x = torch.tensor(X_train[:,:,:]).float()\n",
    "x = torch.where(torch.isnan(x), torch.full_like(x, 0), x)\n",
    "    \n",
    "y = y_train[:]\n",
    "y = np.array(y)\n",
    "# y = torch.tensor(np.reshape(y,[-1,1]))\n",
    "y = torch.tensor(y)\n",
    "y = y.float()\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "torch_dataset = Data.TensorDataset(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cbfee787",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 60\n",
    "train_dataloader = torch.utils.data.DataLoader(torch_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "404f4657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f9ece864",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_array = np.zeros(len(train_dataloader))\n",
    "batch_index_array = np.zeros(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756856f",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0ec2e670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch:0 loss:0.04436497390270233 acc:1.0\n",
      "epoch:0 batch:1 loss:0.04692162200808525 acc:1.0\n",
      "epoch:0 batch:2 loss:0.03874116390943527 acc:1.0\n",
      "epoch:0 batch:3 loss:0.04352283477783203 acc:1.0\n",
      "epoch:0 batch:4 loss:0.045079655945301056 acc:1.0\n",
      "epoch:0 batch:5 loss:0.04476911947131157 acc:1.0\n",
      "epoch:0 batch:6 loss:0.054479531943798065 acc:1.0\n",
      "epoch:0 batch:7 loss:0.03830759599804878 acc:1.0\n",
      "epoch:0 batch:8 loss:0.04475592076778412 acc:1.0\n",
      "epoch:0 batch:9 loss:0.04085180163383484 acc:1.0\n",
      "epoch:0 batch:10 loss:0.04259198158979416 acc:1.0\n",
      "epoch:0 batch:11 loss:0.04790929704904556 acc:1.0\n",
      "epoch:0 batch:12 loss:0.0399688184261322 acc:1.0\n",
      "epoch:0 batch:13 loss:0.04109927639365196 acc:1.0\n",
      "epoch:0 batch:14 loss:0.043490130454301834 acc:1.0\n",
      "epoch:0 batch:15 loss:0.04520556703209877 acc:1.0\n",
      "epoch:0 batch:16 loss:0.04810512438416481 acc:1.0\n",
      "epoch:0 batch:17 loss:0.04341646283864975 acc:1.0\n",
      "epoch:0 batch:18 loss:0.05030884966254234 acc:1.0\n",
      "epoch:0 batch:19 loss:0.04468575492501259 acc:1.0\n",
      "epoch:0 batch:20 loss:0.04994979873299599 acc:1.0\n",
      "epoch:0 batch:21 loss:0.04427054896950722 acc:1.0\n",
      "epoch:0 batch:22 loss:0.04871879890561104 acc:1.0\n",
      "epoch:0 batch:23 loss:0.05243100970983505 acc:1.0\n",
      "epoch:0 batch:24 loss:0.057111892849206924 acc:1.0\n",
      "epoch:0 batch:25 loss:0.057072870433330536 acc:1.0\n",
      "epoch:0 batch:26 loss:0.06321211159229279 acc:1.0\n",
      "epoch:0 batch:27 loss:0.05731271579861641 acc:1.0\n",
      "epoch:0 batch:28 loss:0.05640336871147156 acc:1.0\n",
      "epoch:0 batch:29 loss:0.054180487990379333 acc:1.0\n",
      "epoch:0 batch:30 loss:0.06327847391366959 acc:1.0\n",
      "epoch:0 batch:31 loss:0.05929894372820854 acc:1.0\n",
      "epoch:0 batch:32 loss:0.06174338236451149 acc:1.0\n",
      "epoch:0 batch:33 loss:0.05621987208724022 acc:1.0\n",
      "epoch:0 batch:34 loss:0.05866659805178642 acc:1.0\n",
      "epoch:0 batch:35 loss:0.051390182226896286 acc:1.0\n",
      "epoch:0 batch:36 loss:0.049326468259096146 acc:1.0\n",
      "epoch:0 batch:37 loss:0.05267737805843353 acc:1.0\n",
      "epoch:0 batch:38 loss:0.05267877131700516 acc:1.0\n",
      "epoch:0 batch:39 loss:0.05535023659467697 acc:1.0\n",
      "epoch:0 batch:40 loss:0.05659772828221321 acc:1.0\n",
      "epoch:0 batch:41 loss:0.0521276518702507 acc:1.0\n",
      "epoch:0 batch:42 loss:0.04813088849186897 acc:1.0\n",
      "epoch:0 batch:43 loss:0.04484623670578003 acc:1.0\n",
      "epoch:0 batch:44 loss:0.048956356942653656 acc:1.0\n",
      "epoch:0 batch:45 loss:0.04020879417657852 acc:1.0\n",
      "epoch:0 batch:46 loss:0.04177357256412506 acc:1.0\n",
      "epoch:0 batch:47 loss:0.043958842754364014 acc:1.0\n",
      "epoch:0 batch:48 loss:0.046845223754644394 acc:1.0\n",
      "epoch:0 batch:49 loss:0.04248742386698723 acc:1.0\n",
      "epoch:0 batch:50 loss:0.04611261934041977 acc:1.0\n",
      "epoch:0 batch:51 loss:0.04253970831632614 acc:1.0\n",
      "epoch:0 batch:52 loss:0.04741626977920532 acc:1.0\n",
      "epoch:0 batch:53 loss:0.039117567241191864 acc:1.0\n",
      "epoch:0 batch:54 loss:0.03706653416156769 acc:1.0\n",
      "epoch:0 batch:55 loss:0.03400365635752678 acc:1.0\n",
      "epoch:0 batch:56 loss:0.04201822355389595 acc:1.0\n",
      "epoch:0 batch:57 loss:0.037419311702251434 acc:1.0\n",
      "epoch:0 batch:58 loss:0.038855984807014465 acc:1.0\n",
      "epoch:0 batch:59 loss:0.03605388477444649 acc:1.0\n",
      "epoch:0 batch:60 loss:0.039949025958776474 acc:1.0\n",
      "epoch:0 batch:61 loss:0.037106264382600784 acc:1.0\n",
      "epoch:0 batch:62 loss:0.04012807831168175 acc:1.0\n",
      "epoch:0 batch:63 loss:0.03757277503609657 acc:1.0\n",
      "epoch:0 batch:64 loss:0.03869054466485977 acc:1.0\n",
      "epoch:0 batch:65 loss:0.038378361612558365 acc:1.0\n",
      "epoch:0 batch:66 loss:0.03425484150648117 acc:1.0\n",
      "epoch:0 batch:67 loss:0.04192326217889786 acc:1.0\n",
      "epoch:0 batch:68 loss:0.048001036047935486 acc:1.0\n",
      "epoch:0 batch:69 loss:0.046516042202711105 acc:1.0\n",
      "epoch:0 batch:70 loss:0.04374103620648384 acc:1.0\n",
      "epoch:0 batch:71 loss:0.04720182344317436 acc:1.0\n",
      "epoch:0 batch:72 loss:0.04673538729548454 acc:1.0\n",
      "epoch:0 batch:73 loss:0.04415120556950569 acc:1.0\n",
      "epoch:0 batch:74 loss:0.048973627388477325 acc:1.0\n",
      "epoch:0 batch:75 loss:0.03728676587343216 acc:1.0\n",
      "epoch:0 batch:76 loss:0.047490719705820084 acc:1.0\n",
      "epoch:0 batch:77 loss:0.040620725601911545 acc:1.0\n",
      "epoch:0 batch:78 loss:0.04763726517558098 acc:1.0\n",
      "epoch:0 batch:79 loss:0.04590308293700218 acc:1.0\n",
      "epoch:0 batch:80 loss:0.04165313392877579 acc:1.0\n",
      "epoch:0 batch:81 loss:0.043427977710962296 acc:1.0\n",
      "epoch:0 batch:82 loss:0.04337889701128006 acc:1.0\n",
      "epoch:0 batch:83 loss:0.04491129890084267 acc:1.0\n",
      "epoch:0 batch:84 loss:0.03970922529697418 acc:1.0\n",
      "epoch:0 batch:85 loss:0.03957854211330414 acc:1.0\n",
      "epoch:0 batch:86 loss:0.050231508910655975 acc:1.0\n",
      "epoch:0 batch:87 loss:0.04734556004405022 acc:1.0\n",
      "epoch:0 batch:88 loss:0.04477403312921524 acc:1.0\n",
      "epoch:0 batch:89 loss:0.048162270337343216 acc:1.0\n",
      "epoch:0 batch:90 loss:0.037875305861234665 acc:1.0\n",
      "epoch:0 batch:91 loss:0.03584101423621178 acc:1.0\n",
      "epoch:0 batch:92 loss:0.040172841399908066 acc:1.0\n",
      "epoch:0 batch:93 loss:0.037125635892152786 acc:1.0\n",
      "epoch:0 batch:94 loss:0.034545060247182846 acc:1.0\n",
      "epoch:0 batch:95 loss:0.033181868493556976 acc:1.0\n",
      "epoch:0 batch:96 loss:0.03643632307648659 acc:1.0\n",
      "epoch:0 batch:97 loss:0.03792143613100052 acc:1.0\n",
      "epoch:0 batch:98 loss:0.033180125057697296 acc:1.0\n",
      "epoch:0 batch:99 loss:0.03288889676332474 acc:1.0\n",
      "epoch:0 batch:100 loss:0.03624541312456131 acc:1.0\n",
      "epoch:0 batch:101 loss:0.035189129412174225 acc:1.0\n",
      "epoch:0 batch:102 loss:0.031223442405462265 acc:1.0\n",
      "epoch:0 batch:103 loss:0.03032214753329754 acc:1.0\n",
      "epoch:0 batch:104 loss:0.03087749518454075 acc:1.0\n",
      "epoch:0 batch:105 loss:0.03387624770402908 acc:1.0\n",
      "epoch:0 batch:106 loss:0.03252224624156952 acc:1.0\n",
      "epoch:0 batch:107 loss:0.03789165988564491 acc:1.0\n",
      "epoch:0 batch:108 loss:0.028587985783815384 acc:1.0\n",
      "epoch:0 batch:109 loss:0.03349929675459862 acc:1.0\n",
      "epoch:0 batch:110 loss:0.027017582207918167 acc:1.0\n",
      "epoch:0 batch:111 loss:0.029047681018710136 acc:1.0\n",
      "epoch:0 batch:112 loss:0.04086368530988693 acc:1.0\n",
      "epoch:0 batch:113 loss:0.043473076075315475 acc:1.0\n",
      "epoch:0 batch:114 loss:0.040620218962430954 acc:1.0\n",
      "epoch:0 batch:115 loss:0.04078812152147293 acc:1.0\n",
      "epoch:0 batch:116 loss:0.03926077112555504 acc:1.0\n",
      "epoch:0 batch:117 loss:0.03940505161881447 acc:1.0\n",
      "epoch:0 batch:118 loss:0.03966151177883148 acc:1.0\n",
      "epoch:0 batch:119 loss:0.03712190315127373 acc:1.0\n",
      "epoch:0 batch:120 loss:0.03842921555042267 acc:1.0\n",
      "epoch:0 batch:121 loss:0.03338627144694328 acc:1.0\n",
      "epoch:0 batch:122 loss:0.03406809642910957 acc:1.0\n",
      "epoch:0 batch:123 loss:0.04313052445650101 acc:1.0\n",
      "epoch:0 batch:124 loss:0.03867198899388313 acc:1.0\n",
      "epoch:0 batch:125 loss:0.03887118026614189 acc:1.0\n",
      "epoch:0 batch:126 loss:0.03221806138753891 acc:1.0\n",
      "epoch:0 batch:127 loss:0.031038057059049606 acc:1.0\n",
      "epoch:0 batch:128 loss:0.038570016622543335 acc:1.0\n",
      "epoch:0 batch:129 loss:0.03257239609956741 acc:1.0\n",
      "epoch:0 batch:130 loss:0.030355248600244522 acc:1.0\n",
      "epoch:0 batch:131 loss:0.036554966121912 acc:1.0\n",
      "epoch:0 batch:132 loss:0.038687460124492645 acc:1.0\n",
      "epoch:0 batch:133 loss:0.033872175961732864 acc:1.0\n",
      "epoch:0 batch:134 loss:0.033222950994968414 acc:1.0\n",
      "epoch:0 batch:135 loss:0.03443913534283638 acc:1.0\n",
      "epoch:0 batch:136 loss:0.03724764287471771 acc:1.0\n",
      "epoch:0 batch:137 loss:0.030124690383672714 acc:1.0\n",
      "epoch:0 batch:138 loss:0.03452012315392494 acc:1.0\n",
      "epoch:0 batch:139 loss:0.03198375925421715 acc:1.0\n",
      "epoch:0 batch:140 loss:0.032720714807510376 acc:1.0\n",
      "epoch:0 batch:141 loss:0.03193444386124611 acc:1.0\n",
      "epoch:0 batch:142 loss:0.03402641788125038 acc:1.0\n",
      "epoch:0 batch:143 loss:0.027633339166641235 acc:1.0\n",
      "epoch:0 batch:144 loss:0.026465140283107758 acc:1.0\n",
      "epoch:0 batch:145 loss:0.02959527261555195 acc:1.0\n",
      "epoch:0 batch:146 loss:0.024782337248325348 acc:1.0\n",
      "epoch:0 batch:147 loss:0.02865365520119667 acc:1.0\n",
      "epoch:0 batch:148 loss:0.026434659957885742 acc:1.0\n",
      "epoch:0 batch:149 loss:0.02518705651164055 acc:1.0\n",
      "epoch:0 batch:150 loss:0.024962184950709343 acc:1.0\n",
      "epoch:0 batch:151 loss:0.025392519310116768 acc:1.0\n",
      "epoch:0 batch:152 loss:0.032891351729631424 acc:1.0\n",
      "epoch:0 batch:153 loss:0.029349444434046745 acc:1.0\n",
      "epoch:0 batch:154 loss:0.024856973439455032 acc:1.0\n",
      "epoch:0 batch:155 loss:0.025990964844822884 acc:1.0\n",
      "epoch:0 batch:156 loss:0.030047988519072533 acc:1.0\n",
      "epoch:0 batch:157 loss:0.035160090774297714 acc:1.0\n",
      "epoch:0 batch:158 loss:1.1244239807128906 acc:0.7166666666666667\n",
      "epoch:0 batch:159 loss:3.650782823562622 acc:0.0\n",
      "epoch:0 batch:160 loss:1.6991469860076904 acc:0.5333333333333333\n",
      "epoch:0 batch:161 loss:0.03881775587797165 acc:1.0\n",
      "epoch:0 batch:162 loss:0.03883694112300873 acc:1.0\n",
      "epoch:0 batch:163 loss:0.04145321622490883 acc:1.0\n",
      "epoch:0 batch:164 loss:0.21472996473312378 acc:0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch:165 loss:3.4479479789733887 acc:0.0\n",
      "epoch:0 batch:166 loss:2.128584384918213 acc:0.0\n",
      "epoch:0 batch:167 loss:1.0094629526138306 acc:0.38333333333333336\n",
      "epoch:0 batch:168 loss:1.9934817552566528 acc:0.0\n",
      "epoch:0 batch:169 loss:0.8557331562042236 acc:0.48333333333333334\n",
      "epoch:0 batch:170 loss:1.113670825958252 acc:0.1\n",
      "epoch:0 batch:171 loss:1.3217976093292236 acc:0.06666666666666667\n",
      "epoch:0 batch:172 loss:1.3754130601882935 acc:0.08333333333333333\n",
      "epoch:0 batch:173 loss:1.2440848350524902 acc:0.06666666666666667\n",
      "epoch:0 batch:174 loss:1.0962738990783691 acc:0.05\n",
      "epoch:0 batch:175 loss:1.7937302589416504 acc:0.06666666666666667\n",
      "epoch:0 batch:176 loss:1.7577645778656006 acc:0.1\n",
      "epoch:0 batch:177 loss:0.17321926355361938 acc:1.0\n",
      "epoch:0 batch:178 loss:0.18668527901172638 acc:1.0\n",
      "epoch:0 batch:179 loss:0.23137891292572021 acc:1.0\n",
      "epoch:0 batch:180 loss:0.6232397556304932 acc:0.7333333333333333\n",
      "epoch:0 batch:181 loss:1.7124347686767578 acc:0.0\n",
      "epoch:0 batch:182 loss:1.6460074186325073 acc:0.0\n",
      "epoch:0 batch:183 loss:1.5801140069961548 acc:0.0\n",
      "epoch:0 batch:184 loss:1.7564570903778076 acc:0.0\n",
      "epoch:0 batch:185 loss:1.6732492446899414 acc:0.016666666666666666\n",
      "epoch:0 batch:186 loss:1.6000179052352905 acc:0.016666666666666666\n",
      "epoch:0 batch:187 loss:1.4799034595489502 acc:0.0\n",
      "epoch:0 batch:188 loss:1.1512103080749512 acc:0.21666666666666667\n",
      "epoch:0 batch:189 loss:0.29591405391693115 acc:0.9833333333333333\n",
      "epoch:0 batch:190 loss:0.31803205609321594 acc:1.0\n",
      "epoch:0 batch:191 loss:0.32095155119895935 acc:1.0\n",
      "epoch:0 batch:192 loss:0.3303281366825104 acc:1.0\n",
      "epoch:0 batch:193 loss:0.33556288480758667 acc:1.0\n",
      "epoch:0 batch:194 loss:0.3387129306793213 acc:1.0\n",
      "epoch:0 batch:195 loss:0.3491360545158386 acc:0.9833333333333333\n",
      "epoch:0 batch:196 loss:0.990046501159668 acc:0.26666666666666666\n",
      "epoch:0 batch:197 loss:1.218529224395752 acc:0.016666666666666666\n",
      "epoch:0 batch:198 loss:1.1599482297897339 acc:0.016666666666666666\n",
      "epoch:0 batch:199 loss:0.8228018879890442 acc:0.23333333333333334\n",
      "epoch:0 batch:200 loss:0.7665025591850281 acc:0.26666666666666666\n",
      "epoch:0 batch:201 loss:0.7513486742973328 acc:0.35\n",
      "epoch:0 batch:202 loss:1.0638223886489868 acc:0.0\n",
      "epoch:0 batch:203 loss:0.7168490886688232 acc:0.5333333333333333\n",
      "epoch:0 batch:204 loss:0.6786009669303894 acc:0.6333333333333333\n",
      "epoch:0 batch:205 loss:0.7268105149269104 acc:0.31666666666666665\n",
      "epoch:0 batch:206 loss:0.622360110282898 acc:0.7666666666666667\n",
      "epoch:0 batch:207 loss:0.6278504729270935 acc:0.8166666666666667\n",
      "epoch:0 batch:208 loss:0.5827698111534119 acc:0.7666666666666667\n",
      "epoch:0 batch:209 loss:0.58741295337677 acc:0.85\n",
      "epoch:0 batch:210 loss:0.5544885396957397 acc:0.9\n",
      "epoch:0 batch:211 loss:0.5935065150260925 acc:0.7166666666666667\n",
      "epoch:0 batch:212 loss:0.9848170876502991 acc:0.21666666666666667\n",
      "epoch:0 batch:213 loss:0.4249906539916992 acc:0.9166666666666666\n",
      "epoch:0 batch:214 loss:0.43312233686447144 acc:0.9333333333333333\n",
      "epoch:0 batch:215 loss:0.42766091227531433 acc:0.9333333333333333\n",
      "epoch:0 batch:216 loss:0.41271987557411194 acc:0.9166666666666666\n",
      "epoch:0 batch:217 loss:0.41705477237701416 acc:0.9166666666666666\n",
      "epoch:0 batch:218 loss:0.407585471868515 acc:0.9666666666666667\n",
      "epoch:0 batch:219 loss:0.4051671028137207 acc:0.9666666666666667\n",
      "epoch:0 batch:220 loss:0.40691742300987244 acc:0.9833333333333333\n",
      "epoch:0 batch:221 loss:0.399565726518631 acc:0.9666666666666667\n",
      "epoch:0 batch:222 loss:0.4028993844985962 acc:0.9666666666666667\n",
      "epoch:0 batch:223 loss:0.37231752276420593 acc:0.9833333333333333\n",
      "epoch:0 batch:224 loss:0.38241225481033325 acc:1.0\n",
      "epoch:5 batch:0 loss:0.023656919598579407 acc:1.0\n",
      "epoch:5 batch:1 loss:0.025723647326231003 acc:1.0\n",
      "epoch:5 batch:2 loss:0.019687673076987267 acc:1.0\n",
      "epoch:5 batch:3 loss:0.020462462678551674 acc:1.0\n",
      "epoch:5 batch:4 loss:0.023195601999759674 acc:1.0\n",
      "epoch:5 batch:5 loss:0.021076267585158348 acc:1.0\n",
      "epoch:5 batch:6 loss:0.02761247381567955 acc:1.0\n",
      "epoch:5 batch:7 loss:0.021214015781879425 acc:1.0\n",
      "epoch:5 batch:8 loss:0.022764621302485466 acc:1.0\n",
      "epoch:5 batch:9 loss:0.017493946477770805 acc:1.0\n",
      "epoch:5 batch:10 loss:0.021281998604536057 acc:1.0\n",
      "epoch:5 batch:11 loss:0.020302344113588333 acc:1.0\n",
      "epoch:5 batch:12 loss:0.017816511914134026 acc:1.0\n",
      "epoch:5 batch:13 loss:0.019748220220208168 acc:1.0\n",
      "epoch:5 batch:14 loss:0.020386558026075363 acc:1.0\n",
      "epoch:5 batch:15 loss:0.020815705880522728 acc:1.0\n",
      "epoch:5 batch:16 loss:0.02556891180574894 acc:1.0\n",
      "epoch:5 batch:17 loss:0.026451151818037033 acc:1.0\n",
      "epoch:5 batch:18 loss:0.024273114278912544 acc:1.0\n",
      "epoch:5 batch:19 loss:0.02306218631565571 acc:1.0\n",
      "epoch:5 batch:20 loss:0.02352220192551613 acc:1.0\n",
      "epoch:5 batch:21 loss:0.024793090298771858 acc:1.0\n",
      "epoch:5 batch:22 loss:0.02571684680879116 acc:1.0\n",
      "epoch:5 batch:23 loss:0.08404332399368286 acc:1.0\n",
      "epoch:5 batch:24 loss:0.10002505779266357 acc:1.0\n",
      "epoch:5 batch:25 loss:0.08405188471078873 acc:1.0\n",
      "epoch:5 batch:26 loss:0.09439221024513245 acc:1.0\n",
      "epoch:5 batch:27 loss:0.09585991501808167 acc:1.0\n",
      "epoch:5 batch:28 loss:0.10187079757452011 acc:1.0\n",
      "epoch:5 batch:29 loss:0.10209091752767563 acc:1.0\n",
      "epoch:5 batch:30 loss:0.08220324665307999 acc:1.0\n",
      "epoch:5 batch:31 loss:0.09991422295570374 acc:1.0\n",
      "epoch:5 batch:32 loss:0.08593656122684479 acc:1.0\n",
      "epoch:5 batch:33 loss:0.09409952908754349 acc:1.0\n",
      "epoch:5 batch:34 loss:0.09138967841863632 acc:1.0\n",
      "epoch:5 batch:35 loss:0.10273303091526031 acc:1.0\n",
      "epoch:5 batch:36 loss:0.09254792332649231 acc:1.0\n",
      "epoch:5 batch:37 loss:0.08380188792943954 acc:1.0\n",
      "epoch:5 batch:38 loss:0.09099207073450089 acc:1.0\n",
      "epoch:5 batch:39 loss:0.08561401814222336 acc:1.0\n",
      "epoch:5 batch:40 loss:0.09681883454322815 acc:1.0\n",
      "epoch:5 batch:41 loss:0.08464840799570084 acc:1.0\n",
      "epoch:5 batch:42 loss:0.09464351832866669 acc:1.0\n",
      "epoch:5 batch:43 loss:0.08519001305103302 acc:1.0\n",
      "epoch:5 batch:44 loss:0.06588727980852127 acc:1.0\n",
      "epoch:5 batch:45 loss:0.024150362238287926 acc:1.0\n",
      "epoch:5 batch:46 loss:0.023184364661574364 acc:1.0\n",
      "epoch:5 batch:47 loss:0.024329131469130516 acc:1.0\n",
      "epoch:5 batch:48 loss:0.025558164343237877 acc:1.0\n",
      "epoch:5 batch:49 loss:0.024259274825453758 acc:1.0\n",
      "epoch:5 batch:50 loss:0.01839246042072773 acc:1.0\n",
      "epoch:5 batch:51 loss:0.023226413875818253 acc:1.0\n",
      "epoch:5 batch:52 loss:0.025951851159334183 acc:1.0\n",
      "epoch:5 batch:53 loss:0.025097090750932693 acc:1.0\n",
      "epoch:5 batch:54 loss:0.025076311081647873 acc:1.0\n",
      "epoch:5 batch:55 loss:0.019933566451072693 acc:1.0\n",
      "epoch:5 batch:56 loss:0.021723896265029907 acc:1.0\n",
      "epoch:5 batch:57 loss:0.018441425636410713 acc:1.0\n",
      "epoch:5 batch:58 loss:0.021146662533283234 acc:1.0\n",
      "epoch:5 batch:59 loss:0.01891671121120453 acc:1.0\n",
      "epoch:5 batch:60 loss:0.02038935199379921 acc:1.0\n",
      "epoch:5 batch:61 loss:0.019627535715699196 acc:1.0\n",
      "epoch:5 batch:62 loss:0.02614021487534046 acc:1.0\n",
      "epoch:5 batch:63 loss:0.020759543403983116 acc:1.0\n",
      "epoch:5 batch:64 loss:0.020396599546074867 acc:1.0\n",
      "epoch:5 batch:65 loss:0.01919473335146904 acc:1.0\n",
      "epoch:5 batch:66 loss:0.021655799821019173 acc:1.0\n",
      "epoch:5 batch:67 loss:0.04672832787036896 acc:1.0\n",
      "epoch:5 batch:68 loss:0.07660495489835739 acc:1.0\n",
      "epoch:5 batch:69 loss:0.09389431774616241 acc:1.0\n",
      "epoch:5 batch:70 loss:0.0777483731508255 acc:1.0\n",
      "epoch:5 batch:71 loss:0.07393670827150345 acc:1.0\n",
      "epoch:5 batch:72 loss:0.07579509913921356 acc:1.0\n",
      "epoch:5 batch:73 loss:0.07721226662397385 acc:1.0\n",
      "epoch:5 batch:74 loss:0.08220621943473816 acc:1.0\n",
      "epoch:5 batch:75 loss:0.08354112505912781 acc:1.0\n",
      "epoch:5 batch:76 loss:0.0881999209523201 acc:1.0\n",
      "epoch:5 batch:77 loss:0.07007753103971481 acc:1.0\n",
      "epoch:5 batch:78 loss:0.08075915277004242 acc:1.0\n",
      "epoch:5 batch:79 loss:0.06572508066892624 acc:1.0\n",
      "epoch:5 batch:80 loss:0.0801563486456871 acc:1.0\n",
      "epoch:5 batch:81 loss:0.08097313344478607 acc:1.0\n",
      "epoch:5 batch:82 loss:0.07359239459037781 acc:1.0\n",
      "epoch:5 batch:83 loss:0.07446326315402985 acc:1.0\n",
      "epoch:5 batch:84 loss:0.07000789791345596 acc:1.0\n",
      "epoch:5 batch:85 loss:0.08496111631393433 acc:1.0\n",
      "epoch:5 batch:86 loss:0.07329005002975464 acc:1.0\n",
      "epoch:5 batch:87 loss:0.06419968605041504 acc:1.0\n",
      "epoch:5 batch:88 loss:0.08901674300432205 acc:1.0\n",
      "epoch:5 batch:89 loss:0.029177753254771233 acc:1.0\n",
      "epoch:5 batch:90 loss:0.026736903935670853 acc:1.0\n",
      "epoch:5 batch:91 loss:0.02860305644571781 acc:1.0\n",
      "epoch:5 batch:92 loss:0.024728016927838326 acc:1.0\n",
      "epoch:5 batch:93 loss:0.023951455950737 acc:1.0\n",
      "epoch:5 batch:94 loss:0.01941787637770176 acc:1.0\n",
      "epoch:5 batch:95 loss:0.021031498908996582 acc:1.0\n",
      "epoch:5 batch:96 loss:0.023842250928282738 acc:1.0\n",
      "epoch:5 batch:97 loss:0.019717125222086906 acc:1.0\n",
      "epoch:5 batch:98 loss:0.02262798324227333 acc:1.0\n",
      "epoch:5 batch:99 loss:0.01696236990392208 acc:1.0\n",
      "epoch:5 batch:100 loss:0.019660886377096176 acc:1.0\n",
      "epoch:5 batch:101 loss:0.022654056549072266 acc:1.0\n",
      "epoch:5 batch:102 loss:0.018002737313508987 acc:1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 batch:103 loss:0.020581932738423347 acc:1.0\n",
      "epoch:5 batch:104 loss:0.0168137326836586 acc:1.0\n",
      "epoch:5 batch:105 loss:0.02125728502869606 acc:1.0\n",
      "epoch:5 batch:106 loss:0.020029447972774506 acc:1.0\n",
      "epoch:5 batch:107 loss:0.019032305106520653 acc:1.0\n",
      "epoch:5 batch:108 loss:0.020784849300980568 acc:1.0\n",
      "epoch:5 batch:109 loss:0.01696639321744442 acc:1.0\n",
      "epoch:5 batch:110 loss:0.018230248242616653 acc:1.0\n",
      "epoch:5 batch:111 loss:0.019445622339844704 acc:1.0\n",
      "epoch:5 batch:112 loss:0.057797499001026154 acc:1.0\n",
      "epoch:5 batch:113 loss:0.07462543249130249 acc:1.0\n",
      "epoch:5 batch:114 loss:0.07209409773349762 acc:1.0\n",
      "epoch:5 batch:115 loss:0.07575671374797821 acc:1.0\n",
      "epoch:5 batch:116 loss:0.06120050325989723 acc:1.0\n",
      "epoch:5 batch:117 loss:0.06134217977523804 acc:1.0\n",
      "epoch:5 batch:118 loss:0.06808482855558395 acc:1.0\n",
      "epoch:5 batch:119 loss:0.06441875547170639 acc:1.0\n",
      "epoch:5 batch:120 loss:0.06349940598011017 acc:1.0\n",
      "epoch:5 batch:121 loss:0.057994991540908813 acc:1.0\n",
      "epoch:5 batch:122 loss:0.07442287355661392 acc:1.0\n",
      "epoch:5 batch:123 loss:0.06869009882211685 acc:1.0\n",
      "epoch:5 batch:124 loss:0.06771409511566162 acc:1.0\n",
      "epoch:5 batch:125 loss:0.06282562017440796 acc:1.0\n",
      "epoch:5 batch:126 loss:0.06503745913505554 acc:1.0\n",
      "epoch:5 batch:127 loss:0.06164634972810745 acc:1.0\n",
      "epoch:5 batch:128 loss:0.06841963529586792 acc:1.0\n",
      "epoch:5 batch:129 loss:0.06517791748046875 acc:1.0\n",
      "epoch:5 batch:130 loss:0.059366095811128616 acc:1.0\n",
      "epoch:5 batch:131 loss:0.0648651123046875 acc:1.0\n",
      "epoch:5 batch:132 loss:0.06465255469083786 acc:1.0\n",
      "epoch:5 batch:133 loss:0.05777118355035782 acc:1.0\n",
      "epoch:5 batch:134 loss:0.017763260751962662 acc:1.0\n",
      "epoch:5 batch:135 loss:0.021219290792942047 acc:1.0\n",
      "epoch:5 batch:136 loss:0.022532396018505096 acc:1.0\n",
      "epoch:5 batch:137 loss:0.027813676744699478 acc:1.0\n",
      "epoch:5 batch:138 loss:0.022815920412540436 acc:1.0\n",
      "epoch:5 batch:139 loss:0.01980995386838913 acc:1.0\n",
      "epoch:5 batch:140 loss:0.023206660524010658 acc:1.0\n",
      "epoch:5 batch:141 loss:0.019394421949982643 acc:1.0\n",
      "epoch:5 batch:142 loss:0.018772853538393974 acc:1.0\n",
      "epoch:5 batch:143 loss:0.019057361409068108 acc:1.0\n",
      "epoch:5 batch:144 loss:0.02064921334385872 acc:1.0\n",
      "epoch:5 batch:145 loss:0.02167830802500248 acc:1.0\n",
      "epoch:5 batch:146 loss:0.016594912856817245 acc:1.0\n",
      "epoch:5 batch:147 loss:0.016857830807566643 acc:1.0\n",
      "epoch:5 batch:148 loss:0.018551474437117577 acc:1.0\n",
      "epoch:5 batch:149 loss:0.016021307557821274 acc:1.0\n",
      "epoch:5 batch:150 loss:0.01944386586546898 acc:1.0\n",
      "epoch:5 batch:151 loss:0.01839115470647812 acc:1.0\n",
      "epoch:5 batch:152 loss:0.016421034932136536 acc:1.0\n",
      "epoch:5 batch:153 loss:0.01798553392291069 acc:1.0\n",
      "epoch:5 batch:154 loss:0.017413564026355743 acc:1.0\n",
      "epoch:5 batch:155 loss:0.018573250621557236 acc:1.0\n",
      "epoch:5 batch:156 loss:0.018321631476283073 acc:1.0\n",
      "epoch:5 batch:157 loss:0.05764453858137131 acc:1.0\n",
      "epoch:5 batch:158 loss:0.9409223198890686 acc:0.7166666666666667\n",
      "epoch:5 batch:159 loss:3.031801700592041 acc:0.0\n",
      "epoch:5 batch:160 loss:1.4512556791305542 acc:0.5333333333333333\n",
      "epoch:5 batch:161 loss:0.06024140119552612 acc:1.0\n",
      "epoch:5 batch:162 loss:0.06198442354798317 acc:1.0\n",
      "epoch:5 batch:163 loss:0.06384918093681335 acc:1.0\n",
      "epoch:5 batch:164 loss:0.22128300368785858 acc:0.95\n",
      "epoch:5 batch:165 loss:2.853222131729126 acc:0.0\n",
      "epoch:5 batch:166 loss:3.088613986968994 acc:0.0\n",
      "epoch:5 batch:167 loss:3.013446092605591 acc:0.0\n",
      "epoch:5 batch:168 loss:2.9462430477142334 acc:0.0\n",
      "epoch:5 batch:169 loss:1.410433053970337 acc:0.5166666666666667\n",
      "epoch:5 batch:170 loss:0.06683111190795898 acc:1.0\n",
      "epoch:5 batch:171 loss:0.072828508913517 acc:1.0\n",
      "epoch:5 batch:172 loss:0.0851474180817604 acc:1.0\n",
      "epoch:5 batch:173 loss:0.07999156415462494 acc:1.0\n",
      "epoch:5 batch:174 loss:0.09347666054964066 acc:1.0\n",
      "epoch:5 batch:175 loss:0.09528596699237823 acc:1.0\n",
      "epoch:5 batch:176 loss:1.0573468208312988 acc:0.13333333333333333\n",
      "epoch:5 batch:177 loss:0.43499141931533813 acc:1.0\n",
      "epoch:5 batch:178 loss:0.43882879614830017 acc:1.0\n",
      "epoch:5 batch:179 loss:0.8445926904678345 acc:0.08333333333333333\n",
      "epoch:5 batch:180 loss:0.7415525317192078 acc:0.31666666666666665\n",
      "epoch:5 batch:181 loss:0.6299844980239868 acc:0.9333333333333333\n",
      "epoch:5 batch:182 loss:0.6419335007667542 acc:0.8333333333333334\n",
      "epoch:5 batch:183 loss:0.6187911033630371 acc:0.9\n",
      "epoch:5 batch:184 loss:0.15780295431613922 acc:1.0\n",
      "epoch:5 batch:185 loss:0.09003470838069916 acc:1.0\n",
      "epoch:5 batch:186 loss:0.0816909670829773 acc:1.0\n",
      "epoch:5 batch:187 loss:0.3408893644809723 acc:1.0\n",
      "epoch:5 batch:188 loss:0.6009911894798279 acc:0.7833333333333333\n",
      "epoch:5 batch:189 loss:0.6945149898529053 acc:0.6166666666666667\n",
      "epoch:5 batch:190 loss:0.4964905083179474 acc:0.9833333333333333\n",
      "epoch:5 batch:191 loss:0.49664512276649475 acc:0.9833333333333333\n",
      "epoch:5 batch:192 loss:0.5014644265174866 acc:0.9833333333333333\n",
      "epoch:5 batch:193 loss:0.5124472975730896 acc:0.9666666666666667\n",
      "epoch:5 batch:194 loss:0.6162315607070923 acc:0.8166666666666667\n",
      "epoch:5 batch:195 loss:1.8396778106689453 acc:0.0\n",
      "epoch:5 batch:196 loss:0.5767425894737244 acc:0.7166666666666667\n",
      "epoch:5 batch:197 loss:0.3884824216365814 acc:1.0\n",
      "epoch:5 batch:198 loss:0.280381441116333 acc:1.0\n",
      "epoch:5 batch:199 loss:0.07988955825567245 acc:1.0\n",
      "epoch:5 batch:200 loss:0.09772328287363052 acc:1.0\n",
      "epoch:5 batch:201 loss:0.11197780817747116 acc:1.0\n",
      "epoch:5 batch:202 loss:0.3773593604564667 acc:0.8\n",
      "epoch:5 batch:203 loss:0.419002503156662 acc:0.9666666666666667\n",
      "epoch:5 batch:204 loss:1.1368367671966553 acc:0.0\n",
      "epoch:5 batch:205 loss:1.8653626441955566 acc:0.03333333333333333\n",
      "epoch:5 batch:206 loss:0.11077471077442169 acc:1.0\n",
      "epoch:5 batch:207 loss:0.13273675739765167 acc:1.0\n",
      "epoch:5 batch:208 loss:0.11863445490598679 acc:1.0\n",
      "epoch:5 batch:209 loss:0.13850489258766174 acc:1.0\n",
      "epoch:5 batch:210 loss:0.12201826274394989 acc:1.0\n",
      "epoch:5 batch:211 loss:0.11692780256271362 acc:1.0\n",
      "epoch:5 batch:212 loss:1.7053049802780151 acc:0.15\n",
      "epoch:5 batch:213 loss:0.17170245945453644 acc:1.0\n",
      "epoch:5 batch:214 loss:0.18416984379291534 acc:0.9833333333333333\n",
      "epoch:5 batch:215 loss:0.1856282651424408 acc:0.9833333333333333\n",
      "epoch:5 batch:216 loss:0.16937297582626343 acc:0.9833333333333333\n",
      "epoch:5 batch:217 loss:0.1620669811964035 acc:1.0\n",
      "epoch:5 batch:218 loss:0.17221219837665558 acc:1.0\n",
      "epoch:5 batch:219 loss:0.17384129762649536 acc:0.9833333333333333\n",
      "epoch:5 batch:220 loss:0.16740013659000397 acc:1.0\n",
      "epoch:5 batch:221 loss:0.1481882482767105 acc:0.9833333333333333\n",
      "epoch:5 batch:222 loss:0.14922751486301422 acc:0.9833333333333333\n",
      "epoch:5 batch:223 loss:0.1583508849143982 acc:0.9833333333333333\n",
      "epoch:5 batch:224 loss:0.14958377182483673 acc:1.0\n",
      "epoch:10 batch:0 loss:0.009013792499899864 acc:1.0\n",
      "epoch:10 batch:1 loss:0.011800190433859825 acc:1.0\n",
      "epoch:10 batch:2 loss:0.010503665544092655 acc:1.0\n",
      "epoch:10 batch:3 loss:0.011807561852037907 acc:1.0\n",
      "epoch:10 batch:4 loss:0.011326558887958527 acc:1.0\n",
      "epoch:10 batch:5 loss:0.011050986126065254 acc:1.0\n",
      "epoch:10 batch:6 loss:0.013805598020553589 acc:1.0\n",
      "epoch:10 batch:7 loss:0.012143975123763084 acc:1.0\n",
      "epoch:10 batch:8 loss:0.01133885607123375 acc:1.0\n",
      "epoch:10 batch:9 loss:0.014931553974747658 acc:1.0\n",
      "epoch:10 batch:10 loss:0.009814100340008736 acc:1.0\n",
      "epoch:10 batch:11 loss:0.01133460458368063 acc:1.0\n",
      "epoch:10 batch:12 loss:0.010269644670188427 acc:1.0\n",
      "epoch:10 batch:13 loss:0.01000357698649168 acc:1.0\n",
      "epoch:10 batch:14 loss:0.00962157640606165 acc:1.0\n",
      "epoch:10 batch:15 loss:0.012348751537501812 acc:1.0\n",
      "epoch:10 batch:16 loss:0.011322342790663242 acc:1.0\n",
      "epoch:10 batch:17 loss:0.01369459182024002 acc:1.0\n",
      "epoch:10 batch:18 loss:0.011705290526151657 acc:1.0\n",
      "epoch:10 batch:19 loss:0.012878100387752056 acc:1.0\n",
      "epoch:10 batch:20 loss:0.014971010386943817 acc:1.0\n",
      "epoch:10 batch:21 loss:0.010886572301387787 acc:1.0\n",
      "epoch:10 batch:22 loss:0.012852754443883896 acc:1.0\n",
      "epoch:10 batch:23 loss:0.07533825933933258 acc:1.0\n",
      "epoch:10 batch:24 loss:0.0972374752163887 acc:1.0\n",
      "epoch:10 batch:25 loss:0.0886315405368805 acc:1.0\n",
      "epoch:10 batch:26 loss:0.0936865359544754 acc:1.0\n",
      "epoch:10 batch:27 loss:0.08261668682098389 acc:1.0\n",
      "epoch:10 batch:28 loss:0.09750550240278244 acc:1.0\n",
      "epoch:10 batch:29 loss:0.10071535408496857 acc:1.0\n",
      "epoch:10 batch:30 loss:0.10592330992221832 acc:1.0\n",
      "epoch:10 batch:31 loss:0.08461985737085342 acc:1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 batch:32 loss:0.09852023422718048 acc:1.0\n",
      "epoch:10 batch:33 loss:0.08019188046455383 acc:1.0\n",
      "epoch:10 batch:34 loss:0.0826493576169014 acc:1.0\n",
      "epoch:10 batch:35 loss:0.07875112444162369 acc:1.0\n",
      "epoch:10 batch:36 loss:0.0742906704545021 acc:1.0\n",
      "epoch:10 batch:37 loss:0.07848785817623138 acc:1.0\n",
      "epoch:10 batch:38 loss:0.08571828156709671 acc:1.0\n",
      "epoch:10 batch:39 loss:0.08352938294410706 acc:1.0\n",
      "epoch:10 batch:40 loss:0.08660920709371567 acc:1.0\n",
      "epoch:10 batch:41 loss:0.085704505443573 acc:1.0\n",
      "epoch:10 batch:42 loss:0.0798739492893219 acc:1.0\n",
      "epoch:10 batch:43 loss:0.07673852145671844 acc:1.0\n",
      "epoch:10 batch:44 loss:0.04420093074440956 acc:1.0\n",
      "epoch:10 batch:45 loss:0.014214790426194668 acc:1.0\n",
      "epoch:10 batch:46 loss:0.010610450059175491 acc:1.0\n",
      "epoch:10 batch:47 loss:0.015165833756327629 acc:1.0\n",
      "epoch:10 batch:48 loss:0.011932946741580963 acc:1.0\n",
      "epoch:10 batch:49 loss:0.012828364968299866 acc:1.0\n",
      "epoch:10 batch:50 loss:0.013858728110790253 acc:1.0\n",
      "epoch:10 batch:51 loss:0.011710096150636673 acc:1.0\n",
      "epoch:10 batch:52 loss:0.01332214567810297 acc:1.0\n",
      "epoch:10 batch:53 loss:0.014468776993453503 acc:1.0\n",
      "epoch:10 batch:54 loss:0.013399246148765087 acc:1.0\n",
      "epoch:10 batch:55 loss:0.011129941791296005 acc:1.0\n",
      "epoch:10 batch:56 loss:0.010259745642542839 acc:1.0\n",
      "epoch:10 batch:57 loss:0.011227397248148918 acc:1.0\n",
      "epoch:10 batch:58 loss:0.011631554923951626 acc:1.0\n",
      "epoch:10 batch:59 loss:0.012679808773100376 acc:1.0\n",
      "epoch:10 batch:60 loss:0.009836793877184391 acc:1.0\n",
      "epoch:10 batch:61 loss:0.013743245974183083 acc:1.0\n",
      "epoch:10 batch:62 loss:0.013415750116109848 acc:1.0\n",
      "epoch:10 batch:63 loss:0.010662567801773548 acc:1.0\n",
      "epoch:10 batch:64 loss:0.013557320460677147 acc:1.0\n",
      "epoch:10 batch:65 loss:0.014123566448688507 acc:1.0\n",
      "epoch:10 batch:66 loss:0.012706590816378593 acc:1.0\n",
      "epoch:10 batch:67 loss:0.03658208250999451 acc:1.0\n",
      "epoch:10 batch:68 loss:0.0747334286570549 acc:1.0\n",
      "epoch:10 batch:69 loss:0.09134548902511597 acc:1.0\n",
      "epoch:10 batch:70 loss:0.07879338413476944 acc:1.0\n",
      "epoch:10 batch:71 loss:0.07241392880678177 acc:1.0\n",
      "epoch:10 batch:72 loss:0.07432577013969421 acc:1.0\n",
      "epoch:10 batch:73 loss:0.07162582129240036 acc:1.0\n",
      "epoch:10 batch:74 loss:0.09067302197217941 acc:1.0\n",
      "epoch:10 batch:75 loss:0.07081875205039978 acc:1.0\n",
      "epoch:10 batch:76 loss:0.07068707793951035 acc:1.0\n",
      "epoch:10 batch:77 loss:0.0787142813205719 acc:1.0\n",
      "epoch:10 batch:78 loss:0.07220093160867691 acc:1.0\n",
      "epoch:10 batch:79 loss:0.07800746709108353 acc:1.0\n",
      "epoch:10 batch:80 loss:0.07021906226873398 acc:1.0\n",
      "epoch:10 batch:81 loss:0.0841405838727951 acc:1.0\n",
      "epoch:10 batch:82 loss:0.07465224713087082 acc:1.0\n",
      "epoch:10 batch:83 loss:0.06629575788974762 acc:1.0\n",
      "epoch:10 batch:84 loss:0.05980556458234787 acc:1.0\n",
      "epoch:10 batch:85 loss:0.06604160368442535 acc:1.0\n",
      "epoch:10 batch:86 loss:0.06995853036642075 acc:1.0\n",
      "epoch:10 batch:87 loss:0.07795976847410202 acc:1.0\n",
      "epoch:10 batch:88 loss:0.07107264548540115 acc:1.0\n",
      "epoch:10 batch:89 loss:0.01626591756939888 acc:1.0\n",
      "epoch:10 batch:90 loss:0.010877827182412148 acc:1.0\n",
      "epoch:10 batch:91 loss:0.012172759510576725 acc:1.0\n",
      "epoch:10 batch:92 loss:0.01146225817501545 acc:1.0\n",
      "epoch:10 batch:93 loss:0.011058427393436432 acc:1.0\n",
      "epoch:10 batch:94 loss:0.011916473507881165 acc:1.0\n",
      "epoch:10 batch:95 loss:0.011588727124035358 acc:1.0\n",
      "epoch:10 batch:96 loss:0.010380196385085583 acc:1.0\n",
      "epoch:10 batch:97 loss:0.010228384286165237 acc:1.0\n",
      "epoch:10 batch:98 loss:0.011374350637197495 acc:1.0\n",
      "epoch:10 batch:99 loss:0.012279574759304523 acc:1.0\n",
      "epoch:10 batch:100 loss:0.011632016859948635 acc:1.0\n",
      "epoch:10 batch:101 loss:0.009368534199893475 acc:1.0\n",
      "epoch:10 batch:102 loss:0.010695240460336208 acc:1.0\n",
      "epoch:10 batch:103 loss:0.01102205365896225 acc:1.0\n",
      "epoch:10 batch:104 loss:0.013429423794150352 acc:1.0\n",
      "epoch:10 batch:105 loss:0.01204654946923256 acc:1.0\n",
      "epoch:10 batch:106 loss:0.009042461402714252 acc:1.0\n",
      "epoch:10 batch:107 loss:0.013088487088680267 acc:1.0\n",
      "epoch:10 batch:108 loss:0.010199339129030704 acc:1.0\n",
      "epoch:10 batch:109 loss:0.009162744507193565 acc:1.0\n",
      "epoch:10 batch:110 loss:0.012330667115747929 acc:1.0\n",
      "epoch:10 batch:111 loss:0.010200316086411476 acc:1.0\n",
      "epoch:10 batch:112 loss:0.06391485780477524 acc:1.0\n",
      "epoch:10 batch:113 loss:0.06991900503635406 acc:1.0\n",
      "epoch:10 batch:114 loss:0.0810333639383316 acc:1.0\n",
      "epoch:10 batch:115 loss:0.06746475398540497 acc:1.0\n",
      "epoch:10 batch:116 loss:0.06120721250772476 acc:1.0\n",
      "epoch:10 batch:117 loss:0.06541130691766739 acc:1.0\n",
      "epoch:10 batch:118 loss:0.09723972529172897 acc:1.0\n",
      "epoch:10 batch:119 loss:0.06269953399896622 acc:1.0\n",
      "epoch:10 batch:120 loss:0.06364323198795319 acc:1.0\n",
      "epoch:10 batch:121 loss:0.06327906996011734 acc:1.0\n",
      "epoch:10 batch:122 loss:0.06706137955188751 acc:1.0\n",
      "epoch:10 batch:123 loss:0.056014303117990494 acc:1.0\n",
      "epoch:10 batch:124 loss:0.06029719114303589 acc:1.0\n",
      "epoch:10 batch:125 loss:0.0594804473221302 acc:1.0\n",
      "epoch:10 batch:126 loss:0.059444088488817215 acc:1.0\n",
      "epoch:10 batch:127 loss:0.05825093388557434 acc:1.0\n",
      "epoch:10 batch:128 loss:0.06779472529888153 acc:1.0\n",
      "epoch:10 batch:129 loss:0.06580422818660736 acc:1.0\n",
      "epoch:10 batch:130 loss:0.0555877760052681 acc:1.0\n",
      "epoch:10 batch:131 loss:0.058742135763168335 acc:1.0\n",
      "epoch:10 batch:132 loss:0.06408339738845825 acc:1.0\n",
      "epoch:10 batch:133 loss:0.05581044778227806 acc:1.0\n",
      "epoch:10 batch:134 loss:0.012246042490005493 acc:1.0\n",
      "epoch:10 batch:135 loss:0.012177430093288422 acc:1.0\n",
      "epoch:10 batch:136 loss:0.011911487206816673 acc:1.0\n",
      "epoch:10 batch:137 loss:0.011133687570691109 acc:1.0\n",
      "epoch:10 batch:138 loss:0.01068895123898983 acc:1.0\n",
      "epoch:10 batch:139 loss:0.009694255888462067 acc:1.0\n",
      "epoch:10 batch:140 loss:0.012970125302672386 acc:1.0\n",
      "epoch:10 batch:141 loss:0.011388753540813923 acc:1.0\n",
      "epoch:10 batch:142 loss:0.010314472950994968 acc:1.0\n",
      "epoch:10 batch:143 loss:0.008731374517083168 acc:1.0\n",
      "epoch:10 batch:144 loss:0.007948034442961216 acc:1.0\n",
      "epoch:10 batch:145 loss:0.008463460952043533 acc:1.0\n",
      "epoch:10 batch:146 loss:0.013943110592663288 acc:1.0\n",
      "epoch:10 batch:147 loss:0.011055227369070053 acc:1.0\n",
      "epoch:10 batch:148 loss:0.01121291983872652 acc:1.0\n",
      "epoch:10 batch:149 loss:0.010375923477113247 acc:1.0\n",
      "epoch:10 batch:150 loss:0.008134942501783371 acc:1.0\n",
      "epoch:10 batch:151 loss:0.009933517314493656 acc:1.0\n",
      "epoch:10 batch:152 loss:0.011070615611970425 acc:1.0\n",
      "epoch:10 batch:153 loss:0.007442199159413576 acc:1.0\n",
      "epoch:10 batch:154 loss:0.008039887063205242 acc:1.0\n",
      "epoch:10 batch:155 loss:0.009239650331437588 acc:1.0\n",
      "epoch:10 batch:156 loss:0.008494777604937553 acc:1.0\n",
      "epoch:10 batch:157 loss:0.0539126843214035 acc:1.0\n",
      "epoch:10 batch:158 loss:0.9913502931594849 acc:0.7166666666666667\n",
      "epoch:10 batch:159 loss:3.0324866771698 acc:0.0\n",
      "epoch:10 batch:160 loss:1.5067330598831177 acc:0.5333333333333333\n",
      "epoch:10 batch:161 loss:0.05610967427492142 acc:1.0\n",
      "epoch:10 batch:162 loss:0.056770723313093185 acc:1.0\n",
      "epoch:10 batch:163 loss:0.05802391096949577 acc:1.0\n",
      "epoch:10 batch:164 loss:0.2240399420261383 acc:0.95\n",
      "epoch:10 batch:165 loss:3.0435807704925537 acc:0.0\n",
      "epoch:10 batch:166 loss:2.8793528079986572 acc:0.0\n",
      "epoch:10 batch:167 loss:2.9976701736450195 acc:0.0\n",
      "epoch:10 batch:168 loss:2.8462860584259033 acc:0.0\n",
      "epoch:10 batch:169 loss:1.3724398612976074 acc:0.5166666666666667\n",
      "epoch:10 batch:170 loss:0.07247112691402435 acc:1.0\n",
      "epoch:10 batch:171 loss:0.07710225880146027 acc:1.0\n",
      "epoch:10 batch:172 loss:0.0858926996588707 acc:1.0\n",
      "epoch:10 batch:173 loss:0.07282224297523499 acc:1.0\n",
      "epoch:10 batch:174 loss:0.07781516760587692 acc:1.0\n",
      "epoch:10 batch:175 loss:0.0891554057598114 acc:1.0\n",
      "epoch:10 batch:176 loss:1.337594985961914 acc:0.11666666666666667\n",
      "epoch:10 batch:177 loss:0.27738475799560547 acc:0.9833333333333333\n",
      "epoch:10 batch:178 loss:0.2927709221839905 acc:0.9833333333333333\n",
      "epoch:10 batch:179 loss:2.072443962097168 acc:0.0\n",
      "epoch:10 batch:180 loss:1.3697556257247925 acc:0.26666666666666666\n",
      "epoch:10 batch:181 loss:0.2576987147331238 acc:1.0\n",
      "epoch:10 batch:182 loss:0.31426379084587097 acc:1.0\n",
      "epoch:10 batch:183 loss:0.3546413481235504 acc:1.0\n",
      "epoch:10 batch:184 loss:0.07350069284439087 acc:1.0\n",
      "epoch:10 batch:185 loss:0.04620852693915367 acc:1.0\n",
      "epoch:10 batch:186 loss:0.0426587238907814 acc:1.0\n",
      "epoch:10 batch:187 loss:0.3147292733192444 acc:0.95\n",
      "epoch:10 batch:188 loss:0.5962832570075989 acc:0.8\n",
      "epoch:10 batch:189 loss:0.4988258183002472 acc:0.7\n",
      "epoch:10 batch:190 loss:0.19916246831417084 acc:1.0\n",
      "epoch:10 batch:191 loss:0.2079090178012848 acc:1.0\n",
      "epoch:10 batch:192 loss:0.20810087025165558 acc:1.0\n",
      "epoch:10 batch:193 loss:0.2182994931936264 acc:1.0\n",
      "epoch:10 batch:194 loss:0.2500472068786621 acc:0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 batch:195 loss:0.8751670122146606 acc:0.05\n",
      "epoch:10 batch:196 loss:0.7328516244888306 acc:0.2833333333333333\n",
      "epoch:10 batch:197 loss:0.8028134703636169 acc:0.08333333333333333\n",
      "epoch:10 batch:198 loss:0.4358341097831726 acc:0.65\n",
      "epoch:10 batch:199 loss:0.063764289021492 acc:1.0\n",
      "epoch:10 batch:200 loss:0.05303521454334259 acc:1.0\n",
      "epoch:10 batch:201 loss:0.06084617227315903 acc:1.0\n",
      "epoch:10 batch:202 loss:0.6633002758026123 acc:0.4166666666666667\n",
      "epoch:10 batch:203 loss:0.8802027702331543 acc:0.38333333333333336\n",
      "epoch:10 batch:204 loss:0.37722933292388916 acc:0.9833333333333333\n",
      "epoch:10 batch:205 loss:0.29052916169166565 acc:0.9666666666666667\n",
      "epoch:10 batch:206 loss:0.13476306200027466 acc:1.0\n",
      "epoch:10 batch:207 loss:0.1354053020477295 acc:1.0\n",
      "epoch:10 batch:208 loss:0.12804214656352997 acc:1.0\n",
      "epoch:10 batch:209 loss:0.12324175238609314 acc:1.0\n",
      "epoch:10 batch:210 loss:0.1322479248046875 acc:1.0\n",
      "epoch:10 batch:211 loss:1.0791367292404175 acc:0.5833333333333334\n",
      "epoch:10 batch:212 loss:2.2011678218841553 acc:0.16666666666666666\n",
      "epoch:10 batch:213 loss:0.14622241258621216 acc:0.9666666666666667\n",
      "epoch:10 batch:214 loss:0.18743382394313812 acc:0.9166666666666666\n",
      "epoch:10 batch:215 loss:2.1153037548065186 acc:0.016666666666666666\n",
      "epoch:10 batch:216 loss:2.2871787548065186 acc:0.0\n",
      "epoch:10 batch:217 loss:2.367966890335083 acc:0.0\n",
      "epoch:10 batch:218 loss:2.333953380584717 acc:0.0\n",
      "epoch:10 batch:219 loss:2.208181858062744 acc:0.0\n",
      "epoch:10 batch:220 loss:2.2443313598632812 acc:0.0\n",
      "epoch:10 batch:221 loss:2.136714220046997 acc:0.0\n",
      "epoch:10 batch:222 loss:1.9662961959838867 acc:0.016666666666666666\n",
      "epoch:10 batch:223 loss:1.9637808799743652 acc:0.016666666666666666\n",
      "epoch:10 batch:224 loss:1.7977129220962524 acc:0.0\n",
      "epoch:15 batch:0 loss:0.011804730631411076 acc:1.0\n",
      "epoch:15 batch:1 loss:0.013777659274637699 acc:1.0\n",
      "epoch:15 batch:2 loss:0.010357387363910675 acc:1.0\n",
      "epoch:15 batch:3 loss:0.010238751769065857 acc:1.0\n",
      "epoch:15 batch:4 loss:0.011786991730332375 acc:1.0\n",
      "epoch:15 batch:5 loss:0.011766833253204823 acc:1.0\n",
      "epoch:15 batch:6 loss:0.01070722658187151 acc:1.0\n",
      "epoch:15 batch:7 loss:0.011039689183235168 acc:1.0\n",
      "epoch:15 batch:8 loss:0.010571685619652271 acc:1.0\n",
      "epoch:15 batch:9 loss:0.011595931835472584 acc:1.0\n",
      "epoch:15 batch:10 loss:0.008944102562963963 acc:1.0\n",
      "epoch:15 batch:11 loss:0.010146575048565865 acc:1.0\n",
      "epoch:15 batch:12 loss:0.011120852082967758 acc:1.0\n",
      "epoch:15 batch:13 loss:0.01115020178258419 acc:1.0\n",
      "epoch:15 batch:14 loss:0.009280120022594929 acc:1.0\n",
      "epoch:15 batch:15 loss:0.012098587118089199 acc:1.0\n",
      "epoch:15 batch:16 loss:0.012332688085734844 acc:1.0\n",
      "epoch:15 batch:17 loss:0.01069501880556345 acc:1.0\n",
      "epoch:15 batch:18 loss:0.011680327355861664 acc:1.0\n",
      "epoch:15 batch:19 loss:0.013019244186580181 acc:1.0\n",
      "epoch:15 batch:20 loss:0.013424481265246868 acc:1.0\n",
      "epoch:15 batch:21 loss:0.013114698231220245 acc:1.0\n",
      "epoch:15 batch:22 loss:0.01266926433891058 acc:1.0\n",
      "epoch:15 batch:23 loss:0.09087245166301727 acc:1.0\n",
      "epoch:15 batch:24 loss:0.07815968245267868 acc:1.0\n",
      "epoch:15 batch:25 loss:0.10180629044771194 acc:1.0\n",
      "epoch:15 batch:26 loss:0.09365805983543396 acc:1.0\n",
      "epoch:15 batch:27 loss:0.09746026992797852 acc:1.0\n",
      "epoch:15 batch:28 loss:0.1005411297082901 acc:1.0\n",
      "epoch:15 batch:29 loss:0.09855350106954575 acc:1.0\n",
      "epoch:15 batch:30 loss:0.09367669373750687 acc:1.0\n",
      "epoch:15 batch:31 loss:0.10602372139692307 acc:1.0\n",
      "epoch:15 batch:32 loss:0.09031260758638382 acc:1.0\n",
      "epoch:15 batch:33 loss:0.08767320215702057 acc:1.0\n",
      "epoch:15 batch:34 loss:0.08884697407484055 acc:1.0\n",
      "epoch:15 batch:35 loss:0.08428093791007996 acc:1.0\n",
      "epoch:15 batch:36 loss:0.08558977395296097 acc:1.0\n",
      "epoch:15 batch:37 loss:0.0943731963634491 acc:1.0\n",
      "epoch:15 batch:38 loss:0.09239894151687622 acc:1.0\n",
      "epoch:15 batch:39 loss:0.10012716054916382 acc:1.0\n",
      "epoch:15 batch:40 loss:0.08431047201156616 acc:1.0\n",
      "epoch:15 batch:41 loss:0.07845429331064224 acc:1.0\n",
      "epoch:15 batch:42 loss:0.08566024154424667 acc:1.0\n",
      "epoch:15 batch:43 loss:0.08419777452945709 acc:1.0\n",
      "epoch:15 batch:44 loss:0.06003352627158165 acc:1.0\n",
      "epoch:15 batch:45 loss:0.013252120465040207 acc:1.0\n",
      "epoch:15 batch:46 loss:0.010865027084946632 acc:1.0\n",
      "epoch:15 batch:47 loss:0.013047212734818459 acc:1.0\n",
      "epoch:15 batch:48 loss:0.013340315781533718 acc:1.0\n",
      "epoch:15 batch:49 loss:0.010624079033732414 acc:1.0\n",
      "epoch:15 batch:50 loss:0.011352148838341236 acc:1.0\n",
      "epoch:15 batch:51 loss:0.009629836305975914 acc:1.0\n",
      "epoch:15 batch:52 loss:0.0125525938346982 acc:1.0\n",
      "epoch:15 batch:53 loss:0.01476231124252081 acc:1.0\n",
      "epoch:15 batch:54 loss:0.011747673153877258 acc:1.0\n",
      "epoch:15 batch:55 loss:0.010780670680105686 acc:1.0\n",
      "epoch:15 batch:56 loss:0.010738633573055267 acc:1.0\n",
      "epoch:15 batch:57 loss:0.010388989932835102 acc:1.0\n",
      "epoch:15 batch:58 loss:0.013274604454636574 acc:1.0\n",
      "epoch:15 batch:59 loss:0.012914163991808891 acc:1.0\n",
      "epoch:15 batch:60 loss:0.015167703852057457 acc:1.0\n",
      "epoch:15 batch:61 loss:0.013020767830312252 acc:1.0\n",
      "epoch:15 batch:62 loss:0.011825518682599068 acc:1.0\n",
      "epoch:15 batch:63 loss:0.01057320274412632 acc:1.0\n",
      "epoch:15 batch:64 loss:0.012367617338895798 acc:1.0\n",
      "epoch:15 batch:65 loss:0.012588577345013618 acc:1.0\n",
      "epoch:15 batch:66 loss:0.013112703338265419 acc:1.0\n",
      "epoch:15 batch:67 loss:0.03876601904630661 acc:1.0\n",
      "epoch:15 batch:68 loss:0.0886927992105484 acc:1.0\n",
      "epoch:15 batch:69 loss:0.08731476962566376 acc:1.0\n",
      "epoch:15 batch:70 loss:0.08672256767749786 acc:1.0\n",
      "epoch:15 batch:71 loss:0.07797669619321823 acc:1.0\n",
      "epoch:15 batch:72 loss:0.08611036092042923 acc:1.0\n",
      "epoch:15 batch:73 loss:0.08102422207593918 acc:1.0\n",
      "epoch:15 batch:74 loss:0.0735301673412323 acc:1.0\n",
      "epoch:15 batch:75 loss:0.0802687555551529 acc:1.0\n",
      "epoch:15 batch:76 loss:0.07617124170064926 acc:1.0\n",
      "epoch:15 batch:77 loss:0.07314185053110123 acc:1.0\n",
      "epoch:15 batch:78 loss:0.07078240066766739 acc:1.0\n",
      "epoch:15 batch:79 loss:0.07343906909227371 acc:1.0\n",
      "epoch:15 batch:80 loss:0.06906101107597351 acc:1.0\n",
      "epoch:15 batch:81 loss:0.06975410133600235 acc:1.0\n",
      "epoch:15 batch:82 loss:0.076508067548275 acc:1.0\n",
      "epoch:15 batch:83 loss:0.07063969224691391 acc:1.0\n",
      "epoch:15 batch:84 loss:0.07356064766645432 acc:1.0\n",
      "epoch:15 batch:85 loss:0.0682523250579834 acc:1.0\n",
      "epoch:15 batch:86 loss:0.06924492865800858 acc:1.0\n",
      "epoch:15 batch:87 loss:0.06994529068470001 acc:1.0\n",
      "epoch:15 batch:88 loss:0.06662961095571518 acc:1.0\n",
      "epoch:15 batch:89 loss:0.026058023795485497 acc:1.0\n",
      "epoch:15 batch:90 loss:0.013409608043730259 acc:1.0\n",
      "epoch:15 batch:91 loss:0.00867076963186264 acc:1.0\n",
      "epoch:15 batch:92 loss:0.012421323917806149 acc:1.0\n",
      "epoch:15 batch:93 loss:0.016090165823698044 acc:1.0\n",
      "epoch:15 batch:94 loss:0.010744558647274971 acc:1.0\n",
      "epoch:15 batch:95 loss:0.013024607673287392 acc:1.0\n",
      "epoch:15 batch:96 loss:0.012860679067671299 acc:1.0\n",
      "epoch:15 batch:97 loss:0.011272410862147808 acc:1.0\n",
      "epoch:15 batch:98 loss:0.011863993480801582 acc:1.0\n",
      "epoch:15 batch:99 loss:0.012263796292245388 acc:1.0\n",
      "epoch:15 batch:100 loss:0.008919985964894295 acc:1.0\n",
      "epoch:15 batch:101 loss:0.011382238939404488 acc:1.0\n",
      "epoch:15 batch:102 loss:0.009993054904043674 acc:1.0\n",
      "epoch:15 batch:103 loss:0.012462171725928783 acc:1.0\n",
      "epoch:15 batch:104 loss:0.011109144426882267 acc:1.0\n",
      "epoch:15 batch:105 loss:0.008660096675157547 acc:1.0\n",
      "epoch:15 batch:106 loss:0.013522038236260414 acc:1.0\n",
      "epoch:15 batch:107 loss:0.011084938421845436 acc:1.0\n",
      "epoch:15 batch:108 loss:0.012528976425528526 acc:1.0\n",
      "epoch:15 batch:109 loss:0.012577453628182411 acc:1.0\n",
      "epoch:15 batch:110 loss:0.00985720008611679 acc:1.0\n",
      "epoch:15 batch:111 loss:0.011473558843135834 acc:1.0\n",
      "epoch:15 batch:112 loss:0.07543075829744339 acc:1.0\n",
      "epoch:15 batch:113 loss:0.0794449970126152 acc:1.0\n",
      "epoch:15 batch:114 loss:0.07149586081504822 acc:1.0\n",
      "epoch:15 batch:115 loss:0.0867995023727417 acc:1.0\n",
      "epoch:15 batch:116 loss:0.084630087018013 acc:1.0\n",
      "epoch:15 batch:117 loss:0.07343140244483948 acc:1.0\n",
      "epoch:15 batch:118 loss:0.08152488619089127 acc:1.0\n",
      "epoch:15 batch:119 loss:0.0660204067826271 acc:1.0\n",
      "epoch:15 batch:120 loss:0.07918382436037064 acc:1.0\n",
      "epoch:15 batch:121 loss:0.0803748294711113 acc:1.0\n",
      "epoch:15 batch:122 loss:0.06545118987560272 acc:1.0\n",
      "epoch:15 batch:123 loss:0.06710147857666016 acc:1.0\n",
      "epoch:15 batch:124 loss:0.06343017518520355 acc:1.0\n",
      "epoch:15 batch:125 loss:0.06399399787187576 acc:1.0\n",
      "epoch:15 batch:126 loss:0.05824339762330055 acc:1.0\n",
      "epoch:15 batch:127 loss:0.0668596625328064 acc:1.0\n",
      "epoch:15 batch:128 loss:0.06836289912462234 acc:1.0\n",
      "epoch:15 batch:129 loss:0.06501223146915436 acc:1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 batch:130 loss:0.06242457032203674 acc:1.0\n",
      "epoch:15 batch:131 loss:0.06200059875845909 acc:1.0\n",
      "epoch:15 batch:132 loss:0.06316976249217987 acc:1.0\n",
      "epoch:15 batch:133 loss:0.062063466757535934 acc:1.0\n",
      "epoch:15 batch:134 loss:0.010762871243059635 acc:1.0\n",
      "epoch:15 batch:135 loss:0.010900722816586494 acc:1.0\n",
      "epoch:15 batch:136 loss:0.012194213457405567 acc:1.0\n",
      "epoch:15 batch:137 loss:0.011287415400147438 acc:1.0\n",
      "epoch:15 batch:138 loss:0.01258205994963646 acc:1.0\n",
      "epoch:15 batch:139 loss:0.010901475325226784 acc:1.0\n",
      "epoch:15 batch:140 loss:0.011505058035254478 acc:1.0\n",
      "epoch:15 batch:141 loss:0.009268355555832386 acc:1.0\n",
      "epoch:15 batch:142 loss:0.013142057694494724 acc:1.0\n",
      "epoch:15 batch:143 loss:0.010014881379902363 acc:1.0\n",
      "epoch:15 batch:144 loss:0.010327554307878017 acc:1.0\n",
      "epoch:15 batch:145 loss:0.013592967763543129 acc:1.0\n",
      "epoch:15 batch:146 loss:0.009442622773349285 acc:1.0\n",
      "epoch:15 batch:147 loss:0.01301547046750784 acc:1.0\n",
      "epoch:15 batch:148 loss:0.009677483700215816 acc:1.0\n",
      "epoch:15 batch:149 loss:0.009731708094477654 acc:1.0\n",
      "epoch:15 batch:150 loss:0.009080875664949417 acc:1.0\n",
      "epoch:15 batch:151 loss:0.010818923823535442 acc:1.0\n",
      "epoch:15 batch:152 loss:0.011783037334680557 acc:1.0\n",
      "epoch:15 batch:153 loss:0.010192206129431725 acc:1.0\n",
      "epoch:15 batch:154 loss:0.011287529952824116 acc:1.0\n",
      "epoch:15 batch:155 loss:0.012724146246910095 acc:1.0\n",
      "epoch:15 batch:156 loss:0.011863075196743011 acc:1.0\n",
      "epoch:15 batch:157 loss:0.06777667999267578 acc:1.0\n",
      "epoch:15 batch:158 loss:0.8534313440322876 acc:0.7166666666666667\n",
      "epoch:15 batch:159 loss:2.9764187335968018 acc:0.0\n",
      "epoch:15 batch:160 loss:1.3756749629974365 acc:0.5333333333333333\n",
      "epoch:15 batch:161 loss:0.061207760125398636 acc:1.0\n",
      "epoch:15 batch:162 loss:0.06940555572509766 acc:1.0\n",
      "epoch:15 batch:163 loss:0.06311483681201935 acc:1.0\n",
      "epoch:15 batch:164 loss:0.169712096452713 acc:0.95\n",
      "epoch:15 batch:165 loss:3.030484676361084 acc:0.0\n",
      "epoch:15 batch:166 loss:2.768399715423584 acc:0.0\n",
      "epoch:15 batch:167 loss:2.9757697582244873 acc:0.0\n",
      "epoch:15 batch:168 loss:2.857332229614258 acc:0.0\n",
      "epoch:15 batch:169 loss:1.3722527027130127 acc:0.5166666666666667\n",
      "epoch:15 batch:170 loss:0.07858556509017944 acc:1.0\n",
      "epoch:15 batch:171 loss:0.0912831500172615 acc:1.0\n",
      "epoch:15 batch:172 loss:0.0794280394911766 acc:1.0\n",
      "epoch:15 batch:173 loss:0.07913025468587875 acc:1.0\n",
      "epoch:15 batch:174 loss:0.07654320448637009 acc:1.0\n",
      "epoch:15 batch:175 loss:0.07344815135002136 acc:1.0\n",
      "epoch:15 batch:176 loss:1.3347105979919434 acc:0.13333333333333333\n",
      "epoch:15 batch:177 loss:0.35377418994903564 acc:0.9833333333333333\n",
      "epoch:15 batch:178 loss:0.30629488825798035 acc:1.0\n",
      "epoch:15 batch:179 loss:1.3202885389328003 acc:0.0\n",
      "epoch:15 batch:180 loss:1.0504541397094727 acc:0.26666666666666666\n",
      "epoch:15 batch:181 loss:0.3535568118095398 acc:1.0\n",
      "epoch:15 batch:182 loss:0.37350592017173767 acc:1.0\n",
      "epoch:15 batch:183 loss:0.3973188102245331 acc:1.0\n",
      "epoch:15 batch:184 loss:0.07191327214241028 acc:1.0\n",
      "epoch:15 batch:185 loss:0.03668215498328209 acc:1.0\n",
      "epoch:15 batch:186 loss:0.03387503698468208 acc:1.0\n",
      "epoch:15 batch:187 loss:0.24124982953071594 acc:1.0\n",
      "epoch:15 batch:188 loss:0.5538167357444763 acc:0.7833333333333333\n",
      "epoch:15 batch:189 loss:0.49320557713508606 acc:0.7166666666666667\n",
      "epoch:15 batch:190 loss:0.12297505140304565 acc:1.0\n",
      "epoch:15 batch:191 loss:0.13553565740585327 acc:1.0\n",
      "epoch:15 batch:192 loss:0.1310584545135498 acc:1.0\n",
      "epoch:15 batch:193 loss:0.11634358018636703 acc:1.0\n",
      "epoch:15 batch:194 loss:0.20368894934654236 acc:0.9333333333333333\n",
      "epoch:15 batch:195 loss:1.3183605670928955 acc:0.0\n",
      "epoch:15 batch:196 loss:0.6450366377830505 acc:0.7166666666666667\n",
      "epoch:15 batch:197 loss:0.43226879835128784 acc:1.0\n",
      "epoch:15 batch:198 loss:0.2566565275192261 acc:1.0\n",
      "epoch:15 batch:199 loss:0.03902845457196236 acc:1.0\n",
      "epoch:15 batch:200 loss:0.04307784512639046 acc:1.0\n",
      "epoch:15 batch:201 loss:0.04657852277159691 acc:1.0\n",
      "epoch:15 batch:202 loss:0.24703925848007202 acc:0.9833333333333333\n",
      "epoch:15 batch:203 loss:0.2765499949455261 acc:0.95\n",
      "epoch:15 batch:204 loss:1.601122498512268 acc:0.0\n",
      "epoch:15 batch:205 loss:0.5622355341911316 acc:0.6166666666666667\n",
      "epoch:15 batch:206 loss:0.12556134164333344 acc:1.0\n",
      "epoch:15 batch:207 loss:0.1238914206624031 acc:1.0\n",
      "epoch:15 batch:208 loss:0.11080102622509003 acc:1.0\n",
      "epoch:15 batch:209 loss:0.1263204663991928 acc:1.0\n",
      "epoch:15 batch:210 loss:0.12437385320663452 acc:1.0\n",
      "epoch:15 batch:211 loss:0.13635171949863434 acc:1.0\n",
      "epoch:15 batch:212 loss:2.28796648979187 acc:0.11666666666666667\n",
      "epoch:15 batch:213 loss:0.09635496139526367 acc:0.9833333333333333\n",
      "epoch:15 batch:214 loss:0.09181546419858932 acc:1.0\n",
      "epoch:15 batch:215 loss:0.09539557993412018 acc:1.0\n",
      "epoch:15 batch:216 loss:0.0934259295463562 acc:0.9833333333333333\n",
      "epoch:15 batch:217 loss:0.09053201973438263 acc:1.0\n",
      "epoch:15 batch:218 loss:0.08652842044830322 acc:1.0\n",
      "epoch:15 batch:219 loss:0.09035372734069824 acc:1.0\n",
      "epoch:15 batch:220 loss:0.08492786437273026 acc:1.0\n",
      "epoch:15 batch:221 loss:0.09528493136167526 acc:1.0\n",
      "epoch:15 batch:222 loss:0.08368666470050812 acc:1.0\n",
      "epoch:15 batch:223 loss:0.09500361233949661 acc:1.0\n",
      "epoch:15 batch:224 loss:0.0890871211886406 acc:1.0\n",
      "totally cost 98.02530860900879\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "time_start=time.time()\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch, (batch_x, batch_y) in enumerate(train_dataloader):   \n",
    "        batch_x = batch_x.cuda()   \n",
    "        output,pre_out = model(batch_x)\n",
    "        output = torch.reshape(output,[-1,1])\n",
    "\n",
    "        batch_y = np.array(batch_y)\n",
    "        batch_y = torch.tensor(np.reshape(batch_y,[-1,1]))\n",
    "        batch_y = batch_y.float()\n",
    "        batch_y = batch_y.cuda()\n",
    "\n",
    "        loss = loss_function(output,batch_y)\n",
    "        acc = evaluate_accuracy(batch_x,batch_y,model)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_array[batch] = loss.item()\n",
    "        batch_index_array[batch] = batch\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"epoch:{} batch:{} loss:{} acc:{}\".format(epoch,batch,loss.item(),acc))\n",
    "\n",
    "time_end=time.time()\n",
    "print('totally cost',time_end-time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ba69890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 loss:0.07267970591783524\n",
      "train acc 0.9190070396443127\n",
      "valid acc: 0.052701801200800535\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "x = torch.tensor( X_train[:,:,:]).float()\n",
    "x = torch.where(torch.isnan(x), torch.full_like(x, 0), x)\n",
    "    \n",
    "y = y_train[:]\n",
    "y = np.array(y)\n",
    "y = torch.tensor(np.reshape(y,[-1,1]))\n",
    "y = y.float()\n",
    "    \n",
    "x = x.cuda()\n",
    "y = y.cuda()\n",
    "\n",
    "x_valid = torch.tensor(X_valid[:,:,:]).float()\n",
    "x_valid = torch.where(torch.isnan(x_valid), torch.full_like(x_valid, 0), x_valid)\n",
    "    \n",
    "y_val = y_valid[:]\n",
    "y_val = np.array(y_val)\n",
    "y_val = torch.tensor(np.reshape(y_val,[-1,1]))\n",
    "y_val = y_val.float()\n",
    "\n",
    "x_valid = x_valid.cuda()\n",
    "y_val = y_val.cuda()\n",
    "\n",
    "print(\"epoch:{} loss:{}\".format(epoch,loss.item()))\n",
    "print(\"train acc\", evaluate_accuracy(x,y,model))\n",
    "print(\"valid acc:\",evaluate_accuracy(x_valid,y_val,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "258faafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.05      1.00      0.10        79\n",
      "           1       0.00      0.00      0.00      1420\n",
      "\n",
      "    accuracy                           0.05      1499\n",
      "   macro avg       0.03      0.50      0.05      1499\n",
      "weighted avg       0.00      0.05      0.01      1499\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as sm\n",
    "output,pre_out = model(x_valid)\n",
    "output = torch.reshape(output,[-1,1])\n",
    "output = output.ge(0.5)\n",
    "\n",
    "output_cpu = output.cpu()\n",
    "cr = sm.classification_report(y_valid[:,0], output_cpu)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0beb4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
