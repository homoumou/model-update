{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e669956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc021f",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21763b30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'..\\Dataset\\SWAT\\SWaT.csv') \n",
    "df.insert(78,'Label','Normal')\n",
    "#Attack on FIT401\n",
    "df.loc[((df['Timestamp'] >='2019-07-20T07:08:46Z') & (df['Timestamp'] < '2019-07-20T07:10:311Z')),'Label']='Attack'\n",
    "#Attack on LIT301\n",
    "df.loc[((df['Timestamp'] >='2019-07-20T07:15:00Z') & (df['Timestamp'] < '2019-07-20T07:19:321Z')),'Label']='Attack'\n",
    "#Attack on P601\n",
    "df.loc[((df['Timestamp'] >='2019-07-20T07:26:57Z') & (df['Timestamp'] < '2019-07-20T07:30:481Z')),'Label']='Attack'\n",
    "#Multipoint Attack\n",
    "df.loc[((df['Timestamp'] >='2019-07-20T07:38:50Z') & (df['Timestamp'] < '2019-07-20T07:46:201Z')),'Label']='Attack'\n",
    "##Attack on MV501\n",
    "df.loc[((df['Timestamp'] >='2019-07-20T07:54:00Z') & (df['Timestamp'] < '2019-07-20T07:56:01Z')),'Label']='Attack'\n",
    "#Attack on P301\n",
    "df.loc[((df['Timestamp'] >='2019-07-20T08:02:56Z') & (df['Timestamp'] < '2019-07-20T08:16:181Z')),'Label']='Attack'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64055929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>FIT 101</th>\n",
       "      <th>LIT 101</th>\n",
       "      <th>MV 101</th>\n",
       "      <th>P1_STATE</th>\n",
       "      <th>P101 Status</th>\n",
       "      <th>P102 Status</th>\n",
       "      <th>AIT 201</th>\n",
       "      <th>AIT 202</th>\n",
       "      <th>AIT 203</th>\n",
       "      <th>...</th>\n",
       "      <th>LSH 602</th>\n",
       "      <th>LSH 603</th>\n",
       "      <th>LSL 601</th>\n",
       "      <th>LSL 602</th>\n",
       "      <th>LSL 603</th>\n",
       "      <th>P6 STATE</th>\n",
       "      <th>P601 Status</th>\n",
       "      <th>P602 Status</th>\n",
       "      <th>P603 Status</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-20T04:30:00Z</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>729.865800</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.293002</td>\n",
       "      <td>198.077423</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-20T04:30:01Z</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>729.434000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.293002</td>\n",
       "      <td>198.385025</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-20T04:30:02.004013Z</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>729.120000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.293002</td>\n",
       "      <td>198.436300</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-20T04:30:03.004013Z</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>728.688200</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.289157</td>\n",
       "      <td>198.667000</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-20T04:30:04Z</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>727.706900</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.289157</td>\n",
       "      <td>198.897720</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14991</th>\n",
       "      <td>2019-07-20T08:39:55.001007Z</td>\n",
       "      <td>4.200429</td>\n",
       "      <td>491.169769</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.319918</td>\n",
       "      <td>257.703156</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14992</th>\n",
       "      <td>2019-07-20T08:39:56.0050048Z</td>\n",
       "      <td>4.253915</td>\n",
       "      <td>491.405273</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.317354</td>\n",
       "      <td>257.703156</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14993</th>\n",
       "      <td>2019-07-20T08:39:57.0050048Z</td>\n",
       "      <td>4.303558</td>\n",
       "      <td>492.308100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.317354</td>\n",
       "      <td>257.703156</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>2019-07-20T08:39:58.0050048Z</td>\n",
       "      <td>4.323736</td>\n",
       "      <td>492.465100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.316713</td>\n",
       "      <td>257.703156</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>2019-07-20T08:39:59.004013Z</td>\n",
       "      <td>4.323736</td>\n",
       "      <td>492.896881</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.313829</td>\n",
       "      <td>257.933868</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>Active</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14996 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Timestamp   FIT 101     LIT 101  MV 101  P1_STATE  \\\n",
       "0              2019-07-20T04:30:00Z  0.000000  729.865800       1         3   \n",
       "1              2019-07-20T04:30:01Z  0.000000  729.434000       1         3   \n",
       "2       2019-07-20T04:30:02.004013Z  0.000000  729.120000       1         3   \n",
       "3       2019-07-20T04:30:03.004013Z  0.000000  728.688200       1         3   \n",
       "4              2019-07-20T04:30:04Z  0.000000  727.706900       1         3   \n",
       "...                             ...       ...         ...     ...       ...   \n",
       "14991   2019-07-20T08:39:55.001007Z  4.200429  491.169769       2         2   \n",
       "14992  2019-07-20T08:39:56.0050048Z  4.253915  491.405273       2         2   \n",
       "14993  2019-07-20T08:39:57.0050048Z  4.303558  492.308100       2         2   \n",
       "14994  2019-07-20T08:39:58.0050048Z  4.323736  492.465100       2         2   \n",
       "14995   2019-07-20T08:39:59.004013Z  4.323736  492.896881       2         2   \n",
       "\n",
       "       P101 Status  P102 Status     AIT 201   AIT 202     AIT 203  ...  \\\n",
       "0                2            1  142.527557  9.293002  198.077423  ...   \n",
       "1                2            1  142.527557  9.293002  198.385025  ...   \n",
       "2                2            1  142.527557  9.293002  198.436300  ...   \n",
       "3                2            1  142.527557  9.289157  198.667000  ...   \n",
       "4                2            1  142.527557  9.289157  198.897720  ...   \n",
       "...            ...          ...         ...       ...         ...  ...   \n",
       "14991            2            1  131.408615  9.319918  257.703156  ...   \n",
       "14992            2            1  131.408615  9.317354  257.703156  ...   \n",
       "14993            2            1  131.408615  9.317354  257.703156  ...   \n",
       "14994            2            1  131.408615  9.316713  257.703156  ...   \n",
       "14995            2            1  131.408615  9.313829  257.933868  ...   \n",
       "\n",
       "       LSH 602   LSH 603   LSL 601   LSL 602 LSL 603  P6 STATE  P601 Status  \\\n",
       "0       Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "1       Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "2       Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "3       Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "4       Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "...        ...       ...       ...       ...     ...       ...          ...   \n",
       "14991   Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "14992   Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "14993   Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "14994   Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "14995   Active  Inactive  Inactive  Inactive  Active         2            1   \n",
       "\n",
       "       P602 Status  P603 Status   Label  \n",
       "0                1            1  Normal  \n",
       "1                1            1  Normal  \n",
       "2                1            1  Normal  \n",
       "3                1            1  Normal  \n",
       "4                1            1  Normal  \n",
       "...            ...          ...     ...  \n",
       "14991            1            1  Normal  \n",
       "14992            1            1  Normal  \n",
       "14993            1            1  Normal  \n",
       "14994            1            1  Normal  \n",
       "14995            1            1  Normal  \n",
       "\n",
       "[14996 rows x 79 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "529fdcbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIT 101</th>\n",
       "      <th>LIT 101</th>\n",
       "      <th>MV 101</th>\n",
       "      <th>P1_STATE</th>\n",
       "      <th>P101 Status</th>\n",
       "      <th>P102 Status</th>\n",
       "      <th>AIT 201</th>\n",
       "      <th>AIT 202</th>\n",
       "      <th>AIT 203</th>\n",
       "      <th>FIT 201</th>\n",
       "      <th>...</th>\n",
       "      <th>P501 Status</th>\n",
       "      <th>P502 Status</th>\n",
       "      <th>PIT 501</th>\n",
       "      <th>PIT 502</th>\n",
       "      <th>PIT 503</th>\n",
       "      <th>FIT 601</th>\n",
       "      <th>P6 STATE</th>\n",
       "      <th>P601 Status</th>\n",
       "      <th>P602 Status</th>\n",
       "      <th>P603 Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.0</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>14996.0</td>\n",
       "      <td>14996.0</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.0</td>\n",
       "      <td>14996.000000</td>\n",
       "      <td>14996.0</td>\n",
       "      <td>14996.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.740876</td>\n",
       "      <td>733.960251</td>\n",
       "      <td>1.156175</td>\n",
       "      <td>2.061616</td>\n",
       "      <td>1.374166</td>\n",
       "      <td>1.0</td>\n",
       "      <td>138.766501</td>\n",
       "      <td>9.210022</td>\n",
       "      <td>247.985162</td>\n",
       "      <td>0.869760</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160.706744</td>\n",
       "      <td>4.673115</td>\n",
       "      <td>115.157048</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.015337</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.634632</td>\n",
       "      <td>110.960185</td>\n",
       "      <td>0.384272</td>\n",
       "      <td>0.240466</td>\n",
       "      <td>0.483923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.265845</td>\n",
       "      <td>0.175812</td>\n",
       "      <td>11.806186</td>\n",
       "      <td>1.121283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.707502</td>\n",
       "      <td>18.183883</td>\n",
       "      <td>5.324942</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>491.169769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>113.849014</td>\n",
       "      <td>8.768457</td>\n",
       "      <td>198.077423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>158.132523</td>\n",
       "      <td>2.450902</td>\n",
       "      <td>111.654060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>640.595184</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>131.536789</td>\n",
       "      <td>9.090170</td>\n",
       "      <td>239.887200</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>159.526400</td>\n",
       "      <td>2.851376</td>\n",
       "      <td>114.233528</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>819.636841</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>143.713150</td>\n",
       "      <td>9.233082</td>\n",
       "      <td>246.218918</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160.231354</td>\n",
       "      <td>2.883414</td>\n",
       "      <td>114.714172</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>820.971436</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>144.033585</td>\n",
       "      <td>9.345873</td>\n",
       "      <td>257.190460</td>\n",
       "      <td>2.320187</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160.695969</td>\n",
       "      <td>2.963509</td>\n",
       "      <td>115.002563</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.403484</td>\n",
       "      <td>825.092957</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.821335</td>\n",
       "      <td>9.490067</td>\n",
       "      <td>272.289154</td>\n",
       "      <td>2.342357</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>219.014359</td>\n",
       "      <td>192.371765</td>\n",
       "      <td>170.565247</td>\n",
       "      <td>0.137315</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            FIT 101       LIT 101        MV 101      P1_STATE   P101 Status  \\\n",
       "count  14996.000000  14996.000000  14996.000000  14996.000000  14996.000000   \n",
       "mean       0.740876    733.960251      1.156175      2.061616      1.374166   \n",
       "std        1.634632    110.960185      0.384272      0.240466      0.483923   \n",
       "min        0.000000    491.169769      0.000000      2.000000      1.000000   \n",
       "25%        0.000000    640.595184      1.000000      2.000000      1.000000   \n",
       "50%        0.000000    819.636841      1.000000      2.000000      1.000000   \n",
       "75%        0.000000    820.971436      1.000000      2.000000      2.000000   \n",
       "max        4.403484    825.092957      2.000000      3.000000      2.000000   \n",
       "\n",
       "       P102 Status       AIT 201       AIT 202       AIT 203       FIT 201  \\\n",
       "count      14996.0  14996.000000  14996.000000  14996.000000  14996.000000   \n",
       "mean           1.0    138.766501      9.210022    247.985162      0.869760   \n",
       "std            0.0      8.265845      0.175812     11.806186      1.121283   \n",
       "min            1.0    113.849014      8.768457    198.077423      0.000000   \n",
       "25%            1.0    131.536789      9.090170    239.887200      0.000384   \n",
       "50%            1.0    143.713150      9.233082    246.218918      0.000513   \n",
       "75%            1.0    144.033585      9.345873    257.190460      2.320187   \n",
       "max            1.0    146.821335      9.490067    272.289154      2.342357   \n",
       "\n",
       "       ...  P501 Status  P502 Status       PIT 501       PIT 502  \\\n",
       "count  ...      14996.0      14996.0  14996.000000  14996.000000   \n",
       "mean   ...          2.0          1.0    160.706744      4.673115   \n",
       "std    ...          0.0          0.0      5.707502     18.183883   \n",
       "min    ...          2.0          1.0    158.132523      2.450902   \n",
       "25%    ...          2.0          1.0    159.526400      2.851376   \n",
       "50%    ...          2.0          1.0    160.231354      2.883414   \n",
       "75%    ...          2.0          1.0    160.695969      2.963509   \n",
       "max    ...          2.0          1.0    219.014359    192.371765   \n",
       "\n",
       "            PIT 503       FIT 601  P6 STATE   P601 Status  P602 Status  \\\n",
       "count  14996.000000  14996.000000   14996.0  14996.000000      14996.0   \n",
       "mean     115.157048      0.000402       2.0      1.015337          1.0   \n",
       "std        5.324942      0.003317       0.0      0.122895          0.0   \n",
       "min      111.654060      0.000000       2.0      1.000000          1.0   \n",
       "25%      114.233528      0.000256       2.0      1.000000          1.0   \n",
       "50%      114.714172      0.000320       2.0      1.000000          1.0   \n",
       "75%      115.002563      0.000320       2.0      1.000000          1.0   \n",
       "max      170.565247      0.137315       2.0      2.000000          1.0   \n",
       "\n",
       "       P603 Status  \n",
       "count      14996.0  \n",
       "mean           1.0  \n",
       "std            0.0  \n",
       "min            1.0  \n",
       "25%            1.0  \n",
       "50%            1.0  \n",
       "75%            1.0  \n",
       "max            1.0  \n",
       "\n",
       "[8 rows x 66 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37b36344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Timestamp', 'FIT 101', 'LIT 101', 'MV 101', 'P1_STATE', 'P101 Status',\n",
       "       'P102 Status', 'AIT 201', 'AIT 202', 'AIT 203', 'FIT 201', 'LS 201',\n",
       "       'LS 202', 'LSL 203', 'LSLL 203', 'MV201', 'P2_STATE', 'P201 Status',\n",
       "       'P202 Status', 'P203 Status', 'P204 Status', 'P205 Status',\n",
       "       'P206 Status', 'P207 Status', 'P208 Status', 'AIT 301', 'AIT 302',\n",
       "       'AIT 303', 'DPIT 301', 'FIT 301', 'LIT 301', 'MV 301', 'MV 302',\n",
       "       'MV 303', 'MV 304', 'P3_STATE', 'P301 Status', 'P302 Status', 'AIT 401',\n",
       "       'AIT 402', 'FIT 401', 'LIT 401', 'LS 401', 'P4_STATE', 'P401 Status',\n",
       "       'P402 Status', 'P403 Status', 'P404 Status', 'UV401', 'AIT 501',\n",
       "       'AIT 502', 'AIT 503', 'AIT 504', 'FIT 501', 'FIT 502', 'FIT 503',\n",
       "       'FIT 504', 'MV 501', 'MV 502', 'MV 503', 'MV 504', 'P5_STATE',\n",
       "       'P501 Status', 'P502 Status', 'PIT 501', 'PIT 502', 'PIT 503',\n",
       "       'FIT 601', 'LSH 601', 'LSH 602', 'LSH 603', 'LSL 601', 'LSL 602',\n",
       "       'LSL 603', 'P6 STATE', 'P601 Status', 'P602 Status', 'P603 Status',\n",
       "       'Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2d88242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal    13016\n",
       "Attack     1980\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9860d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.drop(['Timestamp'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da0f5c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "['LS 201', 'LS 202', 'LSL 203', 'LSLL 203', 'LS 401', 'LSH 601', 'LSH 602', 'LSH 603', 'LSL 601', 'LSL 602', 'LSL 603', 'Label']\n"
     ]
    }
   ],
   "source": [
    "# Get list of categorical variables\n",
    "s = (test_df.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeeccf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIT 101</th>\n",
       "      <th>LIT 101</th>\n",
       "      <th>MV 101</th>\n",
       "      <th>P1_STATE</th>\n",
       "      <th>P101 Status</th>\n",
       "      <th>P102 Status</th>\n",
       "      <th>AIT 201</th>\n",
       "      <th>AIT 202</th>\n",
       "      <th>AIT 203</th>\n",
       "      <th>FIT 201</th>\n",
       "      <th>...</th>\n",
       "      <th>LSH 602</th>\n",
       "      <th>LSH 603</th>\n",
       "      <th>LSL 601</th>\n",
       "      <th>LSL 602</th>\n",
       "      <th>LSL 603</th>\n",
       "      <th>P6 STATE</th>\n",
       "      <th>P601 Status</th>\n",
       "      <th>P602 Status</th>\n",
       "      <th>P603 Status</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>729.865800</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.293002</td>\n",
       "      <td>198.077423</td>\n",
       "      <td>2.335437</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>729.434000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.293002</td>\n",
       "      <td>198.385025</td>\n",
       "      <td>2.335437</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>729.120000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.293002</td>\n",
       "      <td>198.436300</td>\n",
       "      <td>2.335437</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>728.688200</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.289157</td>\n",
       "      <td>198.667000</td>\n",
       "      <td>2.335437</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>727.706900</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142.527557</td>\n",
       "      <td>9.289157</td>\n",
       "      <td>198.897720</td>\n",
       "      <td>2.335437</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14991</th>\n",
       "      <td>4.200429</td>\n",
       "      <td>491.169769</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.319918</td>\n",
       "      <td>257.703156</td>\n",
       "      <td>2.316086</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14992</th>\n",
       "      <td>4.253915</td>\n",
       "      <td>491.405273</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.317354</td>\n",
       "      <td>257.703156</td>\n",
       "      <td>2.314292</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14993</th>\n",
       "      <td>4.303558</td>\n",
       "      <td>492.308100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.317354</td>\n",
       "      <td>257.703156</td>\n",
       "      <td>2.313651</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>4.323736</td>\n",
       "      <td>492.465100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.316713</td>\n",
       "      <td>257.703156</td>\n",
       "      <td>2.313651</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>4.323736</td>\n",
       "      <td>492.896881</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131.408615</td>\n",
       "      <td>9.313829</td>\n",
       "      <td>257.933868</td>\n",
       "      <td>2.313651</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14996 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FIT 101     LIT 101  MV 101  P1_STATE  P101 Status  P102 Status  \\\n",
       "0      0.000000  729.865800       1         3            2            1   \n",
       "1      0.000000  729.434000       1         3            2            1   \n",
       "2      0.000000  729.120000       1         3            2            1   \n",
       "3      0.000000  728.688200       1         3            2            1   \n",
       "4      0.000000  727.706900       1         3            2            1   \n",
       "...         ...         ...     ...       ...          ...          ...   \n",
       "14991  4.200429  491.169769       2         2            2            1   \n",
       "14992  4.253915  491.405273       2         2            2            1   \n",
       "14993  4.303558  492.308100       2         2            2            1   \n",
       "14994  4.323736  492.465100       2         2            2            1   \n",
       "14995  4.323736  492.896881       2         2            2            1   \n",
       "\n",
       "          AIT 201   AIT 202     AIT 203   FIT 201  ...  LSH 602  LSH 603  \\\n",
       "0      142.527557  9.293002  198.077423  2.335437  ...        0        0   \n",
       "1      142.527557  9.293002  198.385025  2.335437  ...        0        0   \n",
       "2      142.527557  9.293002  198.436300  2.335437  ...        0        0   \n",
       "3      142.527557  9.289157  198.667000  2.335437  ...        0        0   \n",
       "4      142.527557  9.289157  198.897720  2.335437  ...        0        0   \n",
       "...           ...       ...         ...       ...  ...      ...      ...   \n",
       "14991  131.408615  9.319918  257.703156  2.316086  ...        0        0   \n",
       "14992  131.408615  9.317354  257.703156  2.314292  ...        0        0   \n",
       "14993  131.408615  9.317354  257.703156  2.313651  ...        0        0   \n",
       "14994  131.408615  9.316713  257.703156  2.313651  ...        0        0   \n",
       "14995  131.408615  9.313829  257.933868  2.313651  ...        0        0   \n",
       "\n",
       "       LSL 601  LSL 602  LSL 603  P6 STATE  P601 Status  P602 Status  \\\n",
       "0            0        0        0         2            1            1   \n",
       "1            0        0        0         2            1            1   \n",
       "2            0        0        0         2            1            1   \n",
       "3            0        0        0         2            1            1   \n",
       "4            0        0        0         2            1            1   \n",
       "...        ...      ...      ...       ...          ...          ...   \n",
       "14991        0        0        0         2            1            1   \n",
       "14992        0        0        0         2            1            1   \n",
       "14993        0        0        0         2            1            1   \n",
       "14994        0        0        0         2            1            1   \n",
       "14995        0        0        0         2            1            1   \n",
       "\n",
       "       P603 Status  Label  \n",
       "0                1      1  \n",
       "1                1      1  \n",
       "2                1      1  \n",
       "3                1      1  \n",
       "4                1      1  \n",
       "...            ...    ...  \n",
       "14991            1      1  \n",
       "14992            1      1  \n",
       "14993            1      1  \n",
       "14994            1      1  \n",
       "14995            1      1  \n",
       "\n",
       "[14996 rows x 78 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_df = test_df.copy()\n",
    "\n",
    "# inactive = 0 active = 1     label: normal = 1 , attack = 0\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    label_df[col] = label_encoder.fit_transform(test_df[col])\n",
    "\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daaf928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = label_df.Label\n",
    "X = label_df.drop(['Label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6188179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7393264b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    13016\n",
       "0     1980\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d02c2b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.71482317, 0.5       , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.71353006, 0.5       , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.71258972, 0.5       , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.97730752, 0.00340896, 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.98188965, 0.00387913, 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.98188965, 0.00517218, 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized=scaler.fit_transform(values)\n",
    "X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b65a2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# # Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_normalized, Y, train_size=0.9, test_size=0.1,\n",
    "                                                                random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe7750f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13496, 77)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae029240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert to time domain dataset\n",
    "def create_dataset(X, time_steps):\n",
    "    Xs = []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "    return np.array(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11e11c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13495, 1, 77)\n",
      "(13495, 1, 1)\n",
      "(1499, 1, 77)\n",
      "(1499, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape to [samples, time_steps, n_features]\n",
    "\n",
    "TIME_STEPS = 1\n",
    "X_train = pd.DataFrame(X_train_full)\n",
    "X_train = create_dataset(X_train, TIME_STEPS)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_train = create_dataset(y_train, TIME_STEPS)\n",
    "X_valid = pd.DataFrame(X_valid_full)\n",
    "X_valid = create_dataset(X_valid, TIME_STEPS)\n",
    "y_valid = pd.DataFrame(y_valid)\n",
    "y_valid = create_dataset(y_valid, TIME_STEPS)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94671d5",
   "metadata": {},
   "source": [
    "# Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5673d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,input_size, output_size, hidden_size, num_layers):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        #layer 1\n",
    "        self.lstm = nn.LSTM(self.input_size, \n",
    "                            self.hidden_size,\n",
    "                            self.num_layers,\n",
    "                            batch_first=False,dropout = 0)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        #layer 2\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "      \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "     \n",
    "    def forward(self, x):     \n",
    "        per_out=[]\n",
    "        lstm_out, self.hidden_cell = self.lstm(x)\n",
    "        per_out.append(lstm_out)\n",
    "        lstm_out = self.dropout(lstm_out)    \n",
    "        out = self.fc(lstm_out)\n",
    "        per_out.append(out)\n",
    "        score = self.sigmoid(out)\n",
    "        return score, per_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "925ecb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(x,y,model):\n",
    "    output,pre_out = model(x)\n",
    "    output = torch.reshape(output,[-1,1])\n",
    "    correct = (output.ge(0.5) == y).sum().item()\n",
    "    n = y.shape[0]\n",
    "    return correct/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70f3b61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 77\n",
    "output_size = 1\n",
    "hidden_size = 20\n",
    "num_layers =2\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = LSTM(input_size,output_size, hidden_size, num_layers).to(device)\n",
    "loss_function = nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "# optimizer = torch.optim.SGD(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad5b7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataloader(X_train_full,y_train,X_valid_full,y_valid):\n",
    "    TIME_STEPS = 1\n",
    "    X_train = pd.DataFrame(X_train_full)\n",
    "    X_train = create_dataset(X_train, TIME_STEPS)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_train = create_dataset(y_train, TIME_STEPS)\n",
    "    X_valid = pd.DataFrame(X_valid_full)\n",
    "    X_valid = create_dataset(X_valid, TIME_STEPS)\n",
    "    y_valid = pd.DataFrame(y_valid)\n",
    "    y_valid = create_dataset(y_valid, TIME_STEPS)\n",
    "# 将输入和输出封装进Data.TensorDataset()类对象\n",
    "    x = torch.tensor(X_train[:,:,:]).float()\n",
    "    x = torch.where(torch.isnan(x), torch.full_like(x, 0), x)\n",
    "    y = y_train[:]\n",
    "    y = np.array(y)\n",
    "# y = torch.tensor(np.reshape(y,[-1,1]))\n",
    "    y = torch.tensor(y)\n",
    "    y = y.float()\n",
    "    torch_dataset = Data.TensorDataset(x,y)\n",
    "    train_dataloader = torch.utils.data.DataLoader(torch_dataset, batch_size=60, shuffle=False, num_workers=0)\n",
    "    return X_train, y_train, X_valid, y_valid, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcb4fe40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13495, 1, 77])\n",
      "torch.Size([13495, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as Data\n",
    "# 将输入和输出封装进Data.TensorDataset()类对象\n",
    "x = torch.tensor(X_train[:,:,:]).float()\n",
    "x = torch.where(torch.isnan(x), torch.full_like(x, 0), x)\n",
    "    \n",
    "y = y_train[:]\n",
    "y = np.array(y)\n",
    "# y = torch.tensor(np.reshape(y,[-1,1]))\n",
    "y = torch.tensor(y)\n",
    "y = y.float()\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "torch_dataset = Data.TensorDataset(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a65fe52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 60\n",
    "train_dataloader = torch.utils.data.DataLoader(torch_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dc66e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9d5ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_array = np.zeros(len(train_dataloader))\n",
    "batch_index_array = np.zeros(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5688e8d0",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2eae93ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch:0 loss:0.7012244462966919 acc:0.26666666666666666\n",
      "epoch:0 batch:1 loss:0.6923128366470337 acc:0.5333333333333333\n",
      "epoch:0 batch:2 loss:0.6826073527336121 acc:0.7833333333333333\n",
      "epoch:0 batch:3 loss:0.6670244932174683 acc:0.9\n",
      "epoch:0 batch:4 loss:0.6533950567245483 acc:0.9166666666666666\n",
      "epoch:0 batch:5 loss:0.6438571214675903 acc:0.95\n",
      "epoch:0 batch:6 loss:0.6308839321136475 acc:0.95\n",
      "epoch:0 batch:7 loss:0.6154579520225525 acc:0.9666666666666667\n",
      "epoch:0 batch:8 loss:0.6109666228294373 acc:0.9833333333333333\n",
      "epoch:0 batch:9 loss:0.6095412373542786 acc:0.9833333333333333\n",
      "epoch:0 batch:10 loss:0.5880549550056458 acc:0.9833333333333333\n",
      "epoch:0 batch:11 loss:0.575346827507019 acc:0.9833333333333333\n",
      "epoch:0 batch:12 loss:0.5619862675666809 acc:0.9833333333333333\n",
      "epoch:0 batch:13 loss:0.5488240718841553 acc:1.0\n",
      "epoch:0 batch:14 loss:0.5303394794464111 acc:1.0\n",
      "epoch:0 batch:15 loss:0.5204190015792847 acc:1.0\n",
      "epoch:0 batch:16 loss:0.5036876201629639 acc:1.0\n",
      "epoch:0 batch:17 loss:0.48306331038475037 acc:1.0\n",
      "epoch:0 batch:18 loss:0.47458502650260925 acc:1.0\n",
      "epoch:0 batch:19 loss:0.4465036690235138 acc:1.0\n",
      "epoch:0 batch:20 loss:0.42901790142059326 acc:1.0\n",
      "epoch:0 batch:21 loss:0.41503843665122986 acc:1.0\n",
      "epoch:0 batch:22 loss:0.40323832631111145 acc:1.0\n",
      "epoch:0 batch:23 loss:0.3756007254123688 acc:1.0\n",
      "epoch:0 batch:24 loss:0.37471967935562134 acc:1.0\n",
      "epoch:0 batch:25 loss:0.34287187457084656 acc:1.0\n",
      "epoch:0 batch:26 loss:0.3362969160079956 acc:1.0\n",
      "epoch:0 batch:27 loss:0.30294284224510193 acc:1.0\n",
      "epoch:0 batch:28 loss:0.28399521112442017 acc:1.0\n",
      "epoch:0 batch:29 loss:0.26472043991088867 acc:1.0\n",
      "epoch:0 batch:30 loss:0.25217804312705994 acc:1.0\n",
      "epoch:0 batch:31 loss:0.2467322200536728 acc:1.0\n",
      "epoch:0 batch:32 loss:0.2225896120071411 acc:1.0\n",
      "epoch:0 batch:33 loss:0.21203379333019257 acc:1.0\n",
      "epoch:0 batch:34 loss:0.20868690311908722 acc:1.0\n",
      "epoch:0 batch:35 loss:0.19512654840946198 acc:1.0\n",
      "epoch:0 batch:36 loss:0.18186742067337036 acc:1.0\n",
      "epoch:0 batch:37 loss:0.1653996706008911 acc:1.0\n",
      "epoch:0 batch:38 loss:0.1687556505203247 acc:1.0\n",
      "epoch:0 batch:39 loss:0.14645053446292877 acc:1.0\n",
      "epoch:0 batch:40 loss:0.15138471126556396 acc:1.0\n",
      "epoch:0 batch:41 loss:0.13234733045101166 acc:1.0\n",
      "epoch:0 batch:42 loss:0.12394207715988159 acc:1.0\n",
      "epoch:0 batch:43 loss:0.13316810131072998 acc:1.0\n",
      "epoch:0 batch:44 loss:0.11924675107002258 acc:1.0\n",
      "epoch:0 batch:45 loss:0.11326168477535248 acc:1.0\n",
      "epoch:0 batch:46 loss:0.11129267513751984 acc:1.0\n",
      "epoch:0 batch:47 loss:0.1036260649561882 acc:1.0\n",
      "epoch:0 batch:48 loss:0.09957392513751984 acc:1.0\n",
      "epoch:0 batch:49 loss:0.09518520534038544 acc:1.0\n",
      "epoch:0 batch:50 loss:0.09321899712085724 acc:1.0\n",
      "epoch:0 batch:51 loss:0.0925329253077507 acc:1.0\n",
      "epoch:0 batch:52 loss:0.09332277625799179 acc:1.0\n",
      "epoch:0 batch:53 loss:0.08785205334424973 acc:1.0\n",
      "epoch:0 batch:54 loss:0.08086179941892624 acc:1.0\n",
      "epoch:0 batch:55 loss:0.0867413580417633 acc:1.0\n",
      "epoch:0 batch:56 loss:0.07864373177289963 acc:1.0\n",
      "epoch:0 batch:57 loss:0.08107813447713852 acc:1.0\n",
      "epoch:0 batch:58 loss:0.07217042148113251 acc:1.0\n",
      "epoch:0 batch:59 loss:0.07445850968360901 acc:1.0\n",
      "epoch:0 batch:60 loss:0.06765805184841156 acc:1.0\n",
      "epoch:0 batch:61 loss:0.06963920593261719 acc:1.0\n",
      "epoch:0 batch:62 loss:0.06521646678447723 acc:1.0\n",
      "epoch:0 batch:63 loss:0.07297270745038986 acc:1.0\n",
      "epoch:0 batch:64 loss:0.06371854245662689 acc:1.0\n",
      "epoch:0 batch:65 loss:0.06597598642110825 acc:1.0\n",
      "epoch:0 batch:66 loss:0.07223383337259293 acc:1.0\n",
      "epoch:0 batch:67 loss:0.06677288562059402 acc:1.0\n",
      "epoch:0 batch:68 loss:0.06383056938648224 acc:1.0\n",
      "epoch:0 batch:69 loss:0.05633676424622536 acc:1.0\n",
      "epoch:0 batch:70 loss:0.0619819201529026 acc:1.0\n",
      "epoch:0 batch:71 loss:0.06010376289486885 acc:1.0\n",
      "epoch:0 batch:72 loss:0.05760006234049797 acc:1.0\n",
      "epoch:0 batch:73 loss:0.058357495814561844 acc:1.0\n",
      "epoch:0 batch:74 loss:0.05459250137209892 acc:1.0\n",
      "epoch:0 batch:75 loss:0.054277047514915466 acc:1.0\n",
      "epoch:0 batch:76 loss:0.04895367473363876 acc:1.0\n",
      "epoch:0 batch:77 loss:0.05671131983399391 acc:1.0\n",
      "epoch:0 batch:78 loss:0.054949719458818436 acc:1.0\n",
      "epoch:0 batch:79 loss:0.049211133271455765 acc:1.0\n",
      "epoch:0 batch:80 loss:0.051312919706106186 acc:1.0\n",
      "epoch:0 batch:81 loss:0.04461866617202759 acc:1.0\n",
      "epoch:0 batch:82 loss:0.04998362809419632 acc:1.0\n",
      "epoch:0 batch:83 loss:0.0438232496380806 acc:1.0\n",
      "epoch:0 batch:84 loss:0.04540431872010231 acc:1.0\n",
      "epoch:0 batch:85 loss:0.043080735951662064 acc:1.0\n",
      "epoch:0 batch:86 loss:0.04425575211644173 acc:1.0\n",
      "epoch:0 batch:87 loss:0.04632822796702385 acc:1.0\n",
      "epoch:0 batch:88 loss:0.042972028255462646 acc:1.0\n",
      "epoch:0 batch:89 loss:0.04830267280340195 acc:1.0\n",
      "epoch:0 batch:90 loss:0.040338996797800064 acc:1.0\n",
      "epoch:0 batch:91 loss:0.04299359768629074 acc:1.0\n",
      "epoch:0 batch:92 loss:0.04103913530707359 acc:1.0\n",
      "epoch:0 batch:93 loss:0.038296498358249664 acc:1.0\n",
      "epoch:0 batch:94 loss:0.0405239537358284 acc:1.0\n",
      "epoch:0 batch:95 loss:0.0398726649582386 acc:1.0\n",
      "epoch:0 batch:96 loss:0.04068685695528984 acc:1.0\n",
      "epoch:0 batch:97 loss:0.03676711395382881 acc:1.0\n",
      "epoch:0 batch:98 loss:0.03870834782719612 acc:1.0\n",
      "epoch:0 batch:99 loss:0.04077718406915665 acc:1.0\n",
      "epoch:0 batch:100 loss:0.033289097249507904 acc:1.0\n",
      "epoch:0 batch:101 loss:0.040343690663576126 acc:1.0\n",
      "epoch:0 batch:102 loss:0.03967418894171715 acc:1.0\n",
      "epoch:0 batch:103 loss:0.04029514640569687 acc:1.0\n",
      "epoch:0 batch:104 loss:0.03587571531534195 acc:1.0\n",
      "epoch:0 batch:105 loss:0.034386176615953445 acc:1.0\n",
      "epoch:0 batch:106 loss:0.03295218572020531 acc:1.0\n",
      "epoch:0 batch:107 loss:0.03552911430597305 acc:1.0\n",
      "epoch:0 batch:108 loss:0.03493000566959381 acc:1.0\n",
      "epoch:0 batch:109 loss:0.0310568455606699 acc:1.0\n",
      "epoch:0 batch:110 loss:0.03425309807062149 acc:1.0\n",
      "epoch:0 batch:111 loss:0.034412164241075516 acc:1.0\n",
      "epoch:0 batch:112 loss:0.03277759626507759 acc:1.0\n",
      "epoch:0 batch:113 loss:0.036788392812013626 acc:1.0\n",
      "epoch:0 batch:114 loss:0.029846828430891037 acc:1.0\n",
      "epoch:0 batch:115 loss:0.033257611095905304 acc:1.0\n",
      "epoch:0 batch:116 loss:0.030817249789834023 acc:1.0\n",
      "epoch:0 batch:117 loss:0.031344957649707794 acc:1.0\n",
      "epoch:0 batch:118 loss:0.03406180813908577 acc:1.0\n",
      "epoch:0 batch:119 loss:0.030644839629530907 acc:1.0\n",
      "epoch:0 batch:120 loss:0.027133183553814888 acc:1.0\n",
      "epoch:0 batch:121 loss:0.03078765980899334 acc:1.0\n",
      "epoch:0 batch:122 loss:0.028554949909448624 acc:1.0\n",
      "epoch:0 batch:123 loss:0.03181590512394905 acc:1.0\n",
      "epoch:0 batch:124 loss:0.03009272739291191 acc:1.0\n",
      "epoch:0 batch:125 loss:0.02661733701825142 acc:1.0\n",
      "epoch:0 batch:126 loss:0.027845896780490875 acc:1.0\n",
      "epoch:0 batch:127 loss:0.03125225752592087 acc:1.0\n",
      "epoch:0 batch:128 loss:0.028648067265748978 acc:1.0\n",
      "epoch:0 batch:129 loss:0.03285805881023407 acc:1.0\n",
      "epoch:0 batch:130 loss:0.030800335109233856 acc:1.0\n",
      "epoch:0 batch:131 loss:0.026870526373386383 acc:1.0\n",
      "epoch:0 batch:132 loss:0.034048281610012054 acc:1.0\n",
      "epoch:0 batch:133 loss:0.028200414031744003 acc:1.0\n",
      "epoch:0 batch:134 loss:0.031740956008434296 acc:1.0\n",
      "epoch:0 batch:135 loss:0.02659536525607109 acc:1.0\n",
      "epoch:0 batch:136 loss:0.03220073878765106 acc:1.0\n",
      "epoch:0 batch:137 loss:0.026433190330863 acc:1.0\n",
      "epoch:0 batch:138 loss:0.023926982656121254 acc:1.0\n",
      "epoch:0 batch:139 loss:0.024976950138807297 acc:1.0\n",
      "epoch:0 batch:140 loss:0.026427656412124634 acc:1.0\n",
      "epoch:0 batch:141 loss:0.029331693425774574 acc:1.0\n",
      "epoch:0 batch:142 loss:0.02624010294675827 acc:1.0\n",
      "epoch:0 batch:143 loss:0.023800887167453766 acc:1.0\n",
      "epoch:0 batch:144 loss:0.024461857974529266 acc:1.0\n",
      "epoch:0 batch:145 loss:0.028506403788924217 acc:1.0\n",
      "epoch:0 batch:146 loss:0.026837436482310295 acc:1.0\n",
      "epoch:0 batch:147 loss:0.026657801121473312 acc:1.0\n",
      "epoch:0 batch:148 loss:0.02778540924191475 acc:1.0\n",
      "epoch:0 batch:149 loss:0.024202875792980194 acc:1.0\n",
      "epoch:0 batch:150 loss:0.025212226435542107 acc:1.0\n",
      "epoch:0 batch:151 loss:0.026630299165844917 acc:1.0\n",
      "epoch:0 batch:152 loss:0.02382451482117176 acc:1.0\n",
      "epoch:0 batch:153 loss:0.023827824741601944 acc:1.0\n",
      "epoch:0 batch:154 loss:0.022789083421230316 acc:1.0\n",
      "epoch:0 batch:155 loss:0.025612346827983856 acc:1.0\n",
      "epoch:0 batch:156 loss:0.02504347451031208 acc:1.0\n",
      "epoch:0 batch:157 loss:0.023261625319719315 acc:1.0\n",
      "epoch:0 batch:158 loss:1.216511607170105 acc:0.7166666666666667\n",
      "epoch:0 batch:159 loss:4.309131145477295 acc:0.0\n",
      "epoch:0 batch:160 loss:1.9129379987716675 acc:0.5333333333333333\n",
      "epoch:0 batch:161 loss:0.02338642068207264 acc:1.0\n",
      "epoch:0 batch:162 loss:0.02270740643143654 acc:1.0\n",
      "epoch:0 batch:163 loss:0.025415435433387756 acc:1.0\n",
      "epoch:0 batch:164 loss:0.24248448014259338 acc:0.95\n",
      "epoch:0 batch:165 loss:3.986511468887329 acc:0.0\n",
      "epoch:0 batch:166 loss:4.080427646636963 acc:0.0\n",
      "epoch:0 batch:167 loss:4.013369560241699 acc:0.0\n",
      "epoch:0 batch:168 loss:3.7910823822021484 acc:0.0\n",
      "epoch:0 batch:169 loss:1.8899028301239014 acc:0.5166666666666667\n",
      "epoch:0 batch:170 loss:0.03277328982949257 acc:1.0\n",
      "epoch:0 batch:171 loss:0.03498922660946846 acc:1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch:172 loss:0.03721851855516434 acc:1.0\n",
      "epoch:0 batch:173 loss:0.03754488006234169 acc:1.0\n",
      "epoch:0 batch:174 loss:0.04117433354258537 acc:1.0\n",
      "epoch:0 batch:175 loss:0.03714853152632713 acc:1.0\n",
      "epoch:0 batch:176 loss:0.40109872817993164 acc:0.9\n",
      "epoch:0 batch:177 loss:3.490652561187744 acc:0.0\n",
      "epoch:0 batch:178 loss:3.523879289627075 acc:0.0\n",
      "epoch:0 batch:179 loss:3.366628646850586 acc:0.0\n",
      "epoch:0 batch:180 loss:2.493889808654785 acc:0.26666666666666666\n",
      "epoch:0 batch:181 loss:0.049656178802251816 acc:1.0\n",
      "epoch:0 batch:182 loss:0.05254127457737923 acc:1.0\n",
      "epoch:0 batch:183 loss:0.051224660128355026 acc:1.0\n",
      "epoch:0 batch:184 loss:0.05647179111838341 acc:1.0\n",
      "epoch:0 batch:185 loss:0.06343787163496017 acc:1.0\n",
      "epoch:0 batch:186 loss:0.0622171126306057 acc:1.0\n",
      "epoch:0 batch:187 loss:0.06592510640621185 acc:1.0\n",
      "epoch:0 batch:188 loss:0.7560639977455139 acc:0.7833333333333333\n",
      "epoch:0 batch:189 loss:3.0036473274230957 acc:0.0\n",
      "epoch:0 batch:190 loss:2.9398398399353027 acc:0.0\n",
      "epoch:0 batch:191 loss:2.921476125717163 acc:0.0\n",
      "epoch:0 batch:192 loss:2.8151955604553223 acc:0.0\n",
      "epoch:0 batch:193 loss:2.652019500732422 acc:0.0\n",
      "epoch:0 batch:194 loss:2.634660005569458 acc:0.0\n",
      "epoch:0 batch:195 loss:2.625605583190918 acc:0.0\n",
      "epoch:0 batch:196 loss:0.6999992728233337 acc:0.7166666666666667\n",
      "epoch:0 batch:197 loss:0.10512129962444305 acc:1.0\n",
      "epoch:0 batch:198 loss:0.11749716103076935 acc:1.0\n",
      "epoch:0 batch:199 loss:0.12402990460395813 acc:1.0\n",
      "epoch:0 batch:200 loss:0.1266189068555832 acc:1.0\n",
      "epoch:0 batch:201 loss:0.14515404403209686 acc:1.0\n",
      "epoch:0 batch:202 loss:0.15337707102298737 acc:1.0\n",
      "epoch:0 batch:203 loss:0.24990785121917725 acc:0.95\n",
      "epoch:0 batch:204 loss:2.063403844833374 acc:0.0\n",
      "epoch:0 batch:205 loss:1.904172658920288 acc:0.03333333333333333\n",
      "epoch:0 batch:206 loss:0.15770308673381805 acc:1.0\n",
      "epoch:0 batch:207 loss:0.17302018404006958 acc:1.0\n",
      "epoch:0 batch:208 loss:0.17011506855487823 acc:1.0\n",
      "epoch:0 batch:209 loss:0.18672899901866913 acc:1.0\n",
      "epoch:0 batch:210 loss:0.19582819938659668 acc:1.0\n",
      "epoch:0 batch:211 loss:0.1971532106399536 acc:1.0\n",
      "epoch:0 batch:212 loss:0.37345290184020996 acc:0.8833333333333333\n",
      "epoch:0 batch:213 loss:1.774275302886963 acc:0.0\n",
      "epoch:0 batch:214 loss:1.7645139694213867 acc:0.0\n",
      "epoch:0 batch:215 loss:1.7028521299362183 acc:0.0\n",
      "epoch:0 batch:216 loss:1.6572718620300293 acc:0.0\n",
      "epoch:0 batch:217 loss:1.596999168395996 acc:0.0\n",
      "epoch:0 batch:218 loss:1.5556427240371704 acc:0.0\n",
      "epoch:0 batch:219 loss:1.5086545944213867 acc:0.0\n",
      "epoch:0 batch:220 loss:1.4607213735580444 acc:0.0\n",
      "epoch:0 batch:221 loss:1.3538622856140137 acc:0.0\n",
      "epoch:0 batch:222 loss:1.3002729415893555 acc:0.0\n",
      "epoch:0 batch:223 loss:1.2417796850204468 acc:0.0\n",
      "epoch:0 batch:224 loss:1.1384416818618774 acc:0.0\n",
      "epoch:5 batch:0 loss:0.2627181112766266 acc:1.0\n",
      "epoch:5 batch:1 loss:0.2637001574039459 acc:1.0\n",
      "epoch:5 batch:2 loss:0.2695080041885376 acc:1.0\n",
      "epoch:5 batch:3 loss:0.27441951632499695 acc:1.0\n",
      "epoch:5 batch:4 loss:0.28814896941185 acc:1.0\n",
      "epoch:5 batch:5 loss:0.28777918219566345 acc:1.0\n",
      "epoch:5 batch:6 loss:0.2845657765865326 acc:1.0\n",
      "epoch:5 batch:7 loss:0.2961636483669281 acc:1.0\n",
      "epoch:5 batch:8 loss:0.28691330552101135 acc:1.0\n",
      "epoch:5 batch:9 loss:0.29935595393180847 acc:1.0\n",
      "epoch:5 batch:10 loss:0.2950856685638428 acc:1.0\n",
      "epoch:5 batch:11 loss:0.30299708247184753 acc:1.0\n",
      "epoch:5 batch:12 loss:0.2930550277233124 acc:1.0\n",
      "epoch:5 batch:13 loss:0.29921993613243103 acc:1.0\n",
      "epoch:5 batch:14 loss:0.28947800397872925 acc:1.0\n",
      "epoch:5 batch:15 loss:0.29464495182037354 acc:1.0\n",
      "epoch:5 batch:16 loss:0.31872835755348206 acc:1.0\n",
      "epoch:5 batch:17 loss:0.30006691813468933 acc:1.0\n",
      "epoch:5 batch:18 loss:0.3160318434238434 acc:1.0\n",
      "epoch:5 batch:19 loss:0.3038466274738312 acc:1.0\n",
      "epoch:5 batch:20 loss:0.31337836384773254 acc:1.0\n",
      "epoch:5 batch:21 loss:0.2992171049118042 acc:1.0\n",
      "epoch:5 batch:22 loss:0.29812297224998474 acc:1.0\n",
      "epoch:5 batch:23 loss:0.2960149347782135 acc:1.0\n",
      "epoch:5 batch:24 loss:0.2992817461490631 acc:1.0\n",
      "epoch:5 batch:25 loss:0.29935553669929504 acc:1.0\n",
      "epoch:5 batch:26 loss:0.2886507213115692 acc:1.0\n",
      "epoch:5 batch:27 loss:0.28053557872772217 acc:1.0\n",
      "epoch:5 batch:28 loss:0.28025832772254944 acc:1.0\n",
      "epoch:5 batch:29 loss:0.27282068133354187 acc:1.0\n",
      "epoch:5 batch:30 loss:0.2798271179199219 acc:1.0\n",
      "epoch:5 batch:31 loss:0.2677241265773773 acc:1.0\n",
      "epoch:5 batch:32 loss:0.2656055986881256 acc:1.0\n",
      "epoch:5 batch:33 loss:0.25984323024749756 acc:1.0\n",
      "epoch:5 batch:34 loss:0.24571490287780762 acc:1.0\n",
      "epoch:5 batch:35 loss:0.2511492371559143 acc:1.0\n",
      "epoch:5 batch:36 loss:0.25278279185295105 acc:1.0\n",
      "epoch:5 batch:37 loss:0.24681594967842102 acc:1.0\n",
      "epoch:5 batch:38 loss:0.24952971935272217 acc:1.0\n",
      "epoch:5 batch:39 loss:0.23987136781215668 acc:1.0\n",
      "epoch:5 batch:40 loss:0.23127341270446777 acc:1.0\n",
      "epoch:5 batch:41 loss:0.22911426424980164 acc:1.0\n",
      "epoch:5 batch:42 loss:0.23195631802082062 acc:1.0\n",
      "epoch:5 batch:43 loss:0.2209479808807373 acc:1.0\n",
      "epoch:5 batch:44 loss:0.23313653469085693 acc:1.0\n",
      "epoch:5 batch:45 loss:0.2357875406742096 acc:1.0\n",
      "epoch:5 batch:46 loss:0.22651569545269012 acc:1.0\n",
      "epoch:5 batch:47 loss:0.23106519877910614 acc:1.0\n",
      "epoch:5 batch:48 loss:0.20931705832481384 acc:1.0\n",
      "epoch:5 batch:49 loss:0.21558445692062378 acc:1.0\n",
      "epoch:5 batch:50 loss:0.2235480099916458 acc:1.0\n",
      "epoch:5 batch:51 loss:0.2195585072040558 acc:1.0\n",
      "epoch:5 batch:52 loss:0.2211025208234787 acc:1.0\n",
      "epoch:5 batch:53 loss:0.20649923384189606 acc:1.0\n",
      "epoch:5 batch:54 loss:0.2163684368133545 acc:1.0\n",
      "epoch:5 batch:55 loss:0.20798359811306 acc:1.0\n",
      "epoch:5 batch:56 loss:0.19919216632843018 acc:1.0\n",
      "epoch:5 batch:57 loss:0.19983753561973572 acc:1.0\n",
      "epoch:5 batch:58 loss:0.19504167139530182 acc:1.0\n",
      "epoch:5 batch:59 loss:0.1974191814661026 acc:1.0\n",
      "epoch:5 batch:60 loss:0.19823646545410156 acc:1.0\n",
      "epoch:5 batch:61 loss:0.18966668844223022 acc:1.0\n",
      "epoch:5 batch:62 loss:0.18807698786258698 acc:1.0\n",
      "epoch:5 batch:63 loss:0.18013574182987213 acc:1.0\n",
      "epoch:5 batch:64 loss:0.18027551472187042 acc:1.0\n",
      "epoch:5 batch:65 loss:0.18552593886852264 acc:1.0\n",
      "epoch:5 batch:66 loss:0.1705249696969986 acc:1.0\n",
      "epoch:5 batch:67 loss:0.1744324415922165 acc:1.0\n",
      "epoch:5 batch:68 loss:0.17818357050418854 acc:1.0\n",
      "epoch:5 batch:69 loss:0.17312295734882355 acc:1.0\n",
      "epoch:5 batch:70 loss:0.16840127110481262 acc:1.0\n",
      "epoch:5 batch:71 loss:0.17585884034633636 acc:1.0\n",
      "epoch:5 batch:72 loss:0.16820506751537323 acc:1.0\n",
      "epoch:5 batch:73 loss:0.1735779047012329 acc:1.0\n",
      "epoch:5 batch:74 loss:0.16715671122074127 acc:1.0\n",
      "epoch:5 batch:75 loss:0.17677253484725952 acc:1.0\n",
      "epoch:5 batch:76 loss:0.16120584309101105 acc:1.0\n",
      "epoch:5 batch:77 loss:0.16520047187805176 acc:1.0\n",
      "epoch:5 batch:78 loss:0.15343628823757172 acc:1.0\n",
      "epoch:5 batch:79 loss:0.1531989872455597 acc:1.0\n",
      "epoch:5 batch:80 loss:0.1601611226797104 acc:1.0\n",
      "epoch:5 batch:81 loss:0.1610986441373825 acc:1.0\n",
      "epoch:5 batch:82 loss:0.15770575404167175 acc:1.0\n",
      "epoch:5 batch:83 loss:0.14941351115703583 acc:1.0\n",
      "epoch:5 batch:84 loss:0.15284396708011627 acc:1.0\n",
      "epoch:5 batch:85 loss:0.13853329420089722 acc:1.0\n",
      "epoch:5 batch:86 loss:0.1531299650669098 acc:1.0\n",
      "epoch:5 batch:87 loss:0.14194485545158386 acc:1.0\n",
      "epoch:5 batch:88 loss:0.15153759717941284 acc:1.0\n",
      "epoch:5 batch:89 loss:0.12517939507961273 acc:1.0\n",
      "epoch:5 batch:90 loss:0.15624037384986877 acc:1.0\n",
      "epoch:5 batch:91 loss:0.1287132352590561 acc:1.0\n",
      "epoch:5 batch:92 loss:0.12467606365680695 acc:1.0\n",
      "epoch:5 batch:93 loss:0.14508230984210968 acc:1.0\n",
      "epoch:5 batch:94 loss:0.12245050072669983 acc:1.0\n",
      "epoch:5 batch:95 loss:0.1361437737941742 acc:1.0\n",
      "epoch:5 batch:96 loss:0.12993131577968597 acc:1.0\n",
      "epoch:5 batch:97 loss:0.12875328958034515 acc:1.0\n",
      "epoch:5 batch:98 loss:0.12232790887355804 acc:1.0\n",
      "epoch:5 batch:99 loss:0.13000383973121643 acc:1.0\n",
      "epoch:5 batch:100 loss:0.1296098530292511 acc:1.0\n",
      "epoch:5 batch:101 loss:0.12254670262336731 acc:1.0\n",
      "epoch:5 batch:102 loss:0.11357831954956055 acc:1.0\n",
      "epoch:5 batch:103 loss:0.11921737343072891 acc:1.0\n",
      "epoch:5 batch:104 loss:0.1191343367099762 acc:1.0\n",
      "epoch:5 batch:105 loss:0.12892787158489227 acc:1.0\n",
      "epoch:5 batch:106 loss:0.11866270005702972 acc:1.0\n",
      "epoch:5 batch:107 loss:0.1215696856379509 acc:1.0\n",
      "epoch:5 batch:108 loss:0.12042359262704849 acc:1.0\n",
      "epoch:5 batch:109 loss:0.11685594171285629 acc:1.0\n",
      "epoch:5 batch:110 loss:0.10521868616342545 acc:1.0\n",
      "epoch:5 batch:111 loss:0.12128593772649765 acc:1.0\n",
      "epoch:5 batch:112 loss:0.11045487970113754 acc:1.0\n",
      "epoch:5 batch:113 loss:0.1118004247546196 acc:1.0\n",
      "epoch:5 batch:114 loss:0.11019711941480637 acc:1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 batch:115 loss:0.10218095034360886 acc:1.0\n",
      "epoch:5 batch:116 loss:0.10080751776695251 acc:1.0\n",
      "epoch:5 batch:117 loss:0.11387903243303299 acc:1.0\n",
      "epoch:5 batch:118 loss:0.10301053524017334 acc:1.0\n",
      "epoch:5 batch:119 loss:0.0975288450717926 acc:1.0\n",
      "epoch:5 batch:120 loss:0.10469646751880646 acc:1.0\n",
      "epoch:5 batch:121 loss:0.10468757152557373 acc:1.0\n",
      "epoch:5 batch:122 loss:0.10795804113149643 acc:1.0\n",
      "epoch:5 batch:123 loss:0.10126635432243347 acc:1.0\n",
      "epoch:5 batch:124 loss:0.09755521267652512 acc:1.0\n",
      "epoch:5 batch:125 loss:0.10160060226917267 acc:1.0\n",
      "epoch:5 batch:126 loss:0.09682182222604752 acc:1.0\n",
      "epoch:5 batch:127 loss:0.09428635239601135 acc:1.0\n",
      "epoch:5 batch:128 loss:0.10974998772144318 acc:1.0\n",
      "epoch:5 batch:129 loss:0.0941123366355896 acc:1.0\n",
      "epoch:5 batch:130 loss:0.09276574105024338 acc:1.0\n",
      "epoch:5 batch:131 loss:0.0891217589378357 acc:1.0\n",
      "epoch:5 batch:132 loss:0.10002700984477997 acc:1.0\n",
      "epoch:5 batch:133 loss:0.09383179992437363 acc:1.0\n",
      "epoch:5 batch:134 loss:0.08350956439971924 acc:1.0\n",
      "epoch:5 batch:135 loss:0.09432318806648254 acc:1.0\n",
      "epoch:5 batch:136 loss:0.09124113619327545 acc:1.0\n",
      "epoch:5 batch:137 loss:0.09471338987350464 acc:1.0\n",
      "epoch:5 batch:138 loss:0.0903218686580658 acc:1.0\n",
      "epoch:5 batch:139 loss:0.09119918197393417 acc:1.0\n",
      "epoch:5 batch:140 loss:0.08362431824207306 acc:1.0\n",
      "epoch:5 batch:141 loss:0.08673868328332901 acc:1.0\n",
      "epoch:5 batch:142 loss:0.08904959261417389 acc:1.0\n",
      "epoch:5 batch:143 loss:0.0829366073012352 acc:1.0\n",
      "epoch:5 batch:144 loss:0.09027016907930374 acc:1.0\n",
      "epoch:5 batch:145 loss:0.08827882260084152 acc:1.0\n",
      "epoch:5 batch:146 loss:0.08642026782035828 acc:1.0\n",
      "epoch:5 batch:147 loss:0.08990388363599777 acc:1.0\n",
      "epoch:5 batch:148 loss:0.08328044414520264 acc:1.0\n",
      "epoch:5 batch:149 loss:0.07768207043409348 acc:1.0\n",
      "epoch:5 batch:150 loss:0.08500377833843231 acc:1.0\n",
      "epoch:5 batch:151 loss:0.0812365859746933 acc:1.0\n",
      "epoch:5 batch:152 loss:0.07684646546840668 acc:1.0\n",
      "epoch:5 batch:153 loss:0.08630749583244324 acc:1.0\n",
      "epoch:5 batch:154 loss:0.08144812285900116 acc:1.0\n",
      "epoch:5 batch:155 loss:0.0719570517539978 acc:1.0\n",
      "epoch:5 batch:156 loss:0.0860978364944458 acc:1.0\n",
      "epoch:5 batch:157 loss:0.08322174847126007 acc:1.0\n",
      "epoch:5 batch:158 loss:0.8583835959434509 acc:0.7166666666666667\n",
      "epoch:5 batch:159 loss:2.7027437686920166 acc:0.0\n",
      "epoch:5 batch:160 loss:1.2589763402938843 acc:0.5333333333333333\n",
      "epoch:5 batch:161 loss:0.07128489017486572 acc:1.0\n",
      "epoch:5 batch:162 loss:0.07317272573709488 acc:1.0\n",
      "epoch:5 batch:163 loss:0.07667941600084305 acc:1.0\n",
      "epoch:5 batch:164 loss:0.20523720979690552 acc:0.95\n",
      "epoch:5 batch:165 loss:2.6632895469665527 acc:0.0\n",
      "epoch:5 batch:166 loss:2.614365816116333 acc:0.0\n",
      "epoch:5 batch:167 loss:2.659177303314209 acc:0.0\n",
      "epoch:5 batch:168 loss:2.544349193572998 acc:0.0\n",
      "epoch:5 batch:169 loss:1.225886344909668 acc:0.5166666666666667\n",
      "epoch:5 batch:170 loss:0.0871807336807251 acc:1.0\n",
      "epoch:5 batch:171 loss:0.0936729833483696 acc:1.0\n",
      "epoch:5 batch:172 loss:0.09752323478460312 acc:1.0\n",
      "epoch:5 batch:173 loss:0.09458518773317337 acc:1.0\n",
      "epoch:5 batch:174 loss:0.09692954272031784 acc:1.0\n",
      "epoch:5 batch:175 loss:0.10684341192245483 acc:1.0\n",
      "epoch:5 batch:176 loss:0.346181184053421 acc:0.9\n",
      "epoch:5 batch:177 loss:2.340365409851074 acc:0.0\n",
      "epoch:5 batch:178 loss:2.2919440269470215 acc:0.0\n",
      "epoch:5 batch:179 loss:2.308793783187866 acc:0.0\n",
      "epoch:5 batch:180 loss:1.6649913787841797 acc:0.26666666666666666\n",
      "epoch:5 batch:181 loss:0.11520861089229584 acc:1.0\n",
      "epoch:5 batch:182 loss:0.12511472404003143 acc:1.0\n",
      "epoch:5 batch:183 loss:0.13414372503757477 acc:1.0\n",
      "epoch:5 batch:184 loss:0.14394299685955048 acc:1.0\n",
      "epoch:5 batch:185 loss:0.1287824660539627 acc:1.0\n",
      "epoch:5 batch:186 loss:0.13434019684791565 acc:1.0\n",
      "epoch:5 batch:187 loss:0.1391800343990326 acc:1.0\n",
      "epoch:5 batch:188 loss:0.5675269961357117 acc:0.7833333333333333\n",
      "epoch:5 batch:189 loss:2.041421413421631 acc:0.0\n",
      "epoch:5 batch:190 loss:2.0676450729370117 acc:0.0\n",
      "epoch:5 batch:191 loss:2.0964291095733643 acc:0.0\n",
      "epoch:5 batch:192 loss:2.0583715438842773 acc:0.0\n",
      "epoch:5 batch:193 loss:1.9968199729919434 acc:0.0\n",
      "epoch:5 batch:194 loss:2.008300304412842 acc:0.0\n",
      "epoch:5 batch:195 loss:1.9629703760147095 acc:0.0\n",
      "epoch:5 batch:196 loss:0.6280474066734314 acc:0.7166666666666667\n",
      "epoch:5 batch:197 loss:0.17816784977912903 acc:1.0\n",
      "epoch:5 batch:198 loss:0.18996325135231018 acc:1.0\n",
      "epoch:5 batch:199 loss:0.18598760664463043 acc:1.0\n",
      "epoch:5 batch:200 loss:0.20183461904525757 acc:1.0\n",
      "epoch:5 batch:201 loss:0.2097775638103485 acc:1.0\n",
      "epoch:5 batch:202 loss:0.20250815153121948 acc:1.0\n",
      "epoch:5 batch:203 loss:0.2894952893257141 acc:0.95\n",
      "epoch:5 batch:204 loss:1.6477723121643066 acc:0.0\n",
      "epoch:5 batch:205 loss:1.63429856300354 acc:0.03333333333333333\n",
      "epoch:5 batch:206 loss:0.20386771857738495 acc:1.0\n",
      "epoch:5 batch:207 loss:0.20977452397346497 acc:1.0\n",
      "epoch:5 batch:208 loss:0.21680928766727448 acc:1.0\n",
      "epoch:5 batch:209 loss:0.22481928765773773 acc:1.0\n",
      "epoch:5 batch:210 loss:0.22037725150585175 acc:1.0\n",
      "epoch:5 batch:211 loss:0.2258232831954956 acc:1.0\n",
      "epoch:5 batch:212 loss:0.4048255383968353 acc:0.8833333333333333\n",
      "epoch:5 batch:213 loss:1.583298683166504 acc:0.0\n",
      "epoch:5 batch:214 loss:1.6266273260116577 acc:0.0\n",
      "epoch:5 batch:215 loss:1.5509605407714844 acc:0.0\n",
      "epoch:5 batch:216 loss:1.5686030387878418 acc:0.0\n",
      "epoch:5 batch:217 loss:1.5255718231201172 acc:0.0\n",
      "epoch:5 batch:218 loss:1.5182368755340576 acc:0.0\n",
      "epoch:5 batch:219 loss:1.5071109533309937 acc:0.0\n",
      "epoch:5 batch:220 loss:1.4679542779922485 acc:0.0\n",
      "epoch:5 batch:221 loss:1.3623268604278564 acc:0.0\n",
      "epoch:5 batch:222 loss:1.3665584325790405 acc:0.0\n",
      "epoch:5 batch:223 loss:1.259914755821228 acc:0.0\n",
      "epoch:5 batch:224 loss:1.2057448625564575 acc:0.0\n",
      "epoch:10 batch:0 loss:0.06286239624023438 acc:1.0\n",
      "epoch:10 batch:1 loss:0.06192566826939583 acc:1.0\n",
      "epoch:10 batch:2 loss:0.06196832284331322 acc:1.0\n",
      "epoch:10 batch:3 loss:0.06358794122934341 acc:1.0\n",
      "epoch:10 batch:4 loss:0.06227977201342583 acc:1.0\n",
      "epoch:10 batch:5 loss:0.07190892100334167 acc:1.0\n",
      "epoch:10 batch:6 loss:0.05908552557229996 acc:1.0\n",
      "epoch:10 batch:7 loss:0.06683649122714996 acc:1.0\n",
      "epoch:10 batch:8 loss:0.06408759951591492 acc:1.0\n",
      "epoch:10 batch:9 loss:0.06012021005153656 acc:1.0\n",
      "epoch:10 batch:10 loss:0.059015389531850815 acc:1.0\n",
      "epoch:10 batch:11 loss:0.06306009739637375 acc:1.0\n",
      "epoch:10 batch:12 loss:0.06600002944469452 acc:1.0\n",
      "epoch:10 batch:13 loss:0.06655571609735489 acc:1.0\n",
      "epoch:10 batch:14 loss:0.06319291144609451 acc:1.0\n",
      "epoch:10 batch:15 loss:0.07042422890663147 acc:1.0\n",
      "epoch:10 batch:16 loss:0.06451668590307236 acc:1.0\n",
      "epoch:10 batch:17 loss:0.07047953456640244 acc:1.0\n",
      "epoch:10 batch:18 loss:0.07033529877662659 acc:1.0\n",
      "epoch:10 batch:19 loss:0.057033929973840714 acc:1.0\n",
      "epoch:10 batch:20 loss:0.06441448628902435 acc:1.0\n",
      "epoch:10 batch:21 loss:0.05948401242494583 acc:1.0\n",
      "epoch:10 batch:22 loss:0.07847494632005692 acc:1.0\n",
      "epoch:10 batch:23 loss:0.07110285013914108 acc:1.0\n",
      "epoch:10 batch:24 loss:0.07545606791973114 acc:1.0\n",
      "epoch:10 batch:25 loss:0.07782141119241714 acc:1.0\n",
      "epoch:10 batch:26 loss:0.07817903161048889 acc:1.0\n",
      "epoch:10 batch:27 loss:0.07746229320764542 acc:1.0\n",
      "epoch:10 batch:28 loss:0.06574560701847076 acc:1.0\n",
      "epoch:10 batch:29 loss:0.07762043178081512 acc:1.0\n",
      "epoch:10 batch:30 loss:0.06915691494941711 acc:1.0\n",
      "epoch:10 batch:31 loss:0.06810984015464783 acc:1.0\n",
      "epoch:10 batch:32 loss:0.06579388678073883 acc:1.0\n",
      "epoch:10 batch:33 loss:0.06769806891679764 acc:1.0\n",
      "epoch:10 batch:34 loss:0.06408245116472244 acc:1.0\n",
      "epoch:10 batch:35 loss:0.05841059237718582 acc:1.0\n",
      "epoch:10 batch:36 loss:0.05927887186408043 acc:1.0\n",
      "epoch:10 batch:37 loss:0.06472140550613403 acc:1.0\n",
      "epoch:10 batch:38 loss:0.054804518818855286 acc:1.0\n",
      "epoch:10 batch:39 loss:0.05583569407463074 acc:1.0\n",
      "epoch:10 batch:40 loss:0.05754736438393593 acc:1.0\n",
      "epoch:10 batch:41 loss:0.05741181969642639 acc:1.0\n",
      "epoch:10 batch:42 loss:0.05314259976148605 acc:1.0\n",
      "epoch:10 batch:43 loss:0.059314876794815063 acc:1.0\n",
      "epoch:10 batch:44 loss:0.05757640674710274 acc:1.0\n",
      "epoch:10 batch:45 loss:0.055819857865571976 acc:1.0\n",
      "epoch:10 batch:46 loss:0.051632851362228394 acc:1.0\n",
      "epoch:10 batch:47 loss:0.05111253634095192 acc:1.0\n",
      "epoch:10 batch:48 loss:0.058629076927900314 acc:1.0\n",
      "epoch:10 batch:49 loss:0.05344056710600853 acc:1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 batch:50 loss:0.051596060395240784 acc:1.0\n",
      "epoch:10 batch:51 loss:0.059436358511447906 acc:1.0\n",
      "epoch:10 batch:52 loss:0.05812562629580498 acc:1.0\n",
      "epoch:10 batch:53 loss:0.0547831654548645 acc:1.0\n",
      "epoch:10 batch:54 loss:0.060103341937065125 acc:1.0\n",
      "epoch:10 batch:55 loss:0.05221159756183624 acc:1.0\n",
      "epoch:10 batch:56 loss:0.06009998545050621 acc:1.0\n",
      "epoch:10 batch:57 loss:0.061195507645606995 acc:1.0\n",
      "epoch:10 batch:58 loss:0.05315054580569267 acc:1.0\n",
      "epoch:10 batch:59 loss:0.059993013739585876 acc:1.0\n",
      "epoch:10 batch:60 loss:0.054048605263233185 acc:1.0\n",
      "epoch:10 batch:61 loss:0.05431812256574631 acc:1.0\n",
      "epoch:10 batch:62 loss:0.05402762070298195 acc:1.0\n",
      "epoch:10 batch:63 loss:0.05628502368927002 acc:1.0\n",
      "epoch:10 batch:64 loss:0.05472344160079956 acc:1.0\n",
      "epoch:10 batch:65 loss:0.054340288043022156 acc:1.0\n",
      "epoch:10 batch:66 loss:0.05365881323814392 acc:1.0\n",
      "epoch:10 batch:67 loss:0.06349316984415054 acc:1.0\n",
      "epoch:10 batch:68 loss:0.0606285035610199 acc:1.0\n",
      "epoch:10 batch:69 loss:0.054134562611579895 acc:1.0\n",
      "epoch:10 batch:70 loss:0.0602760910987854 acc:1.0\n",
      "epoch:10 batch:71 loss:0.05457238107919693 acc:1.0\n",
      "epoch:10 batch:72 loss:0.05406706780195236 acc:1.0\n",
      "epoch:10 batch:73 loss:0.05123358964920044 acc:1.0\n",
      "epoch:10 batch:74 loss:0.05309887230396271 acc:1.0\n",
      "epoch:10 batch:75 loss:0.05468771606683731 acc:1.0\n",
      "epoch:10 batch:76 loss:0.04895536229014397 acc:1.0\n",
      "epoch:10 batch:77 loss:0.0539446696639061 acc:1.0\n",
      "epoch:10 batch:78 loss:0.04929913952946663 acc:1.0\n",
      "epoch:10 batch:79 loss:0.04538104310631752 acc:1.0\n",
      "epoch:10 batch:80 loss:0.04797475412487984 acc:1.0\n",
      "epoch:10 batch:81 loss:0.05289861187338829 acc:1.0\n",
      "epoch:10 batch:82 loss:0.047604288905858994 acc:1.0\n",
      "epoch:10 batch:83 loss:0.047575876116752625 acc:1.0\n",
      "epoch:10 batch:84 loss:0.04348825663328171 acc:1.0\n",
      "epoch:10 batch:85 loss:0.0460183247923851 acc:1.0\n",
      "epoch:10 batch:86 loss:0.04911796376109123 acc:1.0\n",
      "epoch:10 batch:87 loss:0.04287440702319145 acc:1.0\n",
      "epoch:10 batch:88 loss:0.0476461723446846 acc:1.0\n",
      "epoch:10 batch:89 loss:0.0472232922911644 acc:1.0\n",
      "epoch:10 batch:90 loss:0.04255274310708046 acc:1.0\n",
      "epoch:10 batch:91 loss:0.044818442314863205 acc:1.0\n",
      "epoch:10 batch:92 loss:0.05080889165401459 acc:1.0\n",
      "epoch:10 batch:93 loss:0.04049995541572571 acc:1.0\n",
      "epoch:10 batch:94 loss:0.045407190918922424 acc:1.0\n",
      "epoch:10 batch:95 loss:0.04635199159383774 acc:1.0\n",
      "epoch:10 batch:96 loss:0.05233011394739151 acc:1.0\n",
      "epoch:10 batch:97 loss:0.040981221944093704 acc:1.0\n",
      "epoch:10 batch:98 loss:0.049931127578020096 acc:1.0\n",
      "epoch:10 batch:99 loss:0.04481218755245209 acc:1.0\n",
      "epoch:10 batch:100 loss:0.046002794057130814 acc:1.0\n",
      "epoch:10 batch:101 loss:0.04727946221828461 acc:1.0\n",
      "epoch:10 batch:102 loss:0.04690796136856079 acc:1.0\n",
      "epoch:10 batch:103 loss:0.048945214599370956 acc:1.0\n",
      "epoch:10 batch:104 loss:0.04492563009262085 acc:1.0\n",
      "epoch:10 batch:105 loss:0.045542821288108826 acc:1.0\n",
      "epoch:10 batch:106 loss:0.050813183188438416 acc:1.0\n",
      "epoch:10 batch:107 loss:0.04644228145480156 acc:1.0\n",
      "epoch:10 batch:108 loss:0.0414578914642334 acc:1.0\n",
      "epoch:10 batch:109 loss:0.04546321555972099 acc:1.0\n",
      "epoch:10 batch:110 loss:0.043121106922626495 acc:1.0\n",
      "epoch:10 batch:111 loss:0.04567284137010574 acc:1.0\n",
      "epoch:10 batch:112 loss:0.04996069520711899 acc:1.0\n",
      "epoch:10 batch:113 loss:0.04677511006593704 acc:1.0\n",
      "epoch:10 batch:114 loss:0.045287515968084335 acc:1.0\n",
      "epoch:10 batch:115 loss:0.042437463998794556 acc:1.0\n",
      "epoch:10 batch:116 loss:0.045135632157325745 acc:1.0\n",
      "epoch:10 batch:117 loss:0.04404802620410919 acc:1.0\n",
      "epoch:10 batch:118 loss:0.04334987699985504 acc:1.0\n",
      "epoch:10 batch:119 loss:0.04360875114798546 acc:1.0\n",
      "epoch:10 batch:120 loss:0.0427115224301815 acc:1.0\n",
      "epoch:10 batch:121 loss:0.04030812159180641 acc:1.0\n",
      "epoch:10 batch:122 loss:0.04553477466106415 acc:1.0\n",
      "epoch:10 batch:123 loss:0.042911119759082794 acc:1.0\n",
      "epoch:10 batch:124 loss:0.046242523938417435 acc:1.0\n",
      "epoch:10 batch:125 loss:0.04491562396287918 acc:1.0\n",
      "epoch:10 batch:126 loss:0.03698349744081497 acc:1.0\n",
      "epoch:10 batch:127 loss:0.04399194195866585 acc:1.0\n",
      "epoch:10 batch:128 loss:0.04185722768306732 acc:1.0\n",
      "epoch:10 batch:129 loss:0.037300676107406616 acc:1.0\n",
      "epoch:10 batch:130 loss:0.03723499923944473 acc:1.0\n",
      "epoch:10 batch:131 loss:0.035128455609083176 acc:1.0\n",
      "epoch:10 batch:132 loss:0.03989255800843239 acc:1.0\n",
      "epoch:10 batch:133 loss:0.03732139617204666 acc:1.0\n",
      "epoch:10 batch:134 loss:0.044488660991191864 acc:1.0\n",
      "epoch:10 batch:135 loss:0.03401507809758186 acc:1.0\n",
      "epoch:10 batch:136 loss:0.04081839695572853 acc:1.0\n",
      "epoch:10 batch:137 loss:0.03451519459486008 acc:1.0\n",
      "epoch:10 batch:138 loss:0.040865857154130936 acc:1.0\n",
      "epoch:10 batch:139 loss:0.039174143224954605 acc:1.0\n",
      "epoch:10 batch:140 loss:0.03795172646641731 acc:1.0\n",
      "epoch:10 batch:141 loss:0.03961605578660965 acc:1.0\n",
      "epoch:10 batch:142 loss:0.03804243355989456 acc:1.0\n",
      "epoch:10 batch:143 loss:0.04234505817294121 acc:1.0\n",
      "epoch:10 batch:144 loss:0.03854084759950638 acc:1.0\n",
      "epoch:10 batch:145 loss:0.03700364753603935 acc:1.0\n",
      "epoch:10 batch:146 loss:0.03562173247337341 acc:1.0\n",
      "epoch:10 batch:147 loss:0.03563767671585083 acc:1.0\n",
      "epoch:10 batch:148 loss:0.03814831003546715 acc:1.0\n",
      "epoch:10 batch:149 loss:0.0350116603076458 acc:1.0\n",
      "epoch:10 batch:150 loss:0.038233526051044464 acc:1.0\n",
      "epoch:10 batch:151 loss:0.03712741285562515 acc:1.0\n",
      "epoch:10 batch:152 loss:0.03751715272665024 acc:1.0\n",
      "epoch:10 batch:153 loss:0.03733488917350769 acc:1.0\n",
      "epoch:10 batch:154 loss:0.0349377803504467 acc:1.0\n",
      "epoch:10 batch:155 loss:0.04005518928170204 acc:1.0\n",
      "epoch:10 batch:156 loss:0.04082754626870155 acc:1.0\n",
      "epoch:10 batch:157 loss:0.045653585344552994 acc:1.0\n",
      "epoch:10 batch:158 loss:1.0394890308380127 acc:0.7166666666666667\n",
      "epoch:10 batch:159 loss:3.516127109527588 acc:0.0\n",
      "epoch:10 batch:160 loss:1.6357614994049072 acc:0.5333333333333333\n",
      "epoch:10 batch:161 loss:0.03913799300789833 acc:1.0\n",
      "epoch:10 batch:162 loss:0.03903999924659729 acc:1.0\n",
      "epoch:10 batch:163 loss:0.03836461901664734 acc:1.0\n",
      "epoch:10 batch:164 loss:0.23277390003204346 acc:0.95\n",
      "epoch:10 batch:165 loss:3.3326961994171143 acc:0.0\n",
      "epoch:10 batch:166 loss:3.3930156230926514 acc:0.0\n",
      "epoch:10 batch:167 loss:3.2517971992492676 acc:0.0\n",
      "epoch:10 batch:168 loss:3.3968451023101807 acc:0.0\n",
      "epoch:10 batch:169 loss:1.5900169610977173 acc:0.5166666666666667\n",
      "epoch:10 batch:170 loss:0.04820218309760094 acc:1.0\n",
      "epoch:10 batch:171 loss:0.0453081950545311 acc:1.0\n",
      "epoch:10 batch:172 loss:0.04206761717796326 acc:1.0\n",
      "epoch:10 batch:173 loss:0.04875344783067703 acc:1.0\n",
      "epoch:10 batch:174 loss:0.05167277157306671 acc:1.0\n",
      "epoch:10 batch:175 loss:0.06490647792816162 acc:1.0\n",
      "epoch:10 batch:176 loss:0.32747533917427063 acc:0.9\n",
      "epoch:10 batch:177 loss:1.7607173919677734 acc:0.0\n",
      "epoch:10 batch:178 loss:0.6456716060638428 acc:0.7833333333333333\n",
      "epoch:10 batch:179 loss:1.7668150663375854 acc:0.0\n",
      "epoch:10 batch:180 loss:0.9775050282478333 acc:0.26666666666666666\n",
      "epoch:10 batch:181 loss:0.7809178829193115 acc:0.08333333333333333\n",
      "epoch:10 batch:182 loss:0.7866380214691162 acc:0.05\n",
      "epoch:10 batch:183 loss:0.8006653189659119 acc:0.03333333333333333\n",
      "epoch:10 batch:184 loss:0.7840177416801453 acc:0.03333333333333333\n",
      "epoch:10 batch:185 loss:0.5166618824005127 acc:1.0\n",
      "epoch:10 batch:186 loss:0.48243197798728943 acc:1.0\n",
      "epoch:10 batch:187 loss:0.6557649374008179 acc:0.5333333333333333\n",
      "epoch:10 batch:188 loss:0.7488187551498413 acc:0.26666666666666666\n",
      "epoch:10 batch:189 loss:0.6123597621917725 acc:0.9833333333333333\n",
      "epoch:10 batch:190 loss:0.8925888538360596 acc:0.0\n",
      "epoch:10 batch:191 loss:0.9050794243812561 acc:0.0\n",
      "epoch:10 batch:192 loss:0.8918102979660034 acc:0.0\n",
      "epoch:10 batch:193 loss:0.8978201150894165 acc:0.0\n",
      "epoch:10 batch:194 loss:0.8806967735290527 acc:0.0\n",
      "epoch:10 batch:195 loss:0.9100385308265686 acc:0.0\n",
      "epoch:10 batch:196 loss:0.6370717287063599 acc:0.7166666666666667\n",
      "epoch:10 batch:197 loss:0.5487578511238098 acc:1.0\n",
      "epoch:10 batch:198 loss:0.5534567832946777 acc:1.0\n",
      "epoch:10 batch:199 loss:0.5480789542198181 acc:1.0\n",
      "epoch:10 batch:200 loss:0.5510443449020386 acc:1.0\n",
      "epoch:10 batch:201 loss:0.5611737370491028 acc:1.0\n",
      "epoch:10 batch:202 loss:0.5278708338737488 acc:0.9833333333333333\n",
      "epoch:10 batch:203 loss:0.4988885223865509 acc:0.95\n",
      "epoch:10 batch:204 loss:0.9394665360450745 acc:0.0\n",
      "epoch:10 batch:205 loss:0.9306607842445374 acc:0.03333333333333333\n",
      "epoch:10 batch:206 loss:0.47586730122566223 acc:1.0\n",
      "epoch:10 batch:207 loss:0.461408406496048 acc:1.0\n",
      "epoch:10 batch:208 loss:0.4576176106929779 acc:1.0\n",
      "epoch:10 batch:209 loss:0.46015530824661255 acc:1.0\n",
      "epoch:10 batch:210 loss:0.4543687403202057 acc:1.0\n",
      "epoch:10 batch:211 loss:0.5276960134506226 acc:1.0\n",
      "epoch:10 batch:212 loss:0.5834214687347412 acc:0.8833333333333333\n",
      "epoch:10 batch:213 loss:0.8933957815170288 acc:0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 batch:214 loss:0.907828688621521 acc:0.0\n",
      "epoch:10 batch:215 loss:0.926511824131012 acc:0.0\n",
      "epoch:10 batch:216 loss:0.9015428423881531 acc:0.0\n",
      "epoch:10 batch:217 loss:0.9005843997001648 acc:0.0\n",
      "epoch:10 batch:218 loss:0.8878388404846191 acc:0.0\n",
      "epoch:10 batch:219 loss:0.8716530799865723 acc:0.0\n",
      "epoch:10 batch:220 loss:0.8439070582389832 acc:0.016666666666666666\n",
      "epoch:10 batch:221 loss:0.8259273171424866 acc:0.0\n",
      "epoch:10 batch:222 loss:0.8049310445785522 acc:0.0\n",
      "epoch:10 batch:223 loss:0.7855069637298584 acc:0.03333333333333333\n",
      "epoch:10 batch:224 loss:0.7651892900466919 acc:0.07272727272727272\n",
      "epoch:15 batch:0 loss:0.044322334229946136 acc:1.0\n",
      "epoch:15 batch:1 loss:0.04863972216844559 acc:1.0\n",
      "epoch:15 batch:2 loss:0.04495922103524208 acc:1.0\n",
      "epoch:15 batch:3 loss:0.04385264217853546 acc:1.0\n",
      "epoch:15 batch:4 loss:0.04506655037403107 acc:1.0\n",
      "epoch:15 batch:5 loss:0.048041291534900665 acc:1.0\n",
      "epoch:15 batch:6 loss:0.048486676067113876 acc:1.0\n",
      "epoch:15 batch:7 loss:0.049229782074689865 acc:1.0\n",
      "epoch:15 batch:8 loss:0.04948097839951515 acc:1.0\n",
      "epoch:15 batch:9 loss:0.04707999527454376 acc:1.0\n",
      "epoch:15 batch:10 loss:0.05227978900074959 acc:1.0\n",
      "epoch:15 batch:11 loss:0.04260440915822983 acc:1.0\n",
      "epoch:15 batch:12 loss:0.049927156418561935 acc:1.0\n",
      "epoch:15 batch:13 loss:0.04309255629777908 acc:1.0\n",
      "epoch:15 batch:14 loss:0.05209330469369888 acc:1.0\n",
      "epoch:15 batch:15 loss:0.043852537870407104 acc:1.0\n",
      "epoch:15 batch:16 loss:0.05124850198626518 acc:1.0\n",
      "epoch:15 batch:17 loss:0.051862914115190506 acc:1.0\n",
      "epoch:15 batch:18 loss:0.04869784042239189 acc:1.0\n",
      "epoch:15 batch:19 loss:0.05349617823958397 acc:1.0\n",
      "epoch:15 batch:20 loss:0.05361662432551384 acc:1.0\n",
      "epoch:15 batch:21 loss:0.04882635548710823 acc:1.0\n",
      "epoch:15 batch:22 loss:0.04762798547744751 acc:1.0\n",
      "epoch:15 batch:23 loss:0.055672839283943176 acc:1.0\n",
      "epoch:15 batch:24 loss:0.056474555283784866 acc:1.0\n",
      "epoch:15 batch:25 loss:0.04929693788290024 acc:1.0\n",
      "epoch:15 batch:26 loss:0.053779732435941696 acc:1.0\n",
      "epoch:15 batch:27 loss:0.05201281979680061 acc:1.0\n",
      "epoch:15 batch:28 loss:0.05022827535867691 acc:1.0\n",
      "epoch:15 batch:29 loss:0.04941273853182793 acc:1.0\n",
      "epoch:15 batch:30 loss:0.05887216329574585 acc:1.0\n",
      "epoch:15 batch:31 loss:0.05313745513558388 acc:1.0\n",
      "epoch:15 batch:32 loss:0.05399444326758385 acc:1.0\n",
      "epoch:15 batch:33 loss:0.054839812219142914 acc:1.0\n",
      "epoch:15 batch:34 loss:0.055551882833242416 acc:1.0\n",
      "epoch:15 batch:35 loss:0.05059272423386574 acc:1.0\n",
      "epoch:15 batch:36 loss:0.0566299632191658 acc:1.0\n",
      "epoch:15 batch:37 loss:0.05161180719733238 acc:1.0\n",
      "epoch:15 batch:38 loss:0.05253559350967407 acc:1.0\n",
      "epoch:15 batch:39 loss:0.04906044900417328 acc:1.0\n",
      "epoch:15 batch:40 loss:0.05016368627548218 acc:1.0\n",
      "epoch:15 batch:41 loss:0.05489945784211159 acc:1.0\n",
      "epoch:15 batch:42 loss:0.05310329422354698 acc:1.0\n",
      "epoch:15 batch:43 loss:0.056477271020412445 acc:1.0\n",
      "epoch:15 batch:44 loss:0.05579962953925133 acc:1.0\n",
      "epoch:15 batch:45 loss:0.04590333625674248 acc:1.0\n",
      "epoch:15 batch:46 loss:0.04771123453974724 acc:1.0\n",
      "epoch:15 batch:47 loss:0.059450987726449966 acc:1.0\n",
      "epoch:15 batch:48 loss:0.0458122156560421 acc:1.0\n",
      "epoch:15 batch:49 loss:0.049049559980630875 acc:1.0\n",
      "epoch:15 batch:50 loss:0.050476256757974625 acc:1.0\n",
      "epoch:15 batch:51 loss:0.04802887514233589 acc:1.0\n",
      "epoch:15 batch:52 loss:0.04819021373987198 acc:1.0\n",
      "epoch:15 batch:53 loss:0.04314329847693443 acc:1.0\n",
      "epoch:15 batch:54 loss:0.04859936982393265 acc:1.0\n",
      "epoch:15 batch:55 loss:0.04721425473690033 acc:1.0\n",
      "epoch:15 batch:56 loss:0.041830528527498245 acc:1.0\n",
      "epoch:15 batch:57 loss:0.04589466378092766 acc:1.0\n",
      "epoch:15 batch:58 loss:0.04567090794444084 acc:1.0\n",
      "epoch:15 batch:59 loss:0.04857931286096573 acc:1.0\n",
      "epoch:15 batch:60 loss:0.0481538288295269 acc:1.0\n",
      "epoch:15 batch:61 loss:0.04397161304950714 acc:1.0\n",
      "epoch:15 batch:62 loss:0.04643222689628601 acc:1.0\n",
      "epoch:15 batch:63 loss:0.041895732283592224 acc:1.0\n",
      "epoch:15 batch:64 loss:0.04169187694787979 acc:1.0\n",
      "epoch:15 batch:65 loss:0.043386317789554596 acc:1.0\n",
      "epoch:15 batch:66 loss:0.0408603772521019 acc:1.0\n",
      "epoch:15 batch:67 loss:0.03934958949685097 acc:1.0\n",
      "epoch:15 batch:68 loss:0.046122051775455475 acc:1.0\n",
      "epoch:15 batch:69 loss:0.03809470683336258 acc:1.0\n",
      "epoch:15 batch:70 loss:0.04373502731323242 acc:1.0\n",
      "epoch:15 batch:71 loss:0.04643039032816887 acc:1.0\n",
      "epoch:15 batch:72 loss:0.04072648286819458 acc:1.0\n",
      "epoch:15 batch:73 loss:0.042561888694763184 acc:1.0\n",
      "epoch:15 batch:74 loss:0.04215215891599655 acc:1.0\n",
      "epoch:15 batch:75 loss:0.03810665383934975 acc:1.0\n",
      "epoch:15 batch:76 loss:0.04124162346124649 acc:1.0\n",
      "epoch:15 batch:77 loss:0.034758321940898895 acc:1.0\n",
      "epoch:15 batch:78 loss:0.04365824535489082 acc:1.0\n",
      "epoch:15 batch:79 loss:0.04265228286385536 acc:1.0\n",
      "epoch:15 batch:80 loss:0.04027828946709633 acc:1.0\n",
      "epoch:15 batch:81 loss:0.04261873662471771 acc:1.0\n",
      "epoch:15 batch:82 loss:0.04192067310214043 acc:1.0\n",
      "epoch:15 batch:83 loss:0.04408157244324684 acc:1.0\n",
      "epoch:15 batch:84 loss:0.05041768401861191 acc:1.0\n",
      "epoch:15 batch:85 loss:0.039461277425289154 acc:1.0\n",
      "epoch:15 batch:86 loss:0.0436418391764164 acc:1.0\n",
      "epoch:15 batch:87 loss:0.04281853139400482 acc:1.0\n",
      "epoch:15 batch:88 loss:0.04259071871638298 acc:1.0\n",
      "epoch:15 batch:89 loss:0.03494846820831299 acc:1.0\n",
      "epoch:15 batch:90 loss:0.040571022778749466 acc:1.0\n",
      "epoch:15 batch:91 loss:0.04129134863615036 acc:1.0\n",
      "epoch:15 batch:92 loss:0.03571486845612526 acc:1.0\n",
      "epoch:15 batch:93 loss:0.038890451192855835 acc:1.0\n",
      "epoch:15 batch:94 loss:0.03423813357949257 acc:1.0\n",
      "epoch:15 batch:95 loss:0.03589922562241554 acc:1.0\n",
      "epoch:15 batch:96 loss:0.03620254620909691 acc:1.0\n",
      "epoch:15 batch:97 loss:0.036183860152959824 acc:1.0\n",
      "epoch:15 batch:98 loss:0.04053536802530289 acc:1.0\n",
      "epoch:15 batch:99 loss:0.0433996208012104 acc:1.0\n",
      "epoch:15 batch:100 loss:0.0312809944152832 acc:1.0\n",
      "epoch:15 batch:101 loss:0.03551988676190376 acc:1.0\n",
      "epoch:15 batch:102 loss:0.03503316268324852 acc:1.0\n",
      "epoch:15 batch:103 loss:0.040774449706077576 acc:1.0\n",
      "epoch:15 batch:104 loss:0.03643283247947693 acc:1.0\n",
      "epoch:15 batch:105 loss:0.03380874916911125 acc:1.0\n",
      "epoch:15 batch:106 loss:0.03954490274190903 acc:1.0\n",
      "epoch:15 batch:107 loss:0.037000689655542374 acc:1.0\n",
      "epoch:15 batch:108 loss:0.035668279975652695 acc:1.0\n",
      "epoch:15 batch:109 loss:0.03776836767792702 acc:1.0\n",
      "epoch:15 batch:110 loss:0.036035869270563126 acc:1.0\n",
      "epoch:15 batch:111 loss:0.033983148634433746 acc:1.0\n",
      "epoch:15 batch:112 loss:0.040507033467292786 acc:1.0\n",
      "epoch:15 batch:113 loss:0.03062872216105461 acc:1.0\n",
      "epoch:15 batch:114 loss:0.03990550339221954 acc:1.0\n",
      "epoch:15 batch:115 loss:0.03318437561392784 acc:1.0\n",
      "epoch:15 batch:116 loss:0.0319780632853508 acc:1.0\n",
      "epoch:15 batch:117 loss:0.03384386748075485 acc:1.0\n",
      "epoch:15 batch:118 loss:0.038510192185640335 acc:1.0\n",
      "epoch:15 batch:119 loss:0.03669792041182518 acc:1.0\n",
      "epoch:15 batch:120 loss:0.04265511408448219 acc:1.0\n",
      "epoch:15 batch:121 loss:0.03420601785182953 acc:1.0\n",
      "epoch:15 batch:122 loss:0.039821892976760864 acc:1.0\n",
      "epoch:15 batch:123 loss:0.04059767723083496 acc:1.0\n",
      "epoch:15 batch:124 loss:0.0399387963116169 acc:1.0\n",
      "epoch:15 batch:125 loss:0.035519883036613464 acc:1.0\n",
      "epoch:15 batch:126 loss:0.037997834384441376 acc:1.0\n",
      "epoch:15 batch:127 loss:0.03839070722460747 acc:1.0\n",
      "epoch:15 batch:128 loss:0.032117925584316254 acc:1.0\n",
      "epoch:15 batch:129 loss:0.030935121700167656 acc:1.0\n",
      "epoch:15 batch:130 loss:0.0373501293361187 acc:1.0\n",
      "epoch:15 batch:131 loss:0.04047822207212448 acc:1.0\n",
      "epoch:15 batch:132 loss:0.03968312218785286 acc:1.0\n",
      "epoch:15 batch:133 loss:0.040309835225343704 acc:1.0\n",
      "epoch:15 batch:134 loss:0.032777585089206696 acc:1.0\n",
      "epoch:15 batch:135 loss:0.03123188018798828 acc:1.0\n",
      "epoch:15 batch:136 loss:0.03545605391263962 acc:1.0\n",
      "epoch:15 batch:137 loss:0.033332064747810364 acc:1.0\n",
      "epoch:15 batch:138 loss:0.03256550431251526 acc:1.0\n",
      "epoch:15 batch:139 loss:0.031633079051971436 acc:1.0\n",
      "epoch:15 batch:140 loss:0.03452591225504875 acc:1.0\n",
      "epoch:15 batch:141 loss:0.03607521206140518 acc:1.0\n",
      "epoch:15 batch:142 loss:0.034645162522792816 acc:1.0\n",
      "epoch:15 batch:143 loss:0.03172074630856514 acc:1.0\n",
      "epoch:15 batch:144 loss:0.031984008848667145 acc:1.0\n",
      "epoch:15 batch:145 loss:0.0331881083548069 acc:1.0\n",
      "epoch:15 batch:146 loss:0.035964515060186386 acc:1.0\n",
      "epoch:15 batch:147 loss:0.03579980880022049 acc:1.0\n",
      "epoch:15 batch:148 loss:0.030010828748345375 acc:1.0\n",
      "epoch:15 batch:149 loss:0.031056473031640053 acc:1.0\n",
      "epoch:15 batch:150 loss:0.030269071459770203 acc:1.0\n",
      "epoch:15 batch:151 loss:0.0319664441049099 acc:1.0\n",
      "epoch:15 batch:152 loss:0.03190302103757858 acc:1.0\n",
      "epoch:15 batch:153 loss:0.03148030489683151 acc:1.0\n",
      "epoch:15 batch:154 loss:0.03043190762400627 acc:1.0\n",
      "epoch:15 batch:155 loss:0.030445890501141548 acc:1.0\n",
      "epoch:15 batch:156 loss:0.031126944348216057 acc:1.0\n",
      "epoch:15 batch:157 loss:0.034436870366334915 acc:1.0\n",
      "epoch:15 batch:158 loss:1.0316897630691528 acc:0.7166666666666667\n",
      "epoch:15 batch:159 loss:3.9199490547180176 acc:0.0\n",
      "epoch:15 batch:160 loss:1.717903971672058 acc:0.5333333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 batch:161 loss:0.033021457493305206 acc:1.0\n",
      "epoch:15 batch:162 loss:0.02971765771508217 acc:1.0\n",
      "epoch:15 batch:163 loss:0.031761884689331055 acc:1.0\n",
      "epoch:15 batch:164 loss:0.2735968232154846 acc:0.95\n",
      "epoch:15 batch:165 loss:3.616220712661743 acc:0.0\n",
      "epoch:15 batch:166 loss:3.715786933898926 acc:0.0\n",
      "epoch:15 batch:167 loss:3.527913808822632 acc:0.0\n",
      "epoch:15 batch:168 loss:3.586193084716797 acc:0.0\n",
      "epoch:15 batch:169 loss:1.6527693271636963 acc:0.5166666666666667\n",
      "epoch:15 batch:170 loss:0.045383162796497345 acc:1.0\n",
      "epoch:15 batch:171 loss:0.04848034307360649 acc:1.0\n",
      "epoch:15 batch:172 loss:0.04754514992237091 acc:1.0\n",
      "epoch:15 batch:173 loss:0.05756707116961479 acc:1.0\n",
      "epoch:15 batch:174 loss:0.052135806530714035 acc:1.0\n",
      "epoch:15 batch:175 loss:0.055414240807294846 acc:1.0\n",
      "epoch:15 batch:176 loss:1.0166239738464355 acc:0.1\n",
      "epoch:15 batch:177 loss:0.41874808073043823 acc:1.0\n",
      "epoch:15 batch:178 loss:0.41345566511154175 acc:1.0\n",
      "epoch:15 batch:179 loss:0.4452068507671356 acc:0.9666666666666667\n",
      "epoch:15 batch:180 loss:0.6099151372909546 acc:0.7333333333333333\n",
      "epoch:15 batch:181 loss:1.0588691234588623 acc:0.0\n",
      "epoch:15 batch:182 loss:1.0709667205810547 acc:0.0\n",
      "epoch:15 batch:183 loss:1.0543028116226196 acc:0.0\n",
      "epoch:15 batch:184 loss:0.5756285786628723 acc:0.45\n",
      "epoch:15 batch:185 loss:0.15171298384666443 acc:1.0\n",
      "epoch:15 batch:186 loss:0.14815224707126617 acc:1.0\n",
      "epoch:15 batch:187 loss:0.5658031702041626 acc:0.5666666666666667\n",
      "epoch:15 batch:188 loss:0.8794789910316467 acc:0.23333333333333334\n",
      "epoch:15 batch:189 loss:0.45977523922920227 acc:1.0\n",
      "epoch:15 batch:190 loss:0.5461575984954834 acc:0.9833333333333333\n",
      "epoch:15 batch:191 loss:0.5689604878425598 acc:1.0\n",
      "epoch:15 batch:192 loss:0.568986713886261 acc:0.9833333333333333\n",
      "epoch:15 batch:193 loss:0.5815756916999817 acc:0.9666666666666667\n",
      "epoch:15 batch:194 loss:0.7446169257164001 acc:0.8\n",
      "epoch:15 batch:195 loss:2.20225191116333 acc:0.0\n",
      "epoch:15 batch:196 loss:0.624266505241394 acc:0.7166666666666667\n",
      "epoch:15 batch:197 loss:0.15975934267044067 acc:1.0\n",
      "epoch:15 batch:198 loss:0.1533343642950058 acc:1.0\n",
      "epoch:15 batch:199 loss:0.13214008510112762 acc:1.0\n",
      "epoch:15 batch:200 loss:0.14370353519916534 acc:1.0\n",
      "epoch:15 batch:201 loss:0.13755138218402863 acc:1.0\n",
      "epoch:15 batch:202 loss:0.25523698329925537 acc:0.8333333333333334\n",
      "epoch:15 batch:203 loss:0.20744451880455017 acc:0.95\n",
      "epoch:15 batch:204 loss:2.2323338985443115 acc:0.0\n",
      "epoch:15 batch:205 loss:1.7870333194732666 acc:0.11666666666666667\n",
      "epoch:15 batch:206 loss:0.1158272996544838 acc:1.0\n",
      "epoch:15 batch:207 loss:0.12787848711013794 acc:1.0\n",
      "epoch:15 batch:208 loss:0.1465614140033722 acc:1.0\n",
      "epoch:15 batch:209 loss:0.14573445916175842 acc:1.0\n",
      "epoch:15 batch:210 loss:0.15059353411197662 acc:1.0\n",
      "epoch:15 batch:211 loss:0.965755820274353 acc:0.3333333333333333\n",
      "epoch:15 batch:212 loss:1.154194712638855 acc:0.11666666666666667\n",
      "epoch:15 batch:213 loss:0.3462096154689789 acc:1.0\n",
      "epoch:15 batch:214 loss:0.4122033715248108 acc:1.0\n",
      "epoch:15 batch:215 loss:0.413921982049942 acc:1.0\n",
      "epoch:15 batch:216 loss:0.40259653329849243 acc:1.0\n",
      "epoch:15 batch:217 loss:0.3900321424007416 acc:1.0\n",
      "epoch:15 batch:218 loss:0.3468289375305176 acc:1.0\n",
      "epoch:15 batch:219 loss:0.3426908552646637 acc:1.0\n",
      "epoch:15 batch:220 loss:0.3544715940952301 acc:1.0\n",
      "epoch:15 batch:221 loss:0.3555390238761902 acc:1.0\n",
      "epoch:15 batch:222 loss:0.349781334400177 acc:1.0\n",
      "epoch:15 batch:223 loss:0.35523682832717896 acc:1.0\n",
      "epoch:15 batch:224 loss:0.3315674662590027 acc:1.0\n",
      "totally cost 80.18315505981445\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "time_start=time.time()\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch, (batch_x, batch_y) in enumerate(train_dataloader):   \n",
    "        batch_x = batch_x.cuda()   \n",
    "        output,pre_out = model(batch_x)\n",
    "        output = torch.reshape(output,[-1,1])\n",
    "\n",
    "        batch_y = np.array(batch_y)\n",
    "        batch_y = torch.tensor(np.reshape(batch_y,[-1,1]))\n",
    "        batch_y = batch_y.float()\n",
    "        batch_y = batch_y.cuda()\n",
    "\n",
    "        loss = loss_function(output,batch_y)\n",
    "        acc = evaluate_accuracy(batch_x,batch_y,model)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_array[batch] = loss.item()\n",
    "        batch_index_array[batch] = batch\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"epoch:{} batch:{} loss:{} acc:{}\".format(epoch,batch,loss.item(),acc))\n",
    "\n",
    "time_end=time.time()\n",
    "print('totally cost',time_end-time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06fdbe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 loss:0.18570905923843384\n",
      "train acc 0.9332345313078918\n",
      "valid acc: 0.951967978652435\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "x = torch.tensor( X_train[:,:,:]).float()\n",
    "x = torch.where(torch.isnan(x), torch.full_like(x, 0), x)\n",
    "    \n",
    "y = y_train[:]\n",
    "y = np.array(y)\n",
    "y = torch.tensor(np.reshape(y,[-1,1]))\n",
    "y = y.float()\n",
    "    \n",
    "x = x.cuda()\n",
    "y = y.cuda()\n",
    "\n",
    "x_valid = torch.tensor(X_valid[:,:,:]).float()\n",
    "x_valid = torch.where(torch.isnan(x_valid), torch.full_like(x_valid, 0), x_valid)\n",
    "    \n",
    "y_val = y_valid[:]\n",
    "y_val = np.array(y_val)\n",
    "y_val = torch.tensor(np.reshape(y_val,[-1,1]))\n",
    "y_val = y_val.float()\n",
    "\n",
    "x_valid = x_valid.cuda()\n",
    "y_val = y_val.cuda()\n",
    "\n",
    "print(\"epoch:{} loss:{}\".format(epoch,loss.item()))\n",
    "print(\"train acc\", evaluate_accuracy(x,y,model))\n",
    "print(\"valid acc:\",evaluate_accuracy(x_valid,y_val,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81b5cb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.30      0.40        79\n",
      "           1       0.96      0.99      0.97      1420\n",
      "\n",
      "    accuracy                           0.95      1499\n",
      "   macro avg       0.77      0.65      0.69      1499\n",
      "weighted avg       0.94      0.95      0.94      1499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as sm\n",
    "output,pre_out = model(x_valid)\n",
    "output = torch.reshape(output,[-1,1])\n",
    "output = output.ge(0.5)\n",
    "\n",
    "output_cpu = output.cpu()\n",
    "cr = sm.classification_report(y_valid[:,0], output_cpu)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187967c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
